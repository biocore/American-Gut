{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This IPython notebook assumes precomputed BIOM tables and metadata are readily available. The purpose of this notebook is to output a summary PDF, per sample, present in the American Gut dataset. This requires the full American Gut, the [Human Microbiome Project](http://www.ncbi.nlm.nih.gov/pubmed/22699609), [Global Gut](http://www.ncbi.nlm.nih.gov/pubmed/22699611) and unpublished [Personal Genome Project](http://personalgenomes.org/) microbiome samples. Please see XXX for a discussion on how these tables were created. This notebook assumes the following: \n",
      "\n",
      "* a PBS/Torque-based compute cluster in which to submit jobs to\n",
      "* QIIME 1.7 is available in the path\n",
      "* A custom Emperor [branch](https://github.com/eldeveloper/emperor/tree/faces_plus_lines) is available in the path\n",
      "* TexLive"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal of this notebook is to produce framable results for every American Gut fecal sample. Specifically, for each sample, we will be producing the following figures:\n",
      "\n",
      "* Principal coordinates plot showing where the sample lies in the context of the American Gut Project, the Human Microbiome Project, the Global Gut and the Personal Genome Project datasets.\n",
      "* Principal coordinates plot showing where the sample lies in the context of the American Gut Project, Global Gut project, highlighting the variation in host age has on the sample.\n",
      "* Principal coordinates plot showing where the sample lies in the context of the American Gut Project.\n",
      "* Taxonomy summary of the sample, and for comparison, a few collapsed American Gut groups and Michael Pollan's pre and post antibiotics sample.\n",
      "* Significantly differentiated operational taxonomic units in the sample compared to other American Gut samples."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, lets get our environment up and running. The script utils.ipy contains some helper methods for submitting jobs to Torque-based clusters. In addition, lets create a new directory for us to work under, setup some helper functions and some paths. Please note, you'll need to specify the path to the 97% Greengenes tree."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run cluster_utils.ipy\n",
      "\n",
      "# get the current absolute path\n",
      "_basedir = os.path.abspath('.')\n",
      "\n",
      "### to toss existing saved environment\n",
      "#drop_env()\n",
      "\n",
      "if recover():\n",
      "    recover_env()\n",
      "    print \"Recovered %s\" % prj_name\n",
      "else:    \n",
      "    create_env(\"ag_mod2\")\n",
      "    print \"Working on: %s\" % prj_name\n",
      "\n",
      "if not working_dir.rsplit('/', 1)[0].endswith('American-Gut/ipynb'):\n",
      "    raise ValueError, \"The notebook must be run from the American-Gut/ipynb directory!\"\n",
      "\n",
      "# submission wrapper\n",
      "submit = lambda cmd: submit_qsub(cmd, job_name=prj_name, queue='memroute', extra_args='-l pvmem=8gb')\n",
      "    \n",
      "# path wrapper\n",
      "get_path = lambda x: os.path.join(working_dir, x)\n",
      "    \n",
      "# set the number of processors parallel tasks will use\n",
      "NUM_PROCS = 100\n",
      "\n",
      "# set the path to the Greengenes 13_5 97% OTU tree\n",
      "#greengenes135_97_tree_fp = os.path.expandvars('/ABSOLUTE/PATH/MUST/BE/SPECIFIED')\n",
      "greengenes135_97_tree_fp = os.path.expandvars('$HOME/ResearchWork/gg_13_5_otus/trees/97_otus.tree')\n",
      "if not os.path.exists(greengenes135_97_tree_fp):\n",
      "    raise ValueError(\"Greengenes tree not found, make sure to set the path in the variable 'greengenes135_97_tree_fp'\")\n",
      "    \n",
      "def check_file(f):\n",
      "    if not os.path.exists(f):\n",
      "        raise ValueError(\"Cannot continue! The file %s does not exist!\" % f)\n",
      "\n",
      "def cp_raw(src,dst):\n",
      "    \"\"\"make copies of the original data\"\"\"\n",
      "    !cp $src $dst\n",
      "    \n",
      "def farm_commands(commands, chunk_size):\n",
      "    commands = list(commands)\n",
      "    start = 0\n",
      "    jobs = []\n",
      "    for end in range(chunk_size, len(commands) + chunk_size, chunk_size):\n",
      "        chunk = commands[start:end]\n",
      "        start = end\n",
      "        jobs.append(submit(chunk)) \n",
      "    return wait_on(jobs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Recovered ag_mod2_4sy\n"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print working_dir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_4sy\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "qiime_scripts = {\n",
      "    'Merge OTU Tables':'merge_otu_tables.py -i %(input_a)s,%(input_b)s -o %(output)s',\n",
      "    'Single Rarifaction':'single_rarefaction.py -i %(input)s -o %(output)s -d %(depth)s',\n",
      "    'Parallel Beta Diversity':'parallel_beta_diversity.py -i %(input)s -o %(output)s -X %(job_prefix)s -O %(num_jobs)s -m unweighted_unifrac -t %(gg97_tree)s',\n",
      "    'Principal Coordinates':'principal_coordinates.py -i %(input)s -o %(output)s',\n",
      "    'Merge Mapping Files':'merge_mapping_files.py -m %(input_a)s,%(input_b)s -o %(output)s',\n",
      "    'Filter Samples':'filter_samples_from_otu_table.py -i %(input)s -o %(output)s --sample_id_fp=%(sample_id_fp)s',\n",
      "    'Summarize OTU by Category':'summarize_otu_by_cat.py -i %(mapping)s -o %(output)s -n -c %(otu_table)s -m %(category)s',\n",
      "    'Filter Distance Matrix':'filter_distance_matrix.py -i %(input)s -o %(output)s --sample_id_fp=%(sample_ids)s',\n",
      "    'Summarize Taxa':'summarize_taxa.py -i %(input)s -o %(output)s -L %(level)s',\n",
      "    'Summarize Taxa Mapping':'summarize_taxa.py -i %(input)s -o %(output)s -L %(level)s -m %(mapping)s'\n",
      "    }\n",
      "other_scripts = {\n",
      "    'Signifigant OTU Table':'otu_significance.py -i %(input)s -o %(output)s -l %(level)s -t adasdsdasd', # input -> biomtable, output->directory,\n",
      "    'Taxonomy Comparison':'taxonomy_comparison.py -i %(input)s -m %(mapping)s -l %(level)s -o %(output)s -c %(list_of_categories)s',\n",
      "    'Make Emperor':'make_emperor.py -i %(input)s -o %(output)s -m %(mapping)s',\n",
      "    'SVG Smash':'replace_svg_object.py -i %(input)s -o %(output)s --prefix %(prefix)s --sample_id=%(sample_id)s',\n",
      "    'Make Phyla Plots':\"make_phyla_plots_AGP.py -i %(input)s -m %(mapping)s -o %(output)s -c '%(categories)s' -s %(samples)s\",\n",
      "    'OTU Significance':\"generate_otu_signifigance_tables_AGP.py -i %(input)s -o %(output)s\",\n",
      "    'Create Titles':'create_titles.py -m %(mapping)s -f',\n",
      "    'Format Template':'format_file.py -i %(template)s -k %(keys_for_replace)s -v %(values_for_replace)s -K %(keys_for_insert)s -V %(values_for_insert)s -o %(output)s',\n",
      "    'To PDF':'module load texlive_2013; cd %(path)s; lualatex %(input)s',\n",
      "    'To PNG':'inkscape -e %{png}s -d 300 %{pdf}s',\n",
      "    'PDF Smash':'gs -r150 -q -sPAPERSIZE=ledger -dNOPAUSE -dBATCH -sDEVICE=pdfwrite -dFIXEDMEDIA -dPDFFitPage -dCompatibilityLevel=1.4 -sOutputFile=%(output)s -c 100000000 setvmthreshold -f %(pdfs)s'\n",
      "    }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, lets setup the paths and some helper methods. The first PCoA plot is a combination of the American Gut, Human Microbiome Project, Personal Genome Project and Global Gut datasets. These projects all used three different sequencing technologies however, and in order to combine them, we need to use the BIOM tables derived from sequence data all trimmed to the same length. See XXX for a more detailed discussion."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# working directory file paths for tables and metadata. \n",
      "# ag -> American Gut\n",
      "# pgp -> Personal Genome Project\n",
      "# hmp -> Human Microbiome Project\n",
      "# gg -> Global Gut\n",
      "#\n",
      "# _t_ -> table\n",
      "# _m_ -> mapping file\n",
      "#\n",
      "# 100nt -> trimmed to the first 100 nucleotides \n",
      "# original file paths\n",
      "get_raw_latex_filepath = lambda x: os.path.join('../latex', x)\n",
      "get_raw_latexextras_filepath = lambda x: os.path.join('../latex-extras/pdfs', x)\n",
      "get_raw_data_filepath = lambda x, y: os.path.join('../data', x, y)\n",
      "raw_to_working = lambda x: get_path(os.path.basename(x))\n",
      "raw_to_working_gz = lambda x: os.path.splitext(raw_to_working(x))[0]\n",
      "\n",
      "# setup raw data sources\n",
      "raw_ag_100nt_t_fp = get_raw_data_filepath('AG', 'AG_100nt.biom.gz')\n",
      "raw_ag_100nt_m_fp = get_raw_data_filepath('AG', 'AG_100nt.txt')\n",
      "raw_ag_t_fp = get_raw_data_filepath('AG', 'AG.biom.gz')\n",
      "raw_ag_m_fp = get_raw_data_filepath('AG', 'AG.txt')\n",
      "\n",
      "raw_pgp_100nt_t_fp = get_raw_data_filepath('PGP', 'PGP_100nt.biom.gz')\n",
      "raw_pgp_100nt_m_fp = get_raw_data_filepath('PGP', 'PGP_100nt.txt')\n",
      "\n",
      "raw_hmp_100nt_t_fp = get_raw_data_filepath('HMP', 'HMPv35_100nt.biom.gz')\n",
      "raw_hmp_100nt_m_fp = get_raw_data_filepath('HMP', 'HMPv35_100nt.txt')\n",
      "\n",
      "raw_gg_100nt_t_fp = get_raw_data_filepath('GG', 'GG_100nt.biom.gz')\n",
      "raw_gg_100nt_m_fp = get_raw_data_filepath('GG', 'GG_100nt.txt')\n",
      "\n",
      "raw_latex_template       = get_raw_latex_filepath('template.tex')\n",
      "raw_latex_aglogo         = get_raw_latexextras_filepath('logoshape.pdf')\n",
      "raw_latex_header         = get_raw_latexextras_filepath('headerbw.pdf')\n",
      "raw_latex_fig1_legend    = get_raw_latexextras_filepath('figure1_legend.pdf')\n",
      "raw_latex_fig2_legend    = get_raw_latexextras_filepath('figure2_legend.pdf')\n",
      "raw_latex_fig2_2ndlegend = get_raw_latexextras_filepath('figure2_country_legend.pdf')\n",
      "raw_latex_fig3_legend    = get_raw_latexextras_filepath('figure3_legend.pdf')\n",
      "raw_latex_fig1_ovals     = get_raw_latexextras_filepath('figure1_ovals.png')\n",
      "raw_latex_fig2_ovals     = get_raw_latexextras_filepath('figure2_ovals.png')\n",
      "raw_latex_fig4_overlay   = get_raw_latexextras_filepath('figure4_overlay.pdf')\n",
      "raw_latex_ball_legend    = get_raw_latexextras_filepath('ball_legend.pdf')\n",
      "raw_latex_title          = get_raw_latexextras_filepath('youramericangutsampletext.pdf')\n",
      "\n",
      "raw_previously_printed = get_path('previously_printed.txt')\n",
      "raw_participants = get_path('participants.zip')\n",
      "\n",
      "# verify the data sources exist\n",
      "check_file(raw_ag_100nt_t_fp)\n",
      "check_file(raw_ag_100nt_m_fp)\n",
      "check_file(raw_ag_t_fp)\n",
      "check_file(raw_ag_m_fp)\n",
      "check_file(raw_pgp_100nt_t_fp)\n",
      "check_file(raw_pgp_100nt_m_fp)\n",
      "check_file(raw_hmp_100nt_t_fp)\n",
      "check_file(raw_hmp_100nt_m_fp)\n",
      "check_file(raw_gg_100nt_t_fp)\n",
      "check_file(raw_gg_100nt_t_fp)\n",
      "\n",
      "check_file(raw_latex_template)\n",
      "check_file(raw_latex_header)\n",
      "check_file(raw_latex_aglogo)\n",
      "check_file(raw_latex_fig1_legend)\n",
      "check_file(raw_latex_fig2_legend)\n",
      "check_file(raw_latex_fig2_2ndlegend)\n",
      "check_file(raw_latex_fig3_legend)\n",
      "check_file(raw_latex_fig4_overlay)\n",
      "check_file(raw_latex_fig1_ovals)\n",
      "check_file(raw_latex_fig2_ovals)\n",
      "check_file(raw_latex_ball_legend)\n",
      "check_file(raw_latex_title)\n",
      "\n",
      "# use participant names only if the data are available.\n",
      "# NOTE: these data are _not_ part of the github repository for\n",
      "#       privacy reasons.\n",
      "try:\n",
      "    check_file(raw_participants)\n",
      "    PARTICIPANTS_PASSWORD = None\n",
      "except ValueError:\n",
      "    raw_participants = None\n",
      "\n",
      "# are there a set of previously printed results that we can ignore\n",
      "# downstream?\n",
      "try:\n",
      "    check_file(raw_previously_printed)\n",
      "    prev_printed = set([l.strip() for l in open(raw_previously_printed)])\n",
      "except:\n",
      "    prev_printed = set([])\n",
      "    \n",
      "# copy raw to working directory\n",
      "cp_raw(raw_ag_100nt_t_fp, working_dir)\n",
      "cp_raw(raw_ag_100nt_m_fp, working_dir)\n",
      "cp_raw(raw_ag_t_fp, working_dir)\n",
      "cp_raw(raw_ag_m_fp, working_dir)\n",
      "cp_raw(raw_pgp_100nt_t_fp, working_dir)\n",
      "cp_raw(raw_pgp_100nt_m_fp, working_dir)\n",
      "cp_raw(raw_hmp_100nt_t_fp, working_dir)\n",
      "cp_raw(raw_hmp_100nt_m_fp, working_dir)\n",
      "cp_raw(raw_gg_100nt_t_fp, working_dir)\n",
      "cp_raw(raw_gg_100nt_m_fp, working_dir)\n",
      "cp_raw(raw_latex_template, working_dir)\n",
      "cp_raw(raw_latex_aglogo, working_dir)\n",
      "cp_raw(raw_latex_header, working_dir)\n",
      "cp_raw(raw_latex_fig1_legend, working_dir)\n",
      "cp_raw(raw_latex_fig2_legend, working_dir)\n",
      "cp_raw(raw_latex_fig2_2ndlegend, working_dir)\n",
      "cp_raw(raw_latex_fig3_legend, working_dir)\n",
      "cp_raw(raw_latex_fig4_overlay, working_dir)\n",
      "cp_raw(raw_latex_fig1_ovals, working_dir)\n",
      "cp_raw(raw_latex_fig2_ovals, working_dir)\n",
      "cp_raw(raw_latex_ball_legend, working_dir)\n",
      "cp_raw(raw_latex_title, working_dir)\n",
      "\n",
      "# setup working file paths (note, .biom files decompressed in the following cell\n",
      "ag_100nt_t_fp  = raw_to_working_gz(raw_ag_100nt_t_fp)\n",
      "ag_100nt_m_fp  = raw_to_working(raw_ag_100nt_m_fp)\n",
      "ag_t_fp        = raw_to_working_gz(raw_ag_t_fp)\n",
      "ag_m_fp        = raw_to_working(raw_ag_m_fp)\n",
      "pgp_100nt_t_fp = raw_to_working_gz(raw_pgp_100nt_t_fp)\n",
      "pgp_100nt_m_fp = raw_to_working(raw_pgp_100nt_m_fp)\n",
      "hmp_100nt_t_fp = raw_to_working_gz(raw_hmp_100nt_t_fp)\n",
      "hmp_100nt_m_fp = raw_to_working(raw_hmp_100nt_m_fp)\n",
      "gg_100nt_t_fp  = raw_to_working_gz(raw_gg_100nt_t_fp)\n",
      "gg_100nt_m_fp  = raw_to_working(raw_gg_100nt_m_fp)\n",
      "template       = raw_to_working(raw_latex_template)\n",
      "aglogo         = raw_to_working(raw_latex_aglogo)\n",
      "header         = raw_to_working(raw_latex_header)\n",
      "fig1_legend    = raw_to_working(raw_latex_fig1_legend)\n",
      "fig2_legend    = raw_to_working(raw_latex_fig2_legend)\n",
      "fig2_2ndlegend = raw_to_working(raw_latex_fig2_2ndlegend)\n",
      "fig3_legend    = raw_to_working(raw_latex_fig3_legend)\n",
      "fig1_ovals     = raw_to_working(raw_latex_fig1_ovals)\n",
      "fig2_ovals     = raw_to_working(raw_latex_fig2_ovals)\n",
      "fig4_overlay   = raw_to_working(raw_latex_fig4_overlay)\n",
      "ball_legend    = raw_to_working(raw_latex_ball_legend)\n",
      "title          = raw_to_working(raw_latex_title)\n",
      "\n",
      "if raw_participants is not None:\n",
      "    import zipfile\n",
      "    zf = zipfile.ZipFile(raw_participants)\n",
      "    zf.setpassword(PARTICIPANTS_PASSWORD)\n",
      "    participants = dict([l.strip().split('\\t')[:2] for l in zf.read('participants.txt').splitlines() if not l.startswith('#')])\n",
      "    print \"Using identified data!\"\n",
      "else:\n",
      "    participants = None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using identified data!\n"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not all QIIME scripts support gzip'd BIOM tables, so lets uncompress if necessary."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "files = !ls $working_dir\n",
      "for f in files.grep(\"biom.gz$\"):\n",
      "    f = get_path(f)\n",
      "    !gunzip -f $f"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "gzip: /home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_4sy/AG_100nt.biom already exists; do you wish to overwrite (y or n)? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "^C"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "gzip: /home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_4sy/AG.biom already exists; do you wish to overwrite (y or n)? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "^C"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "gzip: /home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_4sy/GG_100nt.biom already exists; do you wish to overwrite (y or n)? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "^C"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "gzip: /home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_4sy/HMPv35_100nt.biom already exists; do you wish to overwrite (y or n)? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "^C\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "gzip: /home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_4sy/PGP_100nt.biom already exists; do you wish to overwrite (y or n)? "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "^C\r\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we need to massage the metadata to improve our ability to compare samples. Specifically, were going to: \n",
      "\n",
      "* simplify body sites into their major categories (e.g., transform \"forehead\" and \"skin of hand\" to just \"skin\")\n",
      "* simplify country codes (e.g., GAZ:Venezuela to Venezuela)\n",
      "* simplify the experiment title (e.g., American Gut Project to AGP)\n",
      "* create a hybrid field combining the experiment title with the body site"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a body site mapping:\n",
      "simple_matter_map = {\n",
      "        'feces':'FECAL',\n",
      "        'sebum':'SKIN',\n",
      "        'tongue':'ORAL',\n",
      "        'skin':'SKIN',\n",
      "        'mouth':'ORAL',\n",
      "        'gingiva':'ORAL',\n",
      "        'gingival epithelium':'ORAL',\n",
      "        'nares':'SKIN',\n",
      "        'skin of hand':'SKIN',\n",
      "        'hand':'SKIN',\n",
      "        'skin of head':'SKIN',\n",
      "        'hand skin':'SKIN',\n",
      "        'throat':'ORAL',\n",
      "        'auricular region zone of skin':'SKIN',\n",
      "        'mucosa of tongue':'ORAL',\n",
      "        'mucosa of vagina':'SKIN',\n",
      "        'palatine tonsil':'ORAL',\n",
      "        'hard palate':'ORAL',\n",
      "        'saliva':'ORAL',\n",
      "        'stool':'FECAL',\n",
      "        'vagina':'SKIN',\n",
      "        'fossa':'SKIN',\n",
      "        'buccal mucosa':'ORAL',\n",
      "        'vaginal fornix':'SKIN',\n",
      "        'hair follicle':'SKIN',\n",
      "        'nostril':'SKIN'\n",
      "        }\n",
      "\n",
      "def massage_mapping(in_fp, out_fp, body_site_column_name, exp_acronym):\n",
      "    \"\"\"Simplify the mapping file for use in figures\n",
      "\n",
      "    body_site_column_name : specify the column name for body\n",
      "    exp_acronym : short name for the study\n",
      "\n",
      "    Returns False on failure, True on success\n",
      "    \"\"\"\n",
      "    age_cat_map = [(0,2,'Baby'),\n",
      "                   (2,13,'Child'),\n",
      "                   (13,20,'Teen'),\n",
      "                   (20,30,'20s'),\n",
      "                   (30,40,'30s'),\n",
      "                   (40,50,'40s'),\n",
      "                   (50,60,'50s'),\n",
      "                   (60,70,'60s'),\n",
      "                   (70,80,'70s'),\n",
      "                   (80,99999,'Older than 80')]\n",
      "    bmi_cat_map = [(0, 18.5,'Underweight'),\n",
      "                   (18.5, 25,'Normal'),\n",
      "                   (25, 30,'Overweight'),\n",
      "                   (30, 35,'Moderately obese'),\n",
      "                   (35, 40,'Severely obese'),\n",
      "                   (40, 99999,'Very severely obese')]\n",
      "    \n",
      "    mapping_lines = [l.strip().split('\\t') for l in open(in_fp)]\n",
      "    \n",
      "    header = mapping_lines[0]\n",
      "    header_low = map(lambda x: x.lower(), header)\n",
      "        \n",
      "    try:\n",
      "        bodysite_idx = header_low.index(body_site_column_name.lower())\n",
      "    except ValueError:\n",
      "        print \"Could not find '%s' in the mapping file header!\" % body_site_column_name\n",
      "        return False\n",
      "    \n",
      "    try:\n",
      "        country_idx = header_low.index('country')\n",
      "    except ValueError:\n",
      "        print \"Could not find the country in the mapping file header!\"\n",
      "        return False\n",
      "\n",
      "    # see if there is an age category\n",
      "    try:\n",
      "        age_idx = header_low.index('age')\n",
      "    except ValueError:\n",
      "        age_idx = None\n",
      "        \n",
      "    # see if there is a bmi category    \n",
      "    try:\n",
      "        bmi_idx = header_low.index('bmi')\n",
      "    except ValueError:\n",
      "        bmi_idx = None\n",
      "    \n",
      "    new_mapping_lines = [header[:]]\n",
      "    new_mapping_lines[0].append('SIMPLE_BODY_SITE')\n",
      "    new_mapping_lines[0].append('TITLE_ACRONYM')\n",
      "    new_mapping_lines[0].append('TITLE_BODY_SITE')\n",
      "    new_mapping_lines[0].append('HMP_SITE')\n",
      "\n",
      "    if age_idx is not None:\n",
      "        new_mapping_lines[0].append('AGE_CATEGORY')\n",
      "    if bmi_idx is not None:\n",
      "        new_mapping_lines[0].append('BMI_CATEGORY')\n",
      "    \n",
      "    for l in mapping_lines[1:]:\n",
      "        new_line = l[:]\n",
      "        body_site = new_line[bodysite_idx]\n",
      "        country = new_line[country_idx]\n",
      "        \n",
      "        # grab the body site\n",
      "        if body_site.startswith('UBERON_'):\n",
      "            body_site = body_site.split('_',1)[-1].replace(\"_\",\" \")\n",
      "        elif body_site.startswith('UBERON:'):\n",
      "            body_site = body_site.split(':',1)[-1]\n",
      "        elif body_site in ['NA', 'unknown']:\n",
      "            # controls, environmental, etc\n",
      "            continue\n",
      "        else:\n",
      "            print \"Unknown body site value: %s, SampleID: %s\" % (body_site, new_line[0])\n",
      "            continue\n",
      "        \n",
      "        # remap the body site\n",
      "        if body_site.lower() not in simple_matter_map:\n",
      "            print \"Could not remap body site: %s, SampleID: %s\" % (body_site, new_line[0])\n",
      "            return False\n",
      "        else:\n",
      "            body_site = simple_matter_map[body_site.lower()]\n",
      " \n",
      "        if exp_acronym == 'HMP':\n",
      "            hmp_site = 'HMP-%s' % body_site\n",
      "        else:\n",
      "            hmp_site = body_site            \n",
      "            \n",
      "        # simplify the country    \n",
      "        if country.startswith('GAZ:'):\n",
      "            country = country.split(':',1)[-1]\n",
      "        else:\n",
      "            print \"Could not parse country value: %s\" % country\n",
      "            return False\n",
      "        \n",
      "        if age_idx is not None:\n",
      "            age_cat = None\n",
      "            if new_line[age_idx] in ['NA','None']:\n",
      "                age_cat = 'Unknown'\n",
      "            else:\n",
      "                try:\n",
      "                    # PGP is currently in age ranges, ignoring those for now\n",
      "                    age = float(new_line[age_idx])\n",
      "                except ValueError:\n",
      "                    age_cat = 'Unknown'\n",
      "                    \n",
      "            if age_cat is not 'Unknown':    \n",
      "                for low,high,cat in age_cat_map:\n",
      "                    if low <= age < high:\n",
      "                        age_cat = cat\n",
      "                        break\n",
      "                if age_cat is None:\n",
      "                    print \"Count not map %f to an age category!\" % age\n",
      "                    return False\n",
      "            \n",
      "        if bmi_idx is not None:\n",
      "            if new_line[bmi_idx] in ['NA','', 'None']:\n",
      "                bmi_cat = 'Unknown'\n",
      "            else:\n",
      "                bmi = float(new_line[bmi_idx])\n",
      "                bmi_cat = None\n",
      "                for low,high,cat in bmi_cat_map:\n",
      "                    if low <= bmi < high:\n",
      "                        bmi_cat = cat\n",
      "                        break\n",
      "                if bmi_cat is None:\n",
      "                    print \"Count not map %f to a BMI category!\" % bmi\n",
      "                \n",
      "        new_line.append(body_site)\n",
      "        new_line.append(exp_acronym)\n",
      "        new_line.append(\"%s-%s\" % (exp_acronym, body_site))\n",
      "        new_line[country_idx] = country\n",
      "        new_line.append(hmp_site)\n",
      "        \n",
      "        if age_idx is not None:\n",
      "            new_line.append(age_cat)\n",
      "        \n",
      "        if bmi_idx is not None:\n",
      "            new_line.append(bmi_cat)\n",
      "            \n",
      "        new_mapping_lines.append(new_line)\n",
      "    \n",
      "    output = open(out_fp,'w')\n",
      "    output.write('\\n'.join(['\\t'.join(l) for l in new_mapping_lines]))\n",
      "    output.write('\\n')\n",
      "    output.close()\n",
      "    \n",
      "    return True\n",
      "\n",
      "def test_massage_mapping():\n",
      "    \"\"\"Exercise the massage mapping code, verify expected results\"\"\"\n",
      "    # output test file\n",
      "    ag_100nt_m_TEST_fp = get_path('AG_100nt_TEST.txt')\n",
      "    \n",
      "    # make sure massage_mapping fails with a bad column header\n",
      "    assert not massage_mapping(ag_100nt_m_fp, ag_100nt_m_TEST_fp, 'does not exist', 'AGP')\n",
      "    \n",
      "    # make sure massage mapping works with a good header\n",
      "    assert massage_mapping(ag_100nt_m_fp, ag_100nt_m_TEST_fp, 'body_site', 'AGP')\n",
      "    \n",
      "    # verify the resulting header structure\n",
      "    test_mapping = [l.strip().split('\\t') for l in open(ag_100nt_m_TEST_fp)]\n",
      "    test_header = test_mapping[0]\n",
      "    test_header_length = len(test_header)\n",
      "    assert test_header[-6:] == ['SIMPLE_BODY_SITE', 'TITLE_ACRONYM', 'TITLE_BODY_SITE', 'HMP_SITE', 'AGE_CATEGORY','BMI_CATEGORY']\n",
      "    \n",
      "    # verify each line in the test file\n",
      "    for l in test_mapping[1:]:\n",
      "        assert l[-6] in ['FECAL','SKIN','ORAL']\n",
      "        assert l[-5] == 'AGP'\n",
      "        acro, site = l[-4].split('-')\n",
      "        assert acro == 'AGP'\n",
      "        assert site in ['FECAL','SKIN','ORAL']\n",
      "        assert l[-3] in ['FECAL','SKIN','ORAL']\n",
      "        assert len(l) == test_header_length\n",
      "    \n",
      "    # missing explicit check on age category and bmi category. visual inspection:\n",
      "    # !head AG_100nt_TEST.txt | cut -f 11,53,168,169\n",
      "    # ...passes, sufficient for right now. \n",
      "\n",
      "### uncomment the next line to test the massage_mapping function\n",
      "#test_massage_mapping()\n",
      "\n",
      "# new file paths\n",
      "ag_100nt_m_massaged_fp = get_path('AG_100nt_massaged.txt')\n",
      "gg_100nt_m_massaged_fp = get_path('GG_100nt_massaged.txt')\n",
      "pgp_100nt_m_massaged_fp = get_path('PGP_100nt_massaged.txt')\n",
      "hmp_100nt_m_massaged_fp = get_path('HMP_100nt_massaged.txt')\n",
      "\n",
      "# massage\n",
      "assert massage_mapping(ag_100nt_m_fp, ag_100nt_m_massaged_fp, 'body_site', 'AGP')\n",
      "assert massage_mapping(gg_100nt_m_fp, gg_100nt_m_massaged_fp, 'body_site', 'GG')\n",
      "assert massage_mapping(pgp_100nt_m_fp, pgp_100nt_m_massaged_fp, 'body_site', 'PGP')\n",
      "assert massage_mapping(hmp_100nt_m_fp, hmp_100nt_m_massaged_fp, 'bodysite', 'HMP')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Unknown body site value: None, SampleID: Blank20.G8.1124128\n",
        "Unknown body site value: None, SampleID: 000003431.1123718\n",
        "Unknown body site value: None, SampleID: Blank16.A7.1123804\n",
        "Unknown body site value: None, SampleID: Blank21.E9.1128296\n",
        "Unknown body site value: None, SampleID: Blank18.G6.1123819\n",
        "Unknown body site value: None, SampleID: Blank19.C8.1123995\n",
        "Unknown body site value: None, SampleID: Blank24r.H12.1123994\n",
        "Unknown body site value: None, SampleID: 000004672b.1123996\n",
        "Unknown body site value: None, SampleID: Blank17.E5.1128309\n",
        "Unknown body site value: None, SampleID: Blank16.D7.1123977\n",
        "Unknown body site value: None, SampleID: 000009239.1128709\n",
        "Unknown body site value: None, SampleID: Blank22.H9.1128329\n",
        "Unknown body site value: None, SampleID: 000009353b.1124011\n",
        "Unknown body site value: None, SampleID: Blank17.A5.1128334\n",
        "Unknown body site value: None, SampleID: Blank19.H8.1123754\n",
        "Unknown body site value: None, SampleID: Blank23.H11.1128419\n",
        "Unknown body site value: None, SampleID: Blank27.D3.1128717\n",
        "Unknown body site value: None, SampleID: Blank22.B6.1128412\n",
        "Unknown body site value: None, SampleID: 000009353a.1123866\n",
        "Unknown body site value: None, SampleID: Blank21.B9.1128810\n",
        "Unknown body site value: None, SampleID: Blank22.G7.1128649\n",
        "Unknown body site value: None, SampleID: Blank16.G7.1124106\n",
        "Unknown body site value: None, SampleID: Blank25.C2.1128723\n",
        "Unknown body site value: None, SampleID: Blank22.G6.1128615\n",
        "Unknown body site value: None, SampleID: Blank27.F3.1128532\n",
        "Unknown body site value: None, SampleID: Blank16.H7.1123899\n",
        "Unknown body site value: None, SampleID: Blank16.D12.1124015\n",
        "Unknown body site value: None, SampleID: Blank27.B3.1128351\n",
        "Unknown body site value: None, SampleID: Blank18.E6.1123919\n",
        "Unknown body site value: None, SampleID: Blank27.G3.1128369\n",
        "Unknown body site value: None, SampleID: Blank18.C6.1123985\n",
        "Unknown body site value: None, SampleID: Blank23.G11.1128581\n",
        "Unknown body site value: None, SampleID: Blank22.C6.1128804\n",
        "Unknown body site value: None, SampleID: Blank16.B7.1123727\n",
        "Unknown body site value: None, SampleID: Blank24r.H8.1123822\n",
        "Unknown body site value: None, SampleID: Blank22.H11.1128730\n",
        "Unknown body site value: None, SampleID: Blank20.C8.1124021\n",
        "Unknown body site value: None, SampleID: Blank16.F7.1123789\n",
        "Unknown body site value: None, SampleID: Blank23.F11.1128407\n",
        "Unknown body site value: None, SampleID: Blank24r.G8.1123986\n",
        "Unknown body site value: None, SampleID: Blank24r.F12.1123843\n",
        "Unknown body site value: None, SampleID: Blank23.A11.1128667\n",
        "Unknown body site value: None, SampleID: 000006930.1128600\n",
        "Unknown body site value: None, SampleID: Blank26.E1.1128599\n",
        "Unknown body site value: None, SampleID: Blank23.E11.1128529\n",
        "Unknown body site value: None, SampleID: Blank21.C9.1128523\n",
        "Unknown body site value: None, SampleID: 000002182.1124085\n",
        "Unknown body site value: None, SampleID: Blank17.C5.1128613\n",
        "Unknown body site value: None, SampleID: Blank17.D5.1128826\n",
        "Unknown body site value: None, SampleID: Blank24r.G12.1123790\n",
        "Unknown body site value: None, SampleID: Blank25.H2.1128776\n",
        "Unknown body site value: None, SampleID: Blank21.F9.1128737\n",
        "Unknown body site value: None, SampleID: Blank24r.D12.1123834\n",
        "Unknown body site value: None, SampleID: Blank21.H9.1128538\n",
        "Unknown body site value: None, SampleID: Blank23.B11.1128405\n",
        "Unknown body site value: None, SampleID: Blank24r.A12.1123997\n",
        "Unknown body site value: None, SampleID: Blank17.G5.1128784\n",
        "Unknown body site value: None, SampleID: Blank17.B5.1128441\n",
        "Unknown body site value: None, SampleID: 000004672a.1123965\n",
        "Unknown body site value: None, SampleID: Blank26.A1.1128771\n",
        "Unknown body site value: None, SampleID: Blank23.D11.1128547\n",
        "Unknown body site value: None, SampleID: Blank24r.C12.1124156\n",
        "Unknown body site value: None, SampleID: Blank27.E3.1128468\n",
        "Unknown body site value: None, SampleID: Blank22.F6.1128582\n",
        "Unknown body site value: None, SampleID: Blank25.B2.1128339\n",
        "Unknown body site value: None, SampleID: Blank25.D2.1128616\n",
        "Unknown body site value: None, SampleID: Blank19.G8.1123923\n",
        "Unknown body site value: None, SampleID: Blank27.C3.1128578\n",
        "Unknown body site value: None, SampleID: Blank19.D8.1123881\n",
        "Unknown body site value: None, SampleID: Blank20.H8.1124136\n",
        "Unknown body site value: None, SampleID: Blank25.E2.1128814\n",
        "Unknown body site value: None, SampleID: Blank25.G2.1128389\n",
        "Unknown body site value: None, SampleID: Blank18.H6.1123720\n",
        "Unknown body site value: None, SampleID: Blank24r.B12.1123828\n",
        "Unknown body site value: None, SampleID: Blank22.A9.1128349\n",
        "Unknown body site value: None, SampleID: Blank24r.E8.1123809\n",
        "Unknown body site value: None, SampleID: Blank18.B6.1123903\n",
        "Unknown body site value: None, SampleID: 000002877.1128808\n",
        "Unknown body site value: None, SampleID: Blank17.H5.1128455\n",
        "Unknown body site value: None, SampleID: Blank22.H6.1128451\n",
        "Unknown body site value: None, SampleID: Blank19.A8.1123962\n",
        "Unknown body site value: None, SampleID: Blank26.D1.1128638\n",
        "Unknown body site value: None, SampleID: blank20.G2.1123897\n",
        "Unknown body site value: None, SampleID: Blank26.F1.1128634\n",
        "Unknown body site value: None, SampleID: Blank21.D9.1128291\n",
        "Unknown body site value: None, SampleID: Blank22.E6.1128655\n",
        "Unknown body site value: None, SampleID: Blank19.B8.1124075\n",
        "Unknown body site value: None, SampleID: Blank19.E8.1123801\n",
        "Unknown body site value: None, SampleID: 000006931.1128585\n",
        "Unknown body site value: None, SampleID: Blank25.A2.1128524\n",
        "Unknown body site value: None, SampleID: Blank18.A6.1124067\n",
        "Unknown body site value: None, SampleID: Blank22.D6.1128333\n",
        "Unknown body site value: None, SampleID: Blank16.C7.1124083\n",
        "Unknown body site value: None, SampleID: Blank20.D8.1124002\n",
        "Unknown body site value: None, SampleID: Blank21.G6.1128542\n",
        "Unknown body site value: None, SampleID: Blank20.F8.1124066\n",
        "Unknown body site value: None, SampleID: Blank26.G1.1128286\n",
        "Unknown body site value: None, SampleID: Blank19.F8.1124010\n",
        "Unknown body site value: None, SampleID: Blank21.A9.1128343\n",
        "Unknown body site value: None, SampleID: Blank18.F6.1123711\n",
        "Unknown body site value: None, SampleID: Blank25.A11.1128595\n",
        "Unknown body site value: None, SampleID: Blank27.H3.1128457\n",
        "Unknown body site value: None, SampleID: Blank16.E7.1124120\n",
        "Unknown body site value: None, SampleID: Blank24r.E12.1123797\n",
        "Unknown body site value: None, SampleID: Blank17.F5.1128386\n",
        "Unknown body site value: None, SampleID: Blank23.C11.1128475\n",
        "Unknown body site value: None, SampleID: Blank18.D6.1124007\n",
        "Unknown body site value: None, SampleID: Blank22.A11.1128623\n",
        "Unknown body site value: None, SampleID: Blank27.A3.1128373\n",
        "Unknown body site value: None, SampleID: Blank25.F2.1128345\n",
        "Unknown body site value: None, SampleID: Blank26.C1.1128282\n",
        "Unknown body site value: None, SampleID: Blank24r.F8.1123825\n",
        "Unknown body site value: None, SampleID: Blank20.E8.1123845\n",
        "Unknown body site value: None, SampleID: XXXX20.C2.1123950\n",
        "Unknown body site value: None, SampleID: Blank26.H1.1128269\n",
        "Unknown body site value: None, SampleID: Blank26.B1.1128707\n",
        "Unknown body site value: None, SampleID: Blank21.G9.1128555\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we've massaged the metadata, we need to merge the mapping files from all the analyses."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# setup output paths, (mm -> massaged mapping)\n",
      "hmp_pgp_mm_fp = get_path('HMP_PGP_100nt_massaged.txt')\n",
      "ag_gg_mm_fp = get_path('AG_GG_100nt_massaged.txt')\n",
      "hmp_pgp_ag_gg_mm_fp = get_path('HMP_GG_AG_PGP_100nt_massaged.txt')\n",
      "\n",
      "hmp_pgp_cmd_args = {'input_a':hmp_100nt_m_massaged_fp,\n",
      "                  'input_b':pgp_100nt_m_massaged_fp,\n",
      "                  'output':hmp_pgp_mm_fp}\n",
      "\n",
      "ag_gg_cmd_args = {'input_a':ag_100nt_m_massaged_fp,\n",
      "                  'input_b':gg_100nt_m_massaged_fp,\n",
      "                  'output':ag_gg_mm_fp}\n",
      "\n",
      "hmp_pgp_ag_gg_cmd_args = {'input_a':hmp_pgp_mm_fp,\n",
      "                          'input_b':ag_gg_mm_fp,\n",
      "                          'output':hmp_pgp_ag_gg_mm_fp}\n",
      "\n",
      "# merge and block until completion\n",
      "hmp_pgp_job = submit(qiime_scripts['Merge Mapping Files'] % hmp_pgp_cmd_args)\n",
      "ag_gg_job = submit(qiime_scripts['Merge Mapping Files'] % ag_gg_cmd_args)\n",
      "jobs = wait_on([hmp_pgp_job, ag_gg_job])\n",
      "\n",
      "# merge and block until completion\n",
      "hmp_pgp_ag_gg_job = submit(qiime_scripts['Merge Mapping Files'] % hmp_pgp_ag_gg_cmd_args)\n",
      "jobs = wait_on(hmp_pgp_ag_gg_job)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 1 jobs still running, approximately  10 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to simplify compute on the downstream processes, we're going to filter out the metadata columns that we aren't interested in."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def filter_mapping_file(in_fp, out_fp, columns_to_keep):\n",
      "    \"\"\"Filter out columns in a mapping file\n",
      "\n",
      "    in_fp : the input file path\n",
      "    out_fp : the output file path\n",
      "    columns_to_keep : a dict of the columns to keep, valued by specific category value\n",
      "        if desired to filter out samples that don't meet a given criteria\n",
      "    \"\"\"\n",
      "    lines = [l.strip().split('\\t') for l in open(in_fp)]\n",
      "    header = lines[0][:]\n",
      "    header_lower = map(lambda x: x.lower(), header)\n",
      "\n",
      "    # ensure SampleID is always first\n",
      "    new_header = [\"#SampleID\"]\n",
      "    indices = [0] # always keep SampleID\n",
      "    for c in columns_to_keep:\n",
      "        if c.lower() not in header_lower:\n",
      "            raise ValueError(\"Cannot find %s!\" % c)\n",
      "        \n",
      "        indices.append(header_lower.index(c.lower()))\n",
      "        new_header.append(c)\n",
      "    columns_to_keep['#SampleID'] = None # add for consistency\n",
      "    \n",
      "    new_lines = [new_header]\n",
      "    for l in lines[1:]:\n",
      "        new_line = []\n",
      "        \n",
      "        keep = True\n",
      "        # fetch values from specific columns\n",
      "        for column, index in zip(new_header, indices):\n",
      "            value = l[index] \n",
      "            if columns_to_keep[column] is None:\n",
      "                new_line.append(value)  \n",
      "            elif not columns_to_keep[column](value):\n",
      "                keep = False\n",
      "                break\n",
      "            else:\n",
      "                new_line.append(value)\n",
      "        \n",
      "        if keep:\n",
      "            new_lines.append(new_line)\n",
      "        \n",
      "    out = open(out_fp, 'w')\n",
      "    out.write('\\n'.join(['\\t'.join(l) for l in new_lines]))\n",
      "    out.write('\\n')\n",
      "    out.close()\n",
      "\n",
      "def test_filter_mapping_file():\n",
      "    # test output file\n",
      "    fmf_TEST_fp = get_path('AG_GG_fmf_TEST.txt')\n",
      "    \n",
      "    # filter to just fecal samples, keep the age and title_acronym columns\n",
      "    filter_mapping_file(ag_gg_mm_fp, fmf_TEST_fp, {'SIMPLE_BODY_SITE':lambda x: x == 'FECAL', 'AGE':None, 'TITLE_ACRONYM':None})\n",
      "    \n",
      "    # parse output\n",
      "    test_mapping = [l.strip().split('\\t') for l in open(fmf_TEST_fp)]\n",
      "    \n",
      "    # fish header, verify sanity of it\n",
      "    test_header = test_mapping[0]\n",
      "    assert len(test_header) == 4\n",
      "    assert test_header[0] == '#SampleID'\n",
      "    assert sorted(test_header) == sorted(['#SampleID','SIMPLE_BODY_SITE','AGE','TITLE_ACRONYM'])\n",
      "    \n",
      "    # check each record\n",
      "    test_sbs = test_header.index('SIMPLE_BODY_SITE')\n",
      "    test_acro = test_header.index('TITLE_ACRONYM')\n",
      "    for l in test_mapping[1:]:\n",
      "        assert len(l) == 4\n",
      "        assert l[test_sbs] == 'FECAL'\n",
      "        assert l[test_acro] in ['AGP','GG']   \n",
      "\n",
      "# uncomment the next line to test filter_mapping_file\n",
      "test_filter_mapping_file()\n",
      "fig1_m_fp = get_path('HMP_GG_AG_PGP_figure1.txt')\n",
      "fig2_m_fp = get_path('AG_GG_fecal_figure2.txt')\n",
      "fig3_m_fp = get_path('AG_fecal_figure3.txt')\n",
      "fig4_m_fp = get_path('AG_fecal_figure4.txt')\n",
      "\n",
      "filter_mapping_file(hmp_pgp_ag_gg_mm_fp, fig1_m_fp, {'TITLE_ACRONYM':None, 'SIMPLE_BODY_SITE':None, 'TITLE_BODY_SITE':None, 'HMP_SITE':None})\n",
      "filter_mapping_file(ag_gg_mm_fp, fig2_m_fp, {'TITLE_ACRONYM':None, 'AGE':lambda x: x != 'None', 'SIMPLE_BODY_SITE':lambda x: x == 'FECAL', 'COUNTRY':None})\n",
      "filter_mapping_file(ag_100nt_m_massaged_fp, fig3_m_fp, {'TITLE_ACRONYM':None, 'SIMPLE_BODY_SITE':lambda x: x == 'FECAL'})\n",
      "filter_mapping_file(ag_100nt_m_massaged_fp, fig4_m_fp, {'TITLE_ACRONYM':None, 'AGE_CATEGORY':None, 'SEX':None, 'BMI_CATEGORY':None, 'DIET_TYPE':None, 'SIMPLE_BODY_SITE':lambda x: x == 'FECAL'})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the figures, we'll need to merge and filter the tables in a few ways. For figure 1, we want to place the American Gut population in the context of other large studies. To do so, we need to first merge the tables together. Since there are 4 tables to merge, we need to use two merge calls. (It is also feasible to use QIIME's parallel_merge_otu_tables.py here as well). Figure 2 is a combination of Global Gut and the American Gut, but only fecal samples, as is figure 3. Note that for figure 3, we're using the full American Gut table and not the 100 nucleotide version. Since this table is not being combined with the HiSeq data in the Global Gut, we can get away with retaining the full read for added specificity. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# resulting paths\n",
      "hmp_pgp_t_fp = get_path(\"HMP_PGP_100nt.biom\")\n",
      "ag_gg_t_fp = get_path(\"AG_GG_100nt.biom\")\n",
      "hmp_gg_ag_pgp_t_fp = get_path(\"HMP_GG_AG_PGP_100nt.biom\")\n",
      "\n",
      "# setup the command arguments for each call\n",
      "hmp_pgp_cmd_args = {'input_a':hmp_100nt_t_fp, \n",
      "                   'input_b':pgp_100nt_t_fp,\n",
      "                   'output':hmp_pgp_t_fp}\n",
      "ag_gg_cmd_args = {'input_a':ag_100nt_t_fp,\n",
      "                   'input_b':gg_100nt_t_fp,\n",
      "                   'output':ag_gg_t_fp}\n",
      "hmp_gg_ag_pgp_cmd_args = {'input_a':ag_gg_t_fp,\n",
      "                          'input_b':hmp_pgp_t_fp,\n",
      "                          'output':hmp_gg_ag_pgp_t_fp}\n",
      "\n",
      "# merge and block until completion\n",
      "hmp_pgp_job = submit(qiime_scripts['Merge OTU Tables'] % hmp_pgp_cmd_args)\n",
      "ag_gg_job = submit(qiime_scripts['Merge OTU Tables'] % ag_gg_cmd_args)\n",
      "jobs = wait_on([hmp_pgp_job, ag_gg_job])\n",
      "\n",
      "# merge and block until completion\n",
      "hmp_gg_ag_pgp_job = submit(qiime_scripts['Merge OTU Tables'] % hmp_gg_ag_pgp_cmd_args)\n",
      "jobs = wait_on(hmp_gg_ag_pgp_job)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 1 jobs still running, approximately  885 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that the data are combined, we need to rarify them in order to normalize for sequencing effort. The Human Microbiome Project has the shallowest coverage per sample, and when operating with the HMP dataset, a rarifaction depth of 1,000 is a reasonable balance between effort and retaining sufficient numbers of samples. The Global Gut and American Gut Project have much deeper coverage per sample (particularly in the case of the Global Gut) and as such, we will rarify at 10,000 sequences per sample for those tables."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# resulting path\n",
      "hmp_gg_ag_pgp_t_1k_fp = get_path(\"HMP_GG_AG_PGP_100nt_even1k.biom\")\n",
      "\n",
      "# setup the command arguments\n",
      "hmp_gg_ag_pgp_t_1k_cmd_args = {'input':hmp_gg_ag_pgp_t_fp,\n",
      "                               'output':hmp_gg_ag_pgp_t_1k_fp,\n",
      "                               'depth':'1000'}\n",
      "\n",
      "# rarifiy and block until completion\n",
      "hmp_gg_ag_pgp_t_1k_job = submit(qiime_scripts['Single Rarifaction'] % hmp_gg_ag_pgp_t_1k_cmd_args)\n",
      "res = wait_on(hmp_gg_ag_pgp_t_1k_job)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 1 jobs still running, approximately  590 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have our rarified table, that combines the AG, PGP, HMP, and GG datasets, and a merged mapping file, we can now compute the beta diversity of this OTU table. This step is computationally demanding, and will run for a few hours on a 100 processors. **Note, the final merge job for parallel beta diversity requires > 8GB of RAM**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# verify we have the files we need to operate on\n",
      "check_file(hmp_gg_ag_pgp_t_1k_fp); check_file(fig1_m_fp)\n",
      "    \n",
      "# setup output directory\n",
      "bdiv_dir = lambda x: 'bdiv_' + os.path.basename(x).split('.',1)[0]\n",
      "hmp_gg_ag_pgp_1k_unweighted_unifrac_d = get_path(bdiv_dir(hmp_gg_ag_pgp_t_1k_fp))\n",
      " \n",
      "# setup beta diversity arguments\n",
      "prefix = 'ag2_bdiv_'\n",
      "hmp_gg_ag_pgp_cmd_args = {'input':hmp_gg_ag_pgp_t_1k_fp,\n",
      "                          'output':hmp_gg_ag_pgp_1k_unweighted_unifrac_d,\n",
      "                          'job_prefix':prefix,\n",
      "                          'num_jobs':NUM_PROCS,\n",
      "                          'gg97_tree':greengenes135_97_tree_fp}\n",
      "\n",
      "# submit and wait\n",
      "hmp_gg_ag_pgp_bdiv_job = submit(qiime_scripts['Parallel Beta Diversity'] % hmp_gg_ag_pgp_cmd_args)\n",
      "res = wait_on(hmp_gg_ag_pgp_bdiv_job, additional_prefix=prefix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "54 / 100 jobs still running, approximately  5645 seconds elapsed\n"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have calculated beta diversity for all of the samples, we need to filter the table down to the subsets that we're interested in. For figure 1 we want to use all of the samples. But, for figure 2 and 3, we only want to use subsets of the full distance matrix."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####bdiv_path = lambda x: os.path.join(x, \"unweighted_unifrac_%s.txt\" % os.path.basename(x).split('_unweighted',1)[0])\n",
      "\n",
      "### actually determine the path here...\n",
      "bdiv_path = get_path('bdiv_HMP_GG_AG_PGP_100nt_even1k/unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k.txt') \n",
      "check_file(bdiv_path)\n",
      "\n",
      "full_bdiv = bdiv_path\n",
      "fig_path = lambda x: full_bdiv.rsplit('.txt',1)[0] + '-' + x + '.txt'\n",
      "fig1_bdiv = fig_path('fig1')\n",
      "fig2_bdiv = fig_path('fig2')\n",
      "fig3_bdiv = fig_path('fig3')\n",
      "\n",
      "fig1_cmd_args = {'input':full_bdiv,\n",
      "                 'output':fig1_bdiv,\n",
      "                 'sample_ids':fig1_m_fp}\n",
      "fig2_cmd_args = {'input':full_bdiv,\n",
      "                 'output':fig2_bdiv,\n",
      "                 'sample_ids':fig2_m_fp}\n",
      "fig3_cmd_args = {'input':full_bdiv,\n",
      "                 'output':fig3_bdiv,\n",
      "                 'sample_ids':fig3_m_fp}\n",
      "\n",
      "jobs = []\n",
      "jobs.append(submit(qiime_scripts['Filter Distance Matrix'] % fig1_cmd_args))\n",
      "jobs.append(submit(qiime_scripts['Filter Distance Matrix'] % fig2_cmd_args))\n",
      "jobs.append(submit(qiime_scripts['Filter Distance Matrix'] % fig3_cmd_args))\n",
      "res = wait_on(jobs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 3 jobs still running, approximately  155 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have our Unifrac distance matrices, need to transform them into principal coordinates space."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pc_path = lambda x: x.rsplit('.txt',1)[0] + '_pc.txt'\n",
      "\n",
      "# verify the expected files are present\n",
      "check_file(fig1_bdiv)\n",
      "check_file(fig2_bdiv)\n",
      "check_file(fig3_bdiv)\n",
      "\n",
      "fig1_pc = pc_path(fig1_bdiv)\n",
      "fig2_pc = pc_path(fig2_bdiv)\n",
      "fig3_pc = pc_path(fig3_bdiv)\n",
      "\n",
      "# setup our arguments\n",
      "fig1_cmd_args = {'input':fig1_bdiv,\n",
      "                 'output':fig1_pc}\n",
      "fig2_cmd_args = {'input':fig2_bdiv,\n",
      "                 'output':fig2_pc}\n",
      "fig3_cmd_args = {'input':fig3_bdiv,\n",
      "                 'output':fig3_pc}\n",
      "\n",
      "# submit the jobs\n",
      "jobs = []\n",
      "jobs.append(submit(qiime_scripts['Principal Coordinates'] % fig1_cmd_args))\n",
      "jobs.append(submit(qiime_scripts['Principal Coordinates'] % fig2_cmd_args))\n",
      "jobs.append(submit(qiime_scripts['Principal Coordinates'] % fig3_cmd_args))\n",
      "job_results = wait_on(jobs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 / 3 jobs still running, approximately  125 seconds elapsed\n"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now to actually make figures 1, 2, and 3 using [Emperor](http://qiime.org/emperor/), a WebGL-based Principal Coordinates viewer. While Emperor is the only tool that we're aware of that can effectively scale to these size datasets for 3D visualization and painting of arbitrary metadata, the tie to WebGL makes its use here a little bit of a challenge. Specifically, we'll be able to generate the plots, but we cannot automatically generate the images from the notebook. First, lets get Emperor up and running, in the following cell, we'll describe how what needs to happen to produce the images."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# quick little helper method\n",
      "emp_path = lambda x: x.rsplit('.txt',1)[0] + '-emp'\n",
      "\n",
      "# verify expected files are present\n",
      "check_file(fig1_pc)\n",
      "check_file(fig2_pc)\n",
      "check_file(fig3_pc)\n",
      "\n",
      "# setup output paths\n",
      "fig1_emp = emp_path(fig1_pc)\n",
      "fig2_emp = emp_path(fig2_pc)\n",
      "fig3_emp = emp_path(fig3_pc)\n",
      "fig3_filter = get_path(\"figure3.biom\")\n",
      "fig3_taxa = get_path(\"figure3_taxa\")\n",
      "fig3_taxa_mapping_fp = os.path.join(fig3_taxa, os.path.splitext(os.path.basename(fig3_m_fp))[0] + '_L2.txt')\n",
      "\n",
      "# setup arguments\n",
      "fig3_filter_args = {'input':hmp_gg_ag_pgp_t_1k_fp,\n",
      "                    'output':fig3_filter,\n",
      "                    'sample_id_fp':fig3_m_fp}\n",
      "fig3_summarize_args = {'input':fig3_filter,\n",
      "                       'output':fig3_taxa,\n",
      "                       'level':'2',\n",
      "                       'mapping':fig3_m_fp}\n",
      "\n",
      "fig1_cmd_args = {'input':fig1_pc, \n",
      "                 'output':fig1_emp, \n",
      "                 'mapping':fig1_m_fp}\n",
      "fig2_cmd_args = {'input':fig2_pc, \n",
      "                 'output':fig2_emp, \n",
      "                 'mapping':fig2_m_fp}\n",
      "fig3_cmd_args = {'input':fig3_pc, \n",
      "                 'output':fig3_emp, \n",
      "                 'mapping':fig3_taxa_mapping_fp}\n",
      "\n",
      "\n",
      "# filter the table down to just the AG fecal samples\n",
      "filter_job = submit(qiime_scripts['Filter Samples'] % fig3_filter_args)\n",
      "res = wait_on(filter_job)\n",
      "\n",
      "taxa_job = submit(qiime_scripts['Summarize Taxa Mapping'] % fig3_summarize_args)\n",
      "res = wait_on(taxa_job)\n",
      "\n",
      "check_file(fig3_taxa_mapping_fp)\n",
      "\n",
      "jobs = []\n",
      "jobs.append(submit(other_scripts['Make Emperor'] % fig1_cmd_args))\n",
      "jobs.append(submit(other_scripts['Make Emperor'] % fig2_cmd_args))\n",
      "jobs.append(submit(other_scripts['Make Emperor'] % fig3_cmd_args))\n",
      "res = wait_on(jobs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 3 jobs still running, approximately  105 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here's the required manual intervention:\n",
      "\n",
      "1. Click on the first URL link in the resulting pane\n",
      "2. Rotate the view to your a nice perspective\n",
      "3. Click the \"Options\" tab on the right side, set a filename prefix (e.g., figure1) and then click on the \"Multishot\" button. **NOTE: this will produce a lot of files on to your LOCAL computer!**\n",
      "4. Wait until all of the images have downloaded\n",
      "5. Pack up the local image files:\n",
      "  - Change directories on your LOCAL computer to your downloads folder\n",
      "  - Execute (make sure to replace **prefix** with the prefix specified in 3): tar czf **prefix**.tar.gz **prefix**.*\n",
      "      - If to many files, you can do: \"find . -name \"Figure_1.*\" -type f -print0 | tar czf Figure_1.tar.gz -T - --null\"\n",
      "      - (adapted from: http://stackoverflow.com/questions/5891866/find-files-and-tar-them-with-spaces)\n",
      "6. Copy the **prefix**.tar.gz file back to your compute resource, and place it in your home directory (~/ or $HOME)\n",
      "7. repeat for each figure\n",
      "\n",
      "If the figures are redone, then you may need to clear your web-browsers cache befone the HTML links below will work correct. Optionally, you could hit refresh a few times after clicking the links as well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.lib.display import FileLink\n",
      "\n",
      "emp_index = lambda x: os.path.join(x, 'index.html')\n",
      "\n",
      "# form the expected paths for Emperor\n",
      "fig1 = emp_index(fig1_emp)\n",
      "fig2 = emp_index(fig2_emp)\n",
      "fig3 = emp_index(fig3_emp)\n",
      "\n",
      "# verify the expected files are present\n",
      "check_file(fig1)\n",
      "check_file(fig2)\n",
      "check_file(fig3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FileLink(fig1, result_html_prefix='<p>Figure 1:</p>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<p>Figure 1:</p><a href='files//home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_4sy/bdiv_HMP_GG_AG_PGP_100nt_even1k/unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k-fig1_pc-emp/index.html' target='_blank'>/home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_4sy/bdiv_HMP_GG_AG_PGP_100nt_even1k/unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k-fig1_pc-emp/index.html</a><br>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 49,
       "text": [
        "/home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_4sy/bdiv_HMP_GG_AG_PGP_100nt_even1k/unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k-fig1_pc-emp/index.html"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Figure 2 is slightly different: we need to produce a global shot with the uninformative ages (None/NA/0.0) removed using the mockup functionality, then we need to unselect mockup, change the uninformative ages to grey, and do a full image dump. The motivation is that the background image will not contain the samples missing the ages, but we can still display them for each populated template. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FileLink(fig2, result_html_prefix='<p>Figure 2:</p>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<p>Figure 2:</p><a href='files//home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_4sy/bdiv_HMP_GG_AG_PGP_100nt_even1k/unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k-fig2_pc-emp/index.html' target='_blank'>/home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_4sy/bdiv_HMP_GG_AG_PGP_100nt_even1k/unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k-fig2_pc-emp/index.html</a><br>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "/home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_4sy/bdiv_HMP_GG_AG_PGP_100nt_even1k/unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k-fig2_pc-emp/index.html"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FileLink(fig3, result_html_prefix='<p>Figure 3:</p>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<p>Figure 3:</p><a href='files//home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_4sy/bdiv_HMP_GG_AG_PGP_100nt_even1k/unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k-fig3_pc-emp/index.html' target='_blank'>/home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_4sy/bdiv_HMP_GG_AG_PGP_100nt_even1k/unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k-fig3_pc-emp/index.html</a><br>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "/home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_4sy/bdiv_HMP_GG_AG_PGP_100nt_even1k/unweighted_unifrac_HMP_GG_AG_PGP_100nt_even1k-fig3_pc-emp/index.html"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once the images are created, we need to copy them to our working area. It is possible file paths will be different, in which case, you may need to change the DOWNLOAD_DIRECTORY variable in the next cell."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DOWNLOAD_DIRECTORY = os.path.expandvars('$HOME')\n",
      "FIGURE1_EXPECTED_FILENAME = 'Figure_1.tar.gz'\n",
      "FIGURE2_EXPECTED_FILENAME = 'Figure_2.tar.gz'\n",
      "FIGURE3_EXPECTED_FILENAME = 'Figure_3.tar.gz'\n",
      "\n",
      "# helper function for creating wildcard paths\n",
      "source_path = lambda x,y: os.path.join(x,y)\n",
      "\n",
      "# setup the destination paths\n",
      "emperor_images = get_path('emperor_images_svg')\n",
      "\n",
      "if not os.path.exists(emperor_images):\n",
      "    os.mkdir(emperor_images)\n",
      "\n",
      "# setup the source paths\n",
      "figure1_src = source_path(DOWNLOAD_DIRECTORY, FIGURE1_EXPECTED_FILENAME)\n",
      "figure2_src = source_path(DOWNLOAD_DIRECTORY, FIGURE2_EXPECTED_FILENAME)\n",
      "figure3_src = source_path(DOWNLOAD_DIRECTORY, FIGURE3_EXPECTED_FILENAME)\n",
      "\n",
      "check_file(figure1_src)\n",
      "check_file(figure2_src)\n",
      "check_file(figure3_src)\n",
      "\n",
      "# unpack the tarballs\n",
      "jobs = []\n",
      "jobs.append(submit('tar xzf %s -C %s' % (figure1_src, emperor_images)))\n",
      "jobs.append(submit('tar xzf %s -C %s' % (figure2_src, emperor_images)))\n",
      "jobs.append(submit('tar xzf %s -C %s' % (figure3_src, emperor_images)))\n",
      "res = wait_on(jobs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 3 jobs still running, approximately  285 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we need to produce the actual PDF files of the PCoA plots that will go into the individualized results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_ag_IDs = set([l.strip().split('\\t')[0] for l in open(ag_m_fp) if not l.startswith('#')])\n",
      "all_emp_SVGs = !ls $emperor_images\n",
      "template_files = get_path('template_files')\n",
      "\n",
      "if not os.path.exists(template_files):\n",
      "    os.mkdir(template_files)\n",
      "\n",
      "svg_smash_args = {'input':emperor_images, \n",
      "                  'output':template_files, \n",
      "                  'prefix':None, \n",
      "                  'sample_id':None}\n",
      "\n",
      "commands = []\n",
      "for f in all_emp_SVGs:\n",
      "    prefix, remainder = f.split('.',1)\n",
      "    \n",
      "    try:\n",
      "        id_, remainder = remainder.rsplit('_',1)\n",
      "    except:\n",
      "        # GLOBAL SVG for each figure\n",
      "        assert remainder == 'GLOBAL'\n",
      "        continue\n",
      "        \n",
      "    # delete svgs for non-AG points    \n",
      "    if id_ not in all_ag_IDs:\n",
      "        #!rm $f \n",
      "        continue\n",
      "        \n",
      "    args = svg_smash_args.copy()\n",
      "    args['sample_id'] = id_\n",
      "    args['prefix'] = prefix\n",
      "    commands.append(other_scripts['SVG Smash'] % args)\n",
      "\n",
      "# submit SVG->PDF processing in batches of chunk_size\n",
      "res = farm_commands(commands, 25)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 182 jobs still running, approximately  5290 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now lets create the phylum level taxonomy summary plots."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# summarize over the categories\n",
      "fig4_sex_fp = get_path('fig4_sex.biom')\n",
      "fig4_age_fp = get_path('fig4_age_category.biom')\n",
      "fig4_diet_fp = get_path('fig4_diet.biom')\n",
      "fig4_bmi_fp = get_path('fig4_bmi_category.biom')\n",
      "ag_fecal_t_1k_fp = get_path('ag_fecal_even1k.biom')\n",
      "template_files = get_path('template_files')\n",
      "\n",
      "check_file(hmp_gg_ag_pgp_t_1k_fp)\n",
      "check_file(fig4_m_fp)\n",
      "\n",
      "filter_args = {'input':hmp_gg_ag_pgp_t_1k_fp,\n",
      "               'output':ag_fecal_t_1k_fp,\n",
      "               'sample_id_fp':fig4_m_fp}\n",
      "\n",
      "otu_by_cat_sex_args = {'otu_table':ag_fecal_t_1k_fp,\n",
      "                       'output':fig4_sex_fp,\n",
      "                       'mapping':fig4_m_fp,\n",
      "                       'category':'SEX'}\n",
      "otu_by_cat_age_args = {'otu_table':ag_fecal_t_1k_fp,\n",
      "                       'output':fig4_age_fp,\n",
      "                       'mapping':fig4_m_fp,\n",
      "                       'category':'AGE_CATEGORY'}\n",
      "otu_by_cat_diet_args = {'otu_table':ag_fecal_t_1k_fp,\n",
      "                       'output':fig4_diet_fp,\n",
      "                       'mapping':fig4_m_fp,\n",
      "                       'category':'DIET_TYPE'}\n",
      "otu_by_cat_bmi_args = {'otu_table':ag_fecal_t_1k_fp,\n",
      "                       'output':fig4_bmi_fp,\n",
      "                       'mapping':fig4_m_fp,\n",
      "                       'category':'BMI_CATEGORY'}\n",
      "\n",
      "phyla_plot_args = {'input':ag_fecal_t_1k_fp,\n",
      "                   'output':template_files,\n",
      "                   'mapping':fig4_m_fp,\n",
      "                   'categories':'SEX:%s, AGE_CATEGORY:%s, DIET_TYPE:%s, BMI_CATEGORY:%s' % (fig4_sex_fp, fig4_age_fp, fig4_diet_fp, fig4_bmi_fp)}\n",
      "                \n",
      "# filter the ag table down to just the fecal samples for fig 4\n",
      "filter_job = submit(qiime_scripts['Filter Samples'] % filter_args)\n",
      "res = wait_on(filter_job)\n",
      "\n",
      "# get the summarized tables\n",
      "jobs = []\n",
      "jobs.append(submit(qiime_scripts['Summarize OTU by Category'] % otu_by_cat_sex_args))\n",
      "jobs.append(submit(qiime_scripts['Summarize OTU by Category'] % otu_by_cat_age_args))\n",
      "jobs.append(submit(qiime_scripts['Summarize OTU by Category'] % otu_by_cat_diet_args))\n",
      "jobs.append(submit(qiime_scripts['Summarize OTU by Category'] % otu_by_cat_bmi_args))\n",
      "res = wait_on(jobs)\n",
      "\n",
      "# farm out the phyla plots\n",
      "chunk_size = 25\n",
      "start = 0\n",
      "jobs = []\n",
      "sample_ids = [i.strip().split('\\t')[0] for i in open(fig4_m_fp) if not i.startswith('#')]\n",
      "for end in range(chunk_size, len(sample_ids) + chunk_size, chunk_size):\n",
      "    chunk = sample_ids[start:end]\n",
      "    start = end\n",
      "    args = phyla_plot_args.copy()\n",
      "    args['samples'] = ','.join(chunk)\n",
      "    jobs.append(submit(other_scripts['Make Phyla Plots'] % args)) \n",
      "res = wait_on(jobs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 61 jobs still running, approximately  500 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And for the last figure, lets create the over represented taxonomy table."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig5_tax_sum = get_path('ag_fecal_even1k_taxasum_L6')\n",
      "fig5_lvl6 = get_path('ag_fecal_even1k_taxasum_L6/ag_fecal_even1k_L6.biom')\n",
      "template_files = get_path('template_files')\n",
      "\n",
      "check_file(ag_fecal_t_1k_fp)\n",
      "\n",
      "sum_args = {'input':ag_fecal_t_1k_fp,\n",
      "            'output':fig5_tax_sum,\n",
      "            'level':\"6\"}\n",
      "sig_args = {'input':fig5_lvl6,\n",
      "            'output':template_files}\n",
      "\n",
      "sum_job = submit(qiime_scripts['Summarize Taxa'] % sum_args)\n",
      "res = wait_on(sum_job)\n",
      "\n",
      "check_file(fig5_lvl6)\n",
      "\n",
      "sig_job = submit(other_scripts['OTU Significance'] % sig_args) \n",
      "res = wait_on(sig_job)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 / 1 jobs still running, approximately  2255 seconds elapsed\n"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets also dump out the per sample taxonomy information that we can distribute on the participant website."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# per sample taxonomy summary\n",
      "from biom.parse import parse_biom_table\n",
      "full_table = get_path('ag_fecal_even1k_taxasum_L6/ag_fecal_even1k_L6.txt')\n",
      "t = parse_biom_table(open(fig5_lvl6))\n",
      "base_persample_path = get_path('template_files/Figure_6_%s.txt')\n",
      "header = \"#taxon\\trelative_abundance\\n\"\n",
      "for v,id_,md in t.iterSamples():\n",
      "    f = open(base_persample_path % id_, 'w')\n",
      "    f.write(header)\n",
      "    for sorted_v, taxa in sorted(zip(v, t.ObservationIds))[::-1]:\n",
      "        if sorted_v:\n",
      "            f.write(\"%s\\t%f\\n\" % (taxa, sorted_v))\n",
      "    f.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we construct a method that will return the necessary commands to populate a template per sample."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MissingFigure(Exception):\n",
      "    pass\n",
      "\n",
      "def bootstrap_result(sample_id, name):\n",
      "    \"\"\"Stage for results\"\"\"\n",
      "    if name is None:\n",
      "        bootstrap_path = lambda x: os.path.join(get_path('unidentified'), x)\n",
      "    else:\n",
      "        bootstrap_path = lambda x: os.path.join(get_path('identified'), x)\n",
      "        \n",
      "    indiv_dir = bootstrap_path(i)\n",
      "    pdf_dir = os.path.join(indiv_dir, 'pdfs')\n",
      "    tex_path = lambda x: os.path.join(indiv_dir, x)\n",
      "    fig_pdf_path = lambda x: os.path.join(pdf_dir, x)\n",
      "    template_files_path = lambda x: os.path.join(get_path('template_files'), x)\n",
      "    \n",
      "    fig1_src   = template_files_path(\"Figure_1.%s_huge.pdf\" % i)\n",
      "    fig2_src   = template_files_path(\"Figure_2.%s_huge.pdf\" % i)\n",
      "    fig3_src   = template_files_path(\"Figure_3.%s_huge.pdf\" % i)\n",
      "    fig4_src   = template_files_path(\"Figure_4_%s.pdf\" % i)\n",
      "    fig6_src   = template_files_path(\"Figure_6_%s.txt\" % i)\n",
      "    macros_src = template_files_path(\"macros_%s.tex\" % i)\n",
      "    \n",
      "    fig1_dst   = fig_pdf_path(\"figure1.pdf\")\n",
      "    fig2_dst   = fig_pdf_path(\"figure2.pdf\")\n",
      "    fig3_dst   = fig_pdf_path(\"figure3.pdf\")\n",
      "    fig4_dst   = fig_pdf_path(\"figure4.pdf\")\n",
      "    fig6_dst   = tex_path(\"%s_taxa.txt\" % sample_id)\n",
      "    macros_dst   = tex_path(\"macros.tex\")\n",
      "    template_dst = tex_path('%s.tex' % sample_id)\n",
      "    \n",
      "    if not os.path.exists(fig1_src): \n",
      "        raise MissingFigure(\"Missing fig1 for %s\" % i)\n",
      "    if not os.path.exists(fig2_src): \n",
      "        raise MissingFigure(\"Missing fig2 for %s\" % i)\n",
      "    if not os.path.exists(fig3_src): \n",
      "        raise MissingFigure(\"Missing fig3 for %s\" % i)\n",
      "    if not os.path.exists(fig4_src): \n",
      "        raise MissingFigure(\"Missing fig4 for %s\" % i)\n",
      "    if not os.path.exists(fig6_src):\n",
      "        raise MissingFigure(\"Missing fig6 for %s\" % i)\n",
      "    if not os.path.exists(macros_src):\n",
      "        raise MissingFigure(\"Missing macros for %s\" % i)\n",
      "    \n",
      "    cmds = []\n",
      "    cmds.append('mkdir -p %s' % pdf_dir)\n",
      "    cmds.append('cp %s %s' % (fig1_src, fig1_dst))\n",
      "    cmds.append('cp %s %s' % (fig2_src, fig2_dst))\n",
      "    cmds.append('cp %s %s' % (fig3_src, fig3_dst))\n",
      "    cmds.append('cp %s %s' % (fig4_src, fig4_dst))\n",
      "    cmds.append('cp %s %s' % (fig6_src, fig6_dst))\n",
      "    cmds.append('cp %s %s' % (macros_src, macros_dst))\n",
      "    cmds.append('cp %s %s' % (template, template_dst))\n",
      "    cmds.append('cp %s %s' % (aglogo, pdf_dir))\n",
      "    cmds.append('cp %s %s' % (header, pdf_dir))\n",
      "    cmds.append('cp %s %s' % (fig1_legend, pdf_dir))\n",
      "    cmds.append('cp %s %s' % (fig2_legend, pdf_dir))\n",
      "    cmds.append('cp %s %s' % (fig2_2ndlegend, pdf_dir))\n",
      "    cmds.append('cp %s %s' % (fig3_legend, pdf_dir))\n",
      "    cmds.append('cp %s %s' % (fig4_overlay, pdf_dir))\n",
      "    cmds.append('cp %s %s' % (fig1_ovals, pdf_dir))\n",
      "    cmds.append('cp %s %s' % (fig2_ovals, pdf_dir))\n",
      "    cmds.append('cp %s %s' % (ball_legend, pdf_dir))\n",
      "    cmds.append('cp %s %s' % (title, pdf_dir))\n",
      "    \n",
      "    if name is None:\n",
      "        cmds.append(\"echo '\\n\\def\\yourname{%s}\\n' >> %s\" % (\"unidentified\", macros_dst))\n",
      "    else:\n",
      "        cmds.append(\"echo '\\n\\def\\yourname{%s}\\n' >> %s\" % (name, macros_dst))\n",
      "    \n",
      "    indiv_cmd = base_setup_cmd % (working_dir, '; '.join(cmds))\n",
      "    latex_cmd = other_scripts['To PDF'] % {'path':indiv_dir, 'input':template_dst}\n",
      "    \n",
      "    return (indiv_cmd, latex_cmd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now lets build up the list of commands that will populate the templates per sample."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "check_file(ag_100nt_m_massaged_fp)\n",
      "check_file(template)\n",
      "\n",
      "strip_ext = lambda x: os.path.splitext(x)[0]\n",
      "base_setup_cmd = 'cd %s; %s'\n",
      "indiv_cmds = []\n",
      "latex_cmds = []\n",
      "ids = [l.strip().split('\\t')[0] for l in open(ag_100nt_m_massaged_fp) if not l.startswith('#')]\n",
      "\n",
      "missing_figures = []\n",
      "for i in ids:\n",
      "    sample_id = i.split('.')[0]\n",
      "    name = None\n",
      "    if participants is not None:\n",
      "        bc = i.split('.')[0]\n",
      "        if bc in participants:\n",
      "            name = participants[bc]\n",
      "\n",
      "    # unidentified\n",
      "    try:\n",
      "        indiv_cmd, latex_cmd = bootstrap_result(sample_id, None)\n",
      "    except MissingFigure, e:\n",
      "        missing_figures.append(e.message)\n",
      "        continue\n",
      "        \n",
      "    indiv_cmds.append(indiv_cmd)\n",
      "    latex_cmds.append(latex_cmd)\n",
      "    \n",
      "    # identified\n",
      "    if name:\n",
      "        # should not be necessary to wrap try/except here\n",
      "        indiv_cmd, latex_cmd = bootstrap_result(sample_id, name)\n",
      "        indiv_cmds.append(indiv_cmd)\n",
      "        latex_cmds.append(latex_cmd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And finally, we farm out the commands to populate the templates."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = farm_commands(indiv_cmds, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 277 jobs still running, approximately  450 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = farm_commands(latex_cmds, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 277 jobs still running, approximately  560 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 114
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These last few cells \"harvest\" the populated templates, which places them in a single folder. We can then \"smash\" or combine multiple templates together to simplify the results printing process."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "def harvest(path):\n",
      "    \"\"\"harvest PDFs\"\"\"\n",
      "    harvest_path = os.path.join(path, 'harvested')\n",
      "    if not os.path.exists(harvest_path):\n",
      "        !mkdir $harvest_path\n",
      "    \n",
      "    for dirpath, dirnames, filenames in os.walk(path):\n",
      "        try:\n",
      "            dirnames.remove('harvested')\n",
      "        except ValueError:\n",
      "            pass\n",
      "        \n",
      "        try:\n",
      "            dirnames.remove('pdfs')\n",
      "        except ValueError:\n",
      "            pass\n",
      "        \n",
      "        sample_suffix = re.search('\\d+\\.\\d+', dirpath)\n",
      "        if sample_suffix is None:\n",
      "            continue\n",
      "        else:\n",
      "            sample = sample_suffix.group().split('.')[0]\n",
      "        \n",
      "        pdf = os.path.join(path, dirpath, \"%s.pdf\" % sample)\n",
      "        if os.path.exists(pdf):\n",
      "            yield \"mv %s %s\" % (pdf, harvest_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = farm_commands(harvest('/home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_4sy/identified/'), 50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 28 jobs still running, approximately  15 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 116
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = farm_commands(harvest('/home/mcdonadt/ResearchWork/American-Gut/ipynb/ag_mod2_4sy/unidentified/'), 50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 28 jobs still running, approximately  10 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pdf_smash(path, tag, n_per_result=30, previously_printed=None):\n",
      "    \"\"\"Combine sets of PDFs into single documents\"\"\"\n",
      "    if previously_printed is None:\n",
      "        previously_printed = set([])\n",
      "        \n",
      "    files = []\n",
      "    for f in os.listdir(path):\n",
      "        bc, extension = os.path.splitext(f)
      "        if extension != '.pdf':\n",
      "            continue\n",
      "        if bc in previously_printed:\n",
      "            continue\n",
      "        files.append(os.path.join(path, f))\n",
      "        \n",
      "    result_path = os.path.join(path, 'pdf_smash')\n",
      "    !mkdir $result_path\n",
      "    \n",
      "    # sort and then filter out. filtering is by barcode without prefix\n",
      "    files_ordered = sorted(files, key=lambda x: int(x.rsplit('/')[-1].split('.')[0]))\n",
      "    \n",
      "    start = 0\n",
      "    smash_set = []\n",
      "    barcode_set = []\n",
      "    for end in range(n_per_result, len(files_ordered) + n_per_result, n_per_result):\n",
      "        chunk = files_ordered[start:end]\n",
      "        start = end\n",
      "        smash_set.append(' '.join(chunk)) \n",
      "        barcode_set.append('\\n'.join([f.rsplit('/')[-1].split('.')[0] for f in chunk]))\n",
      "    \n",
      "    smash_cmds = []\n",
      "    smash_basename = os.path.join(result_path, \"%s_smashset_%d\") \n",
      "    for set_number, (pdfs, barcodes) in enumerate(zip(smash_set, barcode_set)):\n",
      "        filename_base = smash_basename % (tag, set_number)\n",
      "        filename_pdf = filename_base + '.pdf'\n",
      "        filename_txt = filename_base + '.txt'\n",
      "        \n",
      "        f = open(filename_txt, 'w')\n",
      "        f.write(barcodes)\n",
      "        f.write('\\n')\n",
      "        f.close()\n",
      "        \n",
      "        smash_cmds.append(other_scripts['PDF Smash'] % {'output':filename_pdf, 'pdfs':pdfs})\n",
      "    ordered_barcodes = open(os.path.join(result_path, 'ordered_barcodes.txt'), 'w')\n",
      "    ordered_barcodes.write('\\n'.join(barcode_set))\n",
      "    ordered_barcodes.write('\\n')\n",
      "    ordered_barcodes.close()\n",
      "    return smash_cmds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 117
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We only do this part if we have identifying information on hand."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if participants is not None:\n",
      "    harvest_path = get_path('identified/harvested')\n",
      "    identified_smash = pdf_smash(harvest_path, 'identified', previously_printed=prev_printed)\n",
      "    res = farm_commands(identified_smash, 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 / 22 jobs still running, approximately  5655 seconds elapsed\n",
        "All jobs completed!\n"
       ]
      }
     ],
     "prompt_number": 118
    }
   ],
   "metadata": {}
  }
 ]
}
