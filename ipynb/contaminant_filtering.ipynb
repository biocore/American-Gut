{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook is meant for filtering the Gammaproteobacterial contaminants from American Gut samples."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are the modules we will need for this filtering notebook. Also, we will use the trim_fasta function to trim sequences to 100 nucleotides."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from os import mkdir, remove, chdir\n",
      "from os.path import join, split, splitext, abspath\n",
      "from tempfile import mktemp\n",
      "from collections import defaultdict\n",
      "from cPickle import loads\n",
      "from itertools import izip\n",
      "\n",
      "def trim_fasta(input_fasta, output_fp, length):\n",
      "    \"\"\"Trim FASTA sequences to a given length\n",
      "    \n",
      "    input_fasta: should be an open file. Every two lines should compose a complete FASTA record (header, sequence)\n",
      "    output_fp: should be a path to a file to be written\n",
      "    length: what length to trim the sequences to. Sequences shorter than length will not be modified.\n",
      "    \"\"\"\n",
      "    with open(output_fp, 'w') as output_fasta:\n",
      "        for header, sequence in izip(input_fasta, input_fasta):\n",
      "            header = header.strip()\n",
      "            sequence = sequence.strip()[:length]\n",
      "            output_fasta.write(\"%s\\n%s\\n\" %(header, sequence))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are all the filepaths we will be using throughout this notebook. We set them first so that the notebook can be continued easily if it is not run all at once. We also set the contamination abundance threshold here so that files can be named appropriately."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The reference representative set and taxonomy files\n",
      "reference_rep_set = '' # e.g., 97_otus.fasta from Greengenes\n",
      "reference_taxonomy = '' # e.g., 97_otu_taxonomy.txt from Grengenes\n",
      "\n",
      "# If a single sequence composes at least this fraction of a gammaproteo\n",
      "# OTU that is identified to be over-representend, flat it as a contaminant.\n",
      "contamination_abundance_threshold = 0.85\n",
      "\n",
      "working_dir = '' # The directory containing the sequence and mapping files (see below)\n",
      "\n",
      "# fill in the names of the sequence and mapping files here. The files should be in the\n",
      "# working_dir (above)\n",
      "sequence_files = ['']\n",
      "mapping_files = ['']\n",
      "\n",
      "# N.B.: It is unlikely you will want to change the names/paths of the files below!\n",
      "\n",
      "# Merged mapping and sequence files\n",
      "merged_sequences_fp = join(working_dir, 'merged_sequences.fna')\n",
      "merged_sequences_trimmed_100_fp = join(working_dir, 'merged_sequences_trimmed_100.fna')\n",
      "merged_sequences_trimmed_100_fecal_only_fp = join(working_dir, 'merged_sequences_fecal_only_trimmed_100.fna')\n",
      "merged_mapping_file_fp = join(working_dir, 'merged_mapping_file.txt')\n",
      "\n",
      "# File paths for the initial round of OTU picking, which is used to determine the most abundant proteobacterial OTUs\n",
      "pick_otus_trimmed_100_output_dir = join(working_dir, 'otus_trimmed_100')\n",
      "initial_otu_table = join(pick_otus_trimmed_100_output_dir, 'otu_table.biom')\n",
      "initial_otu_map = join(pick_otus_trimmed_100_output_dir, 'uclust_ref_picked_otus', splitext(split(merged_sequences_trimmed_100_fecal_only_fp)[-1])[0]+'_otus.txt')\n",
      "\n",
      "# Files for select_gamma.py, which calculates cumulative abundances\n",
      "gammaproteo_cumulative_abundances_fp = join(working_dir, 'gammaproteo_cumul_abundances.txt')\n",
      "\n",
      "# File paths for the second round of OTU picking, where the full set of sequences are clustered using the most\n",
      "# abundant proteobacterial OTUs as a reference\n",
      "contaminants_fp = join(working_dir, 'contaminants_%f.fna' % contamination_abundance_threshold)\n",
      "contaminant_picking_dir = join(working_dir, 'contaminant_otus_%f' % contamination_abundance_threshold)\n",
      "\n",
      "# The final set of output sequences (after contaminant filtering)\n",
      "filtered_sequences_fp = join(working_dir, 'filtered_sequences_%f.fna' % contamination_abundance_threshold)\n",
      "\n",
      "# The final round of OTU picking, using the filtered_sequences_fp above\n",
      "filtered_otus_dir = join(working_dir, 'filtered_otus_%f' % contamination_abundance_threshold)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are settings that facilitate running this notebook in parallel on a Torque-based cluster"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run cluster_utils.ipy\n",
      "\n",
      "# Jobs submitted via qsub will be named with prj_name\n",
      "prj_name = 'filtering'\n",
      "\n",
      "# set the number of processors parallel tasks will use\n",
      "NUM_PROCS = 60\n",
      "\n",
      "# submission wrapper\n",
      "submit = lambda cmd: submit_qsub(cmd, job_name=prj_name, queue='memroute', extra_args='-l pvmem=8gb')\n",
      "\n",
      "# These are the templates for the command for picking closed reference OTUs\n",
      "closed_ref_template = 'pick_closed_reference_otus.py -i %(input)s -o %(output)s -r %(reference)s -t %(taxonomy)s -aO %(num_procs)s'\n",
      "closed_ref_no_tax_template = 'pick_closed_reference_otus.py -i %(input)s -o %(output)s -r %(reference)s -aO %(num_procs)s'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we will merge the sequences files into a single sequence file, and the mapping files into a single mapping file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chdir(working_dir)\n",
      "\n",
      "if len(sequence_files) > 1:\n",
      "    # concatenate all sequence files into one merged sequence file. This can take a while!\n",
      "    sequence_files_space_separated = ' '.join(sequence_files)\n",
      "    !cat $sequence_files_space_separated > $merged_sequences_fp\n",
      "elif len(sequence_files) == 1:\n",
      "    merged_sequences_fp = join(working_dir, sequence_files[0])\n",
      "else:\n",
      "    raise IOError(\"Must specify at least one sequence file.\")\n",
      "\n",
      "trim_fasta(open(merged_sequences_fp, 'U'), merged_sequences_trimmed_100_fp, 100)\n",
      "\n",
      "if len(mapping_files) > 1:\n",
      "    # merge all mapping files into one mapping file\n",
      "    mapping_files_comma_separated = ','.join(mapping_files)\n",
      "    !merge_mapping_files.py -m $mapping_files_comma_separated -o $merged_mapping_file_fp\n",
      "elif len(mapping_files) == 1:\n",
      "    merged_mapping_file_fp = join(working_dir, mapping_files[0])\n",
      "else:\n",
      "    raise IOError(\"Must specify at least one mapping file.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When finding Gammaproteobacterial contaminants, we want to consider only fecal samples, so we filter our sequences to contain only those that are associated with fecal samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!filter_fasta.py -f $merged_sequences_trimmed_100_fp -o $merged_sequences_trimmed_100_fecal_only_fp --mapping_fp=$merged_mapping_file_fp --valid_states=\"BODY_SITE:UBERON:feces\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The filtering will proceed according to the following steps:\n",
      "\n",
      "1. Identify the most abundant Gammaproteobactia and select the representative sequences from the most abundant, such that the cumulative abundance of remaining Gammaproteobacteria is no greater than 3%\n",
      "2. Find all sequences in the American Gut sequences that map to these OTUs at 97PI\n",
      "3. Of these sequences, if 90% of them are a single unique sequence, treat this sequnence as a contaminant seqeunce\n",
      "4. Create reference database of these contaminant sequences\n",
      "5. Cluster (at 97PI) the full set of American Gut sequence against this reference set of contaminant sequences\n",
      "6. Remove all that map to those contaminant sequences\n",
      "7. Pick OTUs against the standard reference database (Greengenes 13_5 in this case)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "closed_ref_1_args = {\n",
      "    'input': merged_sequences_trimmed_100_fecal_only_fp,\n",
      "    'output': pick_otus_trimmed_100_output_dir,\n",
      "    'reference': reference_rep_set,\n",
      "    'taxonomy': reference_taxonomy,\n",
      "    'num_procs': NUM_PROCS\n",
      "}\n",
      "closed_ref_1_job = submit(closed_ref_template % closed_ref_1_args)\n",
      "jobs = wait_on([closed_ref_1_job])\n",
      "\n",
      "!select_gamma.py -i $initial_otu_table -o $gammaproteo_cumulative_abundances_fp -l 0.03"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we need to grab all of our sequences that mapped to each of these OTUs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The IDs generated in the previous step by select_gamma.py\n",
      "gammaproteo_ids = {x.strip().split('\\t')[0] for x in open(gammaproteo_cumulative_abundances_fp, 'U').readlines()}\n",
      "\n",
      "# This will hold the OTU map for the OTUs in the list above\n",
      "otu_map = {x:set() for x in gammaproteo_ids}\n",
      "\n",
      "# go through the otu map and save the lines of interest to the otu_map\n",
      "# data structure above\n",
      "print \"Reading OTU map...\"\n",
      "for line in open(initial_otu_map, 'U'):\n",
      "    otu_id, seq_ids = line.strip().split('\\t',1)\n",
      "    if otu_id in gammaproteo_ids:\n",
      "        otu_map[otu_id] = set(seq_ids.split('\\t'))\n",
      "\n",
      "# this will hold, for each OTU in otus, counts of each unique sequence\n",
      "# observed in that OTU\n",
      "unique_counts = {x:defaultdict(int) for x in gammaproteo_ids}\n",
      "\n",
      "# go through input fasta file two lines at a time, counting unique\n",
      "# sequences in each OTU of intrest\n",
      "print \"Reading FASTA file and counting unique sequences...\"\n",
      "with open(merged_sequences_trimmed_100_fp, 'U') as input_seqs:\n",
      "    for header, sequence in izip(input_seqs, input_seqs):\n",
      "        header = header.strip()\n",
      "        sequence = sequence.strip()\n",
      "        seq_id = header.split(' ', 1)[0][1:]\n",
      "        for otu_id in gammaproteo_ids:\n",
      "            if seq_id in otu_map[otu_id]:\n",
      "                unique_counts[otu_id][sequence] += 1\n",
      "                break\n",
      "print \"Done.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And then write out the sequences that are over the abundance threshold"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Writing contaminant sequences...\"\n",
      "with open(contaminants_fp, 'w') as contaminants_f:\n",
      "    for otu_id, otu_counts in unique_counts.iteritems():\n",
      "        print \"OTU\", otu_id\n",
      "        otu_total_count = sum([count for seq, count in otu_counts.iteritems()])\n",
      "        \n",
      "        counter = 0\n",
      "        for seq, count in sorted(otu_counts.items(), key=lambda x:x[1], reverse=True):\n",
      "            counter += 1\n",
      "            if 1.0*count/otu_total_count > contamination_abundance_threshold:\n",
      "                print \"    \"+seq\n",
      "                contaminants_f.write('>%s_%d\\n%s\\n' % (otu_id, counter, seq))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now pick OTUs against this FASTA file, which represents the set of contaminant sequences, to identify all sequences in our original merged FASTA file are likely the result of contamination. We will discard these sequences before picking OTUs against our standard reference set.\n",
      "\n",
      "N.B.: If your contaminants.fna file is empty, then no contaminants were detected, and you do not need to proceed any further!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Cluster (at 97PI) the full set of American Gut sequence against this reference set of contaminant sequences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "closed_ref_2_args = {\n",
      "    'input': merged_sequences_trimmed_100_fp,\n",
      "    'output': contaminant_picking_dir,\n",
      "    'reference': contaminants_fp,\n",
      "    'num_procs': NUM_PROCS\n",
      "}\n",
      "print \"Clustering the full set of American Gut sequences against reference set of contaminant sequences\"\n",
      "closed_ref_2_job = submit(closed_ref_no_tax_template % closed_ref_2_args)\n",
      "wait_on([closed_ref_2_job])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remove all that map to those contaminant sequences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Removing all American Gut sequences that clustered against the contaminant sequences\"\n",
      "contaminant_picking_otu_map = join(contaminant_picking_dir, 'uclust_ref_picked_otus', splitext(split(merged_sequences_trimmed_100_fp)[-1])[0]+'_otus.txt')\n",
      "!filter_fasta.py -f $merged_sequences_trimmed_100_fp -m $contaminant_picking_otu_map -o $filtered_sequences_fp -n\n",
      "print \"Done.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pick OTUs against the standard reference database"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "closed_ref_3_args = {\n",
      "    'input': filtered_sequences_fp,\n",
      "    'output': filtered_otus_dir,\n",
      "    'reference': reference_rep_set,\n",
      "    'taxonomy': reference_taxonomy,\n",
      "    'num_procs': NUM_PROCS\n",
      "}\n",
      "print \"Clustering remaining sequences using standard closed-reference OTU picking protocol\"\n",
      "closed_ref_3_job = submit(closed_ref_template % closed_ref_3_args)\n",
      "wait_on([closed_ref_3_job])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}