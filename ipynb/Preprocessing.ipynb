{
 "metadata": {
  "name": "",
  "signature": "sha256:a8111c974fb72f71f5994ea3fb947b4afbaceb0d02b22b69582bcc35129176f9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal of this notebook is to produce a single set of files for easy downstream analysis. The notebook consumes the OTU tables produced during Module 1 or Module 2 processing (a table built from the EBI sequences), massages the mapping data, rarefies the data and calculates alpha and beta diversity. The files in each of the output directories are then consumable by downstream analysis notebooks presented here (i.e. Power), Qiime, Emperor, PiCRUST or other custom analyses.\n",
      "\n",
      "If you choose not to run this notebook, all the OTU tables generated here can be downloaded in a tar file from link. Individual data sets can be found at the linked locations below.\n",
      "<ul><li><a href=\"https://www.dropbox.com/s/so6hpzq1oib8xhv/all_samples.tgz?dl=0\">All bodysites, all samples</a>\n",
      "</li><li><a href=\"https://www.dropbox.com/s/kq4rnn1aw23zwp3/oral_samples.tgz?dl=0\">Oral Samples</a>\n",
      "</li><li><a href=\"https://www.dropbox.com/s/h0m4cscadvlvhtw/skin_samples.tgz?dl=0\">Skin Samples</a>\n",
      "</li><li><a href=\"https://www.dropbox.com/s/afzpr31tmf5i500/fecal_samples.tgz?dl=0\">All fecal samples</a>\n",
      "</li><li><a href=\"https://www.dropbox.com/s/8mecply3fg4wouk/all_otus_all_samples.tgz?dl=0\">Fecal Sample: all samples, all subjects, all OTUs</a>\n",
      "</li><li><a href=\"https://www.dropbox.com/s/w85221gh1tmr4u0/all_otus_single_samples.tgz?dl=0\">Fecal Sample: single sample per subject, all OTUs</a>\n",
      "</li><li><a href=\"https://www.dropbox.com/s/rufi3ej61ewckxz/all_otus_subset_samples.tgz?dl=0\">Fecal Sample: all samples, healthy subset of subjects, all OTUs</a>\n",
      "</li><li><a href=\"https://www.dropbox.com/s/0jnxaubnocer9f8/all_otus_subset_single_samples.tgz?dl=0\">Fecal Sample: sing le sample per healthy subject, all OTUs</a>\n",
      "</li><li><a href=\"https://www.dropbox.com/s/hvdv2t70pm321mg/filtered_otus_all_samples.tgz?dl=0\">Fecal Sample: all samples, all subjects, filtered OTUs</a>\n",
      "</li><li><a href=\"https://www.dropbox.com/s/wevop1lxvun9znf/filtered_otus_single_samples.tgz?dl=0\">Fecal Sample: single sample per subject, filtered OTUs</a>\n",
      "</li><li><a href=\"https://www.dropbox.com/s/t3lr6hv7lcib3n4/filtered_otus_subset_samples.tgz?dl=0\">Fecal Sample: all samples, healthy subset of subjects, filtered OTUs</a>\n",
      "</li><li><a href=\"https://www.dropbox.com/s/4v2hlgkdi0rajk0/filtered_otus_subset_single_samples.tgz?dl=0\">Fecal Sample: single sample per healthy subject, filtered OTUs</a>\n",
      "</li></ul>\n",
      "\n",
      "This notebook assumes the following system requirements:\n",
      "<ul><li><a href=\"https://www.python.org/download/releases/2.7/\">Python 2.7</a>\n",
      "</li><li><a href=\"https://pypi.python.org/pypi/numpy\">Numpy $\\geq$ 1.7</a>\n",
      "</li><li><a href=\"http://qiime.org/install/install.html#latest-development-version\">Qiime 1.8-dev</a>, commit 3e41198d2e43ef6c9492f994043c851b08268748\n",
      "</li><li><a href=\"http://biom-format.org\">Biom 2.0.1</a>\n",
      "</li><li><a href=\"http://scikit-bio.org\">Scikit Bio 0.2.1-dev</a>, commit c90764add74040a937d81d647481dbe958d241d6\n",
      "</li><li><a href=\"http://pandas.pydata.org\">Pandas 0.14.1</a>\n",
      "</li><li><a href=\"http://ipython.org\">iPython</a>\n",
      "</li><li>Two custom code libraries from the <a href=\"https://github.com/biocore/American-Gut\">American Gut github repository</a>: diversity_analysis.py and geography_lib.py\n",
      "</li></ul>\n",
      "\n",
      "<a id=\"top\"></a>\n",
      "###Table of Contents\n",
      "<ul><li><a href=\"#parameters\">Sets up parameters for data analysis</a>\n",
      "</li><li><a href=\"#files\">Set up files and directories for analysis</a>\n",
      "</li><li><a href=\"#github_download\">Download raw OTU tables from GitHub</a>. This will require an internet connection.\n",
      "</li><li><a href=\"#clean_map\">Cleans mapping columns</a>\n",
      "</li><li>Bins data in mapping columns\n",
      "<ul><li><a href=\"#age_bin\">Age by Decade</a>\n",
      "</li><li><a href=\"#etoh_bin\">Alcohol Consumption</a>\n",
      "</li><li><a href=\"#bmi_bin\">Body Mass Index</a>\n",
      "</li><li><a href=\"#month_bin\">Collection Date</a>\n",
      "</li><li><a href=\"#state_bin\">Collection location</a>\n",
      "</li></ul>\n",
      "</li><li><a href=\"#id_subset\">Identifies a healthy subset</a>\n",
      "</li><li><a href=\"#all_beta\">Calcululates beta diversity for all the samples.</a> This can be quite time consuming.\n",
      "</li><li><a href=\"#splitsite\">Splits the OTU table by body site</a>\n",
      "</li><li><a href=\"#filter\">Filters fecal samples to remove uncommon OTUs</a>\n",
      "</li><li><a href=\"#rarefaction\">Rarefies the data to 10,000 sequences per sample</a>\n",
      "</li><li><a href=\"#alpha\">Calculates alpha diversity for filtered and unfiltered samples</a>\n",
      "</li><li><a href=\"#single\">Identifies single samples in the OTU table and mapping files</a>\n",
      "</li><li><a href=\"#subset\">Splits the table to isolate healthy subjects</a>\n",
      "</li><li><a href=\"#beta\">Gets distance matrices for each split table</a>\n",
      "</li></ul>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Imports necessary functions\n",
      "from os import mkdir, remove\n",
      "from os.path import abspath, join as pjoin, exists, split\n",
      "from shutil import copy2, move\n",
      "from copy import deepcopy\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from biom import load_table\n",
      "from skbio import DistanceMatrix\n",
      "from americangut.diversity_analysis import check_dir, pad_index\n",
      "from americangut.geography_lib import (regions_by_state,\n",
      "                                       us_state_map,\n",
      "                                       canadian_map_english)\n",
      "\n",
      "# Writes a file to save the json string tables\n",
      "def write_biom(table, fp):\n",
      "    \"\"\"Writes a biom table as a json string\"\"\"\n",
      "    file_ = open(fp, 'w')\n",
      "    file_.write(table.to_json(''))\n",
      "    file_.close()\n",
      "    \n",
      "# Filters out missing OTUs\n",
      "def filter_emtpy_otus(otu_table):\n",
      "    \"\"\"Filters out empty OTUs from a filtered table\"\"\"\n",
      "    # Gets the otu presence counts\n",
      "    obs_ids =  pd.Series(otu_table.nonzero_counts('observation'), \n",
      "                         index=otu_table.ids('observation'))\n",
      "    # Filters out OTUs which are not present in any samples\n",
      "    obs_keep = obs_ids[obs_ids > 0].index\n",
      "    otu_table = otu_table.filter(obs_keep, 'observation')\n",
      "    return otu_table"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "<a id=\"parameters\"></a>\n",
      "### Sets Analysis Parameters\n",
      "\n",
      "We need to consider several parameters. These will handle the file reading, rarefaction, alpha diversity and beta diversity handling.\n",
      "\n",
      "We can start by setting an important intial parameter: <strong><code>overwrite</code></strong>. This allows us to determin if files files should be downloaded and processed or not. To minimize computational time, <code>overwrite</code> should be <code><font color=\"green\">False</font></code> unless downloading new tables.\n",
      "\n",
      "The <strong><code>retain_intermediate</code></strong> indicates if files used in the generation of the final table (i.e. rarefaction instances, alpha diverisity text files, rough split tables) should be kept after the notebook finishes processing.\n",
      "\n",
      "##### Pandas filehandling parameters\n",
      "<ul><li>The <strong><code>txt_delim</code></strong> specifies the way columns are seperated in the files. Qiime standards typically use text (<code>.txt</code>) files, which are seperated by a tab-character (<code><font color=\"FireBrick\">'\\t'</font></code>).\n",
      "</li><li><strong><code>map_index</code></strong> specifies the name of the file containing the sample names. In Qiime, this is named #SampleID.\n",
      "</li><li>Possible empty values, <strong><code>map_nas</code></strong> provides a list of values which specify data has not been supplied. American Gut participants are free to skip any survey question they do not wish to answer. As a result, the mapping file can contain  empty or missing-data fields.\n",
      "</li><li><strong><code>write_na</code></strong> gives a value used when the files are written. Using an empty space, <code><font color=\"FireBrick\">''</font></code>, scripts like <code>group_signifigance.py</code> will ignore the group.\n",
      "</li><li><strong><code>date_cols</code></strong> tells pandas which columns to parse as time/date stamped during import.\n",
      "</li></ul>\n",
      "\n",
      "##### Rarefaction parameters\n",
      "<ul><li>The <strong><code>rarefaction_depth</code></strong> specifies the number of sequence per samples to be used for analysis. A depth of 10,000 seqs/sample was selected  because it balances a better picture of diversity with retaining samples. Rarefaction begins by removing samples from the table which do not have the minimum number of counts. Sequences are then drawn randomly out of a weighted pool until we reach the appropriate number. Our unpublished data shows rarefaction is conservative and is appropriate for UniFrac measurements.\n",
      "</li><li><strong><code>num_rarifactions</code></strong> gives the number of rarefaction instances which should be drawn. This controls for bias due to single rarefaction instances. We selected 10 rarefactions to achieve a balance between computational effeciency and appropriate depth.\n",
      "</li></ul>\n",
      "\n",
      "##### Filtering parameters\n",
      "The filtering parameters, <strong><code>filt_frac</code></strong> and <strong><code>filt_num</code></strong> refer to the minimum number of samples, or the minimum fraction of samples in which a particular OTU must be present to be retained after filtering. This final filtering fraction is calculated as the minimum of these two numbers.\n",
      "\n",
      "##### Aphha Diversity Metrics\n",
      "<ul><li><strong><code>alpha_metrics</code></strong> is a comma-seperated string, listing the desired metrics. <br>The current notebook is set to calculate four alpha diversity metrics, <em>PD Whole Tree</em>, <em>Observed Species</em>, <em>choa1</em> and <em>shannon</em> diveristy. A list of avaliable metrics can be found through the <a href=\"http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html#module-skbio.diversity.alpha\">Scikit-Bio website</a>.\n",
      "</li><li><strong><code>div_metric</code></strong> is the diversity metric to be used during analysis.<br>While we can calculate a variety of diversity metrics, is advantegous to focus only on one metric during analysis. Here, we use the mean of the 10 PD whole tree diversity calculations, performed one each rarified table. PD whole tree diversity was selected for thsi analysis becuase it takes into account not only the number of different organisms present, but also their similarity and dissimilarity.\n",
      "</li></ul>\n",
      "\n",
      "##### Beta Diversity Metrics\n",
      "<ul><li><strong><code>beta_metrics</code></strong> are a list of beta diversity metrics. The default is to use weighted and unweighted UniFrac distance.\n",
      "</li></ul>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "overwrite = False\n",
      "retain_intermediate = True\n",
      "\n",
      "# Sets up parameters for when pandas reads and writes data.\n",
      "txt_delim = '\\t'\n",
      "map_index = '#SampleID'\n",
      "map_nas = ['NA', 'no_data', 'unknown', '']\n",
      "write_na = ''\n",
      "date_cols = [7, 106, 82, 134]\n",
      "\n",
      "# Sets up rarifaction parameter for 10,000 sequences/sample and \n",
      "# 10 rounds of rarifaction.\n",
      "rarifaction_depth = 10000\n",
      "num_rarifactions = 10\n",
      "\n",
      "# Sets the filtering parameters.\n",
      "filt_frac = 0.01\n",
      "filt_num = 50\n",
      "\n",
      "# Handles the alpha diversity metrics\n",
      "alpha_metrics = 'PD_whole_tree,observed_species,chao1,shannon'\n",
      "div_metric = 'PD_whole_tree'\n",
      "\n",
      "# Handles the beta diversity metrics\n",
      "beta_metrics = \"unweighted_unifrac,weighted_unifrac\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"files\"></a>\n",
      "#### Sets up files and filepaths\n",
      "\n",
      "We can begin by setting up directories to save the files which will be downloaded and handled here. The default setting is to perform the analysis in a new directory, <code>agp_analysis</code> in the parent directory of the current directory (assumed to be the notebook directory).\n",
      "\n",
      "The <code>tree_fp</code> specifies the location of the phylogenetic tree used for the dataset. American Gut data was picked closed reference using 97% similarity in hte the greengenes 13_5 reference set. The reference set can be downloaded <a href=\"http://greengenes.secondgenome.com/downloads/database/13_5\">here</a>. \n",
      "\n",
      "To change where data is saved, the <code>base_dir</code> should be set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sets the base directory for the analysis\n",
      "base_dir = pjoin(abspath('..'), 'agp_analysis_rounds_1_14')\n",
      "check_dir(base_dir)\n",
      "\n",
      "# Sets up a directory for the OTU information\n",
      "otu_dir = pjoin(base_dir, \"otu_tables\")\n",
      "check_dir(otu_dir)\n",
      "\n",
      "# Creates a new directory where the raw OTU table and raw mapping file should\n",
      "# be saved\n",
      "raw_dir = pjoin(otu_dir, 'all_samples')\n",
      "check_dir(raw_dir)\n",
      "\n",
      "# Sets up a directory for rarefaction and intermediate steps\n",
      "working_dir = pjoin(otu_dir, 'intermediate_files')\n",
      "check_dir(working_dir)\n",
      "\n",
      "# Creates a directory where split tables should be saved\n",
      "split_dir = pjoin(working_dir, \"split_by_bodysite\")\n",
      "check_dir(split_dir)\n",
      "\n",
      "# Sets up a directory for rarefaction and alpha diversity\n",
      "rare_dir = pjoin(working_dir, 'rarefaction')\n",
      "check_dir(rare_dir)\n",
      "feces_rare_dir = pjoin(rare_dir, 'raw_feces')\n",
      "check_dir(feces_rare_dir)\n",
      "filt_rare_dir = pjoin(rare_dir, 'filt_feces')\n",
      "check_dir(filt_rare_dir)\n",
      "oral_rare_dir = pjoin(rare_dir, 'oral')\n",
      "check_dir(oral_rare_dir)\n",
      "skin_rare_dir = pjoin(rare_dir, 'skin')\n",
      "check_dir(skin_rare_dir)\n",
      "\n",
      "alpha_dir = pjoin(working_dir, 'alpha')\n",
      "check_dir(alpha_dir)\n",
      "feces_alpha_dir = pjoin(alpha_dir, 'raw_feces')\n",
      "check_dir(feces_alpha_dir)\n",
      "filt_alpha_dir = pjoin(alpha_dir, 'filt_feces')\n",
      "check_dir(filt_alpha_dir)\n",
      "oral_alpha_dir = pjoin(alpha_dir, 'oral')\n",
      "check_dir(oral_alpha_dir)\n",
      "skin_alpha_dir = pjoin(alpha_dir, 'skin')\n",
      "check_dir(skin_alpha_dir)\n",
      "\n",
      "# Sets the reference tree filepath. This is required when using phylogentic\n",
      "# metrics such as PD_whole_tree alpha diversity and UniFrac beta diversity.\n",
      "tree_fp = '/Users/jwdebelius/lib/Greengenes/gg_13_5_otus/trees/97_otus.tree'\n",
      "\n",
      "# Sets up the location filepaths for the raw data\n",
      "raw_otu_zip = pjoin(raw_dir, 'AG_100nt.biom.gz')\n",
      "raw_otu_fp = pjoin(raw_dir, 'AG_100nt.biom')\n",
      "raw_map_fp = pjoin(raw_dir, 'AG_100nt.txt')\n",
      "\n",
      "# Handles renaming the split files\n",
      "ori_feces_otu_fp = pjoin(split_dir, 'AG_100nt_UBERON:feces.biom')\n",
      "ori_feces_map_fp = pjoin(split_dir, 'mapping_UBERON:feces.txt')\n",
      "\n",
      "# Sets the filename pattern which will appear for the rarified table\n",
      "rare_filepattern = 'rarefaction_%i_%i.biom'\n",
      "\n",
      "# Sets up the alpha diversity filepattern\n",
      "alpha_filepattern = 'alpha_rarefaction_%i_%i.txt'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also set the filenames for saving data in the directory. We will produce 12 sets of files:\n",
      "<ul><li>The unfiltered oral table with all samples\n",
      "</li><li>The unfiltered oral table with a single sample from each individual\n",
      "</li><li>The unfiltered skin table with all samples\n",
      "</li><li>The unfiltered skin table with a single sample from each individual\n",
      "</li><li>The unfiltered fecal table with all samples\n",
      "</li><li>The unfiltered table with a single sample per indidivual\n",
      "</li><li>The filtered table, with all samples\n",
      "</li><li>The filtered table, with a single sample per indidivual\n",
      "</li><li>The unfiltered table with only a subset of healhty individuals\n",
      "</li><li>The unfiltered table with a single sample for each healthy indiviual\n",
      "</li><li>The filtered table with only a subset of healhty individuals\n",
      "</li><li>The filtered table with only a single sample for each healthy indiviual\n",
      "</li></ul>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# All samples\n",
      "aawtu_otu_fp = raw_otu_fp\n",
      "aawtu_map_fp = raw_map_fp\n",
      "aawtr_map_fp = pjoin(raw_dir, 'AGP_100nt_even10k.txt')\n",
      "aawtr_otu_fp = pjoin(raw_dir, 'AGP_100nt_even10k.biom')\n",
      "aawtr_ubd_fp = pjoin(raw_dir, 'unweighted_unifrac_AGP_100nt_even10k.txt')\n",
      "aawtr_wbd_fp = pjoin(raw_dir, 'weighted_unifrac_AGP_100nt_even10k.txt')\n",
      "\n",
      "# Oral Samples\n",
      "# Sets up a directory for the bodysite\n",
      "oral_dir = pjoin(otu_dir, 'oral_samples')\n",
      "check_dir(oral_dir)\n",
      "# Creates files for the directory sets\n",
      "oawt_dir = pjoin(oral_dir, 'all_otus_all_samples')\n",
      "check_dir(oawt_dir)\n",
      "oast_dir = pjoin(oral_dir, 'all_otus_single_samples')\n",
      "check_dir(oast_dir)\n",
      "# Sets up the filenames\n",
      "oawtu_otu_fp = pjoin(oawt_dir, 'AGP_100nt_oral.biom')\n",
      "oawtu_map_fp = pjoin(oawt_dir, 'AGP_100nt_oral.txt')\n",
      "oastu_otu_fp = pjoin(oast_dir, 'AGP_100nt_oral.biom')\n",
      "oastu_map_fp = pjoin(oast_dir, 'AGP_100nt_oral.txt')\n",
      "\n",
      "oawtr_otu_fp = pjoin(oawt_dir, 'AGP_100nt_oral_even10k.biom')\n",
      "oawtr_map_fp = pjoin(oawt_dir, 'AGP_100nt_oral_even10k.txt')\n",
      "oastr_otu_fp = pjoin(oast_dir, 'AGP_100nt_oral_even10k.biom')\n",
      "oastr_map_fp = pjoin(oast_dir, 'AGP_100nt_oral_even10k.txt')\n",
      "\n",
      "# Skin Samples\n",
      "# Sets up a directory for the bodysite\n",
      "skin_dir = pjoin(otu_dir, 'skin_samples')\n",
      "check_dir(skin_dir)\n",
      "# Creates files for the directory sets\n",
      "sawt_dir = pjoin(skin_dir, 'all_otus_all_samples')\n",
      "check_dir(sawt_dir)\n",
      "sast_dir = pjoin(skin_dir, 'all_otus_single_samples')\n",
      "check_dir(sast_dir)\n",
      "# Sets up the filenames\n",
      "sawtu_otu_fp = pjoin(sawt_dir, 'AGP_100nt_skin.biom')\n",
      "sawtu_map_fp = pjoin(sawt_dir, 'AGP_100nt_skin.txt')\n",
      "sastu_otu_fp = pjoin(sast_dir, 'AGP_100nt_skin.biom')\n",
      "sastu_map_fp = pjoin(sast_dir, 'AGP_100nt_skin.txt')\n",
      "\n",
      "sawtr_otu_fp = pjoin(sawt_dir, 'AGP_100nt_skin_even10k.biom')\n",
      "sawtr_map_fp = pjoin(sawt_dir, 'AGP_100nt_skin_even10k.txt')\n",
      "sastr_otu_fp = pjoin(sast_dir, 'AGP_100nt_skin_even10k.biom')\n",
      "sastr_map_fp = pjoin(sast_dir, 'AGP_100nt_skin_even10k.txt')\n",
      "\n",
      "# Fecal Samples\n",
      "# Creates a directory for the bodysite data\n",
      "fecal_dir = pjoin(otu_dir, 'fecal_samples')\n",
      "check_dir(fecal_dir)\n",
      "# Creates files for the directory sets\n",
      "fawt_dir = pjoin(fecal_dir, 'all_otus_all_samples')\n",
      "check_dir(fawt_dir)\n",
      "fast_dir = pjoin(fecal_dir, 'all_otus_subset_samples')\n",
      "check_dir(fast_dir)\n",
      "faws_dir = pjoin(fecal_dir, 'all_otus_single_samples')\n",
      "check_dir(faws_dir)\n",
      "fass_dir = pjoin(fecal_dir, 'all_otus_subset_single_samples')\n",
      "check_dir(fass_dir)\n",
      "ffwt_dir = pjoin(fecal_dir, 'filtered_otus_all_samples')\n",
      "check_dir(ffwt_dir)\n",
      "ffst_dir = pjoin(fecal_dir, 'filtered_otus_subset_samples')\n",
      "check_dir(ffst_dir)\n",
      "ffws_dir = pjoin(fecal_dir, 'filtered_otus_single_samples')\n",
      "check_dir(ffws_dir)\n",
      "ffss_dir = pjoin(fecal_dir, 'filtered_otus_subset_single_samples')\n",
      "check_dir(ffss_dir)\n",
      "\n",
      "# Sets the filepaths for the unrarified tables\n",
      "fawtu_otu_fp = pjoin(fawt_dir, 'AGP_100nt_fecal.biom')\n",
      "fawtu_map_fp = pjoin(fawt_dir, 'AGP_100nt_fecal.txt')\n",
      "fastu_otu_fp = pjoin(fast_dir, 'AGP_100nt_fecal.biom')\n",
      "fastu_map_fp = pjoin(fast_dir, 'AGP_100nt_fecal.txt')\n",
      "fawsu_otu_fp = pjoin(faws_dir, 'AGP_100nt_fecal.biom')\n",
      "fawsu_map_fp = pjoin(faws_dir, 'AGP_100nt_fecal.txt')\n",
      "fassu_otu_fp = pjoin(fass_dir, 'AGP_100nt_fecal.biom')\n",
      "fassu_map_fp = pjoin(fass_dir, 'AGP_100nt_fecal.txt')\n",
      "ffwtu_otu_fp = pjoin(ffwt_dir, 'AGP_100nt_fecal.txt')\n",
      "ffwtu_map_fp = pjoin(ffwt_dir, 'AGP_100nt_fecal.biom')\n",
      "ffstu_otu_fp = pjoin(ffst_dir, 'AGP_100nt_fecal.txt')\n",
      "ffstu_map_fp = pjoin(ffst_dir, 'AGP_100nt_fecal.biom')\n",
      "ffwsu_otu_fp = pjoin(ffws_dir, 'AGP_100nt_fecal.txt')\n",
      "ffwsu_map_fp = pjoin(ffws_dir, 'AGP_100nt_fecal.biom')\n",
      "ffssu_otu_fp = pjoin(ffss_dir, 'AGP_100nt_fecal.txt')\n",
      "ffssu_map_fp = pjoin(ffss_dir, 'AGP_100nt_fecal.biom')\n",
      "\n",
      "# Sets the filepath for the rarefied table\n",
      "fawtr_otu_fp = pjoin(fawt_dir, 'AGP_100nt_fecal_even10k.biom')\n",
      "fawtr_map_fp = pjoin(fawt_dir, 'AGP_100nt_fecal_even10k.txt')\n",
      "fastr_otu_fp = pjoin(fast_dir, 'AGP_100nt_fecal_even10k.biom')\n",
      "fastr_map_fp = pjoin(fast_dir, 'AGP_100nt_fecal_even10k.txt')\n",
      "fawsr_otu_fp = pjoin(faws_dir, 'AGP_100nt_fecal_even10k.biom')\n",
      "fawsr_map_fp = pjoin(faws_dir, 'AGP_100nt_fecal_even10k.txt')\n",
      "fassr_otu_fp = pjoin(fass_dir, 'AGP_100nt_fecal_even10k.biom')\n",
      "fassr_map_fp = pjoin(fass_dir, 'AGP_100nt_fecal_even10k.txt')\n",
      "ffwtr_otu_fp = pjoin(ffwt_dir, 'AGP_100nt_fecal_even10k.biom')\n",
      "ffwtr_map_fp = pjoin(ffwt_dir, 'AGP_100nt_fecal_even10k.txt')\n",
      "ffstr_otu_fp = pjoin(ffst_dir, 'AGP_100nt_fecal_even10k.biom')\n",
      "ffstr_map_fp = pjoin(ffst_dir, 'AGP_100nt_fecal_even10k.txt')\n",
      "ffwsr_otu_fp = pjoin(ffws_dir, 'AGP_100nt_fecal_even10k.biom')\n",
      "ffwsr_map_fp = pjoin(ffws_dir, 'AGP_100nt_fecal_even10k.txt')\n",
      "ffssr_otu_fp = pjoin(ffss_dir, 'AGP_100nt_fecal_even10k.biom')\n",
      "ffssr_map_fp = pjoin(ffss_dir, 'AGP_100nt_fecal_even10k.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"github_download\"></a>\n",
      "#### Downloads the data from GitHub\n",
      "\n",
      "We now use cURL to download the data. If the data has already been downloaded, it will not be downloaded again unless the notebook is explicitly set to <code>Overwrite = <font color=\"green\">True</font></code>, which will always create or overwrite files."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Gets the biom file\n",
      "if not exists(raw_otu_fp) or overwrite:\n",
      "    # Downloads the compressed biom file\n",
      "    !curl -OL https://github.com/biocore/American-Gut/raw/master/data/AG/AG_100nt.biom.gz\n",
      "    # Unzips the biom file\n",
      "    !gunzip AG_100nt.biom.gz\n",
      "    # Moves the biom file to its final location\n",
      "    move(pjoin('.', 'AG_100nt.biom'), raw_otu_fp)\n",
      "\n",
      "if not exists(raw_map_fp) or overwrite:\n",
      "    # Downloads the mapping file\n",
      "    !wget https://github.com/biocore/American-Gut/raw/master/data/AG/AG_100nt.txt\n",
      "    move(pjoin('.', 'AG_100nt.txt'), raw_map_fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"clean_map\"></a>\n",
      "#### Cleans up mapping columns\n",
      "There are a set of mapping columns which are currently altered in the EBI submission. These three columns are <code>SEX</code>, <code>AGE_UNIT</code>, and <code>DOMINANT_HAND</code>. So, we will fix the values in these columns for the mapping file we plan to use in our analyses."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Loads the mapping file and the OTU table\n",
      "raw_map = pad_index(pd.read_csv(raw_map_fp,\n",
      "                             sep=txt_delim, \n",
      "                             na_values=map_nas,\n",
      "                             parse_dates=date_cols,\n",
      "                             low_memory=False),\n",
      "                    index_col=map_index)\n",
      "raw_otu = load_table(raw_otu_fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Removes samples from the mapping file which are not present in the\n",
      "# OTU table\n",
      "raw_map = raw_map.loc[raw_otu.ids()]\n",
      "\n",
      "# Cleans up the sex information based on a miscoding\n",
      "raw_map.loc[raw_map.SEX == '47', 'SEX'] = 'male'\n",
      "raw_map.loc[raw_map.SEX == '48', 'SEX'] = 'female'\n",
      "\n",
      "# Cleans up age information based on miscodeing\n",
      "raw_map.loc[raw_map.AGE_UNIT == '78', 'AGE_UNIT'] = 'years'\n",
      "\n",
      "# Cleans up handedness based on miscoding\n",
      "raw_map.loc[raw_map.DOMINANT_HAND == '151', 'DOMINANT_HAND'] = 'left'\n",
      "raw_map.loc[raw_map.DOMINANT_HAND == '152', 'DOMINANT_HAND'] = 'right'\n",
      "raw_map.loc[raw_map.DOMINANT_HAND == '153', 'DOMINANT_HAND'] = 'ambidextrous'\n",
      "\n",
      "# Removes any samples submitted prior to the start of the project, set here as 1 September 2012\n",
      "raw_map.loc[raw_map.COLLECTION_DATE < pd.datetime(2012, 9, 1), \n",
      "            'COLLECTION_DATE'] = pd.to_datetime(['2009-07-31', 'asd'], coerce=True)[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"age_bin\"></a>\n",
      "######Age\n",
      "\n",
      "There are also a set of columns which are not included in the map, but may be useful for downstream anlyses. These include age binned by decade (<code>AGE_CAT</code>). While there are Qiime analyese which can handle simple, linear, regression, binning can help identify more complex patterns and simplify some of the noise.\n",
      "\n",
      "Here, we bin age by decade, with the exception of people under the age of 20. The gut develops in the first two years of life, and the guts of young children are signifigantly different than older children or adults [citation needed]."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Bins age by decade (with the exception of young children)\n",
      "def categorize_age(x):\n",
      "    if np.isnan(x):\n",
      "        return x\n",
      "    elif x < 3:\n",
      "        return \"baby\"\n",
      "    elif x < 13:\n",
      "        return \"child\"\n",
      "    elif x < 20:\n",
      "        return \"teen\"\n",
      "    elif x < 30:\n",
      "        return \"20s\"\n",
      "    elif x < 40:\n",
      "        return \"30s\"\n",
      "    elif x < 50:\n",
      "        return \"40s\"\n",
      "    elif x < 60:\n",
      "        return \"50s\"\n",
      "    elif x < 70:\n",
      "        return \"60s\"\n",
      "    else:\n",
      "        return \"70+\"\n",
      "raw_map['AGE_CAT'] = raw_map.AGE.apply(categorize_age)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"etoh_bin\"></a>\n",
      "######Alcohol Consumption\n",
      "\n",
      "In addition to examining the frequency of alcohol use (Never, Rarely, Occasionally, regularly, or daily), we will also bin alcohol use into those who do use alcohol and those who do not."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def categorize_etoh(x):\n",
      "    if x == 'Never':\n",
      "        return \"No\"\n",
      "    elif isinstance(x, str):\n",
      "        return \"Yes\"\n",
      "    elif np.isnan(x):\n",
      "        return x\n",
      "    \n",
      "raw_map['ALCOHOL_CONSUMPTION'] = raw_map.ALCOHOL_FREQUENCY.apply(categorize_etoh)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"bmi_bin\"></a>\n",
      "######Body Mass Index\n",
      "\n",
      "Maps BMI into large categories, as defined by the <a href=\"http://apps.who.int/bmi/index.jsp?introPage=intro_3.html\">World Health Organization</a>."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Categorizes the BMI into groups\n",
      "def categorize_bmi(x):\n",
      "    if np.isnan(x):\n",
      "        return x\n",
      "    elif x < 18.5:\n",
      "        return \"Underweight\"\n",
      "    elif x < 25:\n",
      "        return \"Normal\"\n",
      "    elif x < 30:\n",
      "        return \"Overweight\"\n",
      "    else:\n",
      "        return \"Obese\"\n",
      "raw_map['BMI_CAT'] = raw_map.BMI.apply(categorize_bmi)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"month_bin\"></a>\n",
      "######Collection Date\n",
      "\n",
      "Maps the collection date to the month and season, agnostic of year."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Categorizes data by collection month and collection season\n",
      "month_map = {-1: [np.nan, np.nan],\n",
      "             np.nan: [np.nan, np.nan],\n",
      "             1: ['January', 'Winter'],\n",
      "             2: ['February', 'Winter'],\n",
      "             3: ['March', 'Spring'],\n",
      "             4: ['April', 'Spring'],\n",
      "             5: ['May', 'Spring'],\n",
      "             6: ['June', 'Summer'],\n",
      "             7: ['July', 'Summer'],\n",
      "             8: ['August', 'Summer'],\n",
      "             9: ['September', 'Fall'],\n",
      "             10: ['October', 'Fall'],\n",
      "             11: ['November', 'Fall'],\n",
      "             12: ['December', 'Winter']}\n",
      "\n",
      "# Maps the data as a month\n",
      "raw_map['COLLECTION_MONTH'] = \\\n",
      "    raw_map.COLLECTION_DATE.apply(lambda x:month_map[x.month][0])\n",
      "\n",
      "# Maps the data as a season\n",
      "raw_map['COLLECTION_SEASON'] = \\\n",
      "    raw_map.COLLECTION_DATE.apply(lambda x:month_map[x.month][1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"state_bin\"></a>\n",
      "######Collection Location\n",
      "\n",
      "We also check the collection state, to insure uniformity in analyses and remove any states which cannot clearly be identified. US States or Canadian providences can be encoded as two letter names (i.e. TX for Texas, BC for British Columbia), or by their full names. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Removes state information for any state not in the US\n",
      "# (This may change as additional countries are added.)\n",
      "countries = raw_map.groupby('COUNTRY').count().STATE.index.values\n",
      "for country in countries:\n",
      "    if country not in {'GAZ:United States of America', 'GAZ:Canada'}:\n",
      "        raw_map.loc[raw_map.COUNTRY == country, 'STATE'] = np.nan\n",
      "\n",
      "# Handles regional mapping, cleaning up states so that only American and\n",
      "# Canadian states are included \n",
      "def check_state(x):\n",
      "    if isinstance(x, str) and x in us_state_map:\n",
      "        return us_state_map[x.upper()]\n",
      "    elif  isinstance(x, str) and x in canadian_map_english:\n",
      "        return canadian_map_english[x.upper()]\n",
      "    else:\n",
      "        return np.nan\n",
      "def census_f(x):\n",
      "    if  isinstance(x, str) and x in regions_by_state:\n",
      "        return regions_by_state[x]['Census_1']\n",
      "    else:\n",
      "        return np.nan\n",
      "\n",
      "def economic_f(x):\n",
      "    if isinstance(x, str) and  x in regions_by_state:\n",
      "        return regions_by_state[x]['Economic']\n",
      "    else:\n",
      "        return np.nan\n",
      "\n",
      "# Applies the functions\n",
      "raw_map['STATE'] = raw_map.STATE.apply(check_state)\n",
      "\n",
      "# Adds the census and economic regions\n",
      "raw_map['CENSUS_REGION'] = raw_map.STATE.apply(census_f)\n",
      "raw_map['ECONOMIC_REGION'] = raw_map.STATE.apply(economic_f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"state_bin\"></a>\n",
      "######Sleep Duration\n",
      "As of round 14, most American Gut participants sleep at least 6 hours a night. Therefore, we will combine indiviudals sleeping less than 5 hours per night (*n*=17) and those sleeping 5-6 hours per night (*n*=176) into a single category."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw_map.loc[ass_map.SLEEP_DURATION == 'Less than 5 hours', 'SLEEP_DURATION'] = 'Less than 6 hours'\n",
      "raw_map.loc[ass_map.SLEEP_DURATION == '5-6 hours', 'SLEEP_DURATION'] = 'Less than 6 hours'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "<a id=\"id_subset\"></a>\n",
      "#### Identifies the \"healthy\" subset\n",
      "\n",
      "<p>Certain health aspects of participants may have known influences on alpha diversity which overwhelms other potential influences. As a result, we chose to filter out individuals who may belong to a group which confounds the data.</p>\n",
      "<p><a href=\"http://www.pnas.org/content/108/Supplement_1/4554.long\">Recent antibiotic use</a> has been shown to affect alpha diversity; only participants who reported not using antibiotics in the last year were considered in this analysis.</p>\n",
      "<p><a href=\"\">Inflammatory Bowel Disease</a>, both <a href=\"http://www.nature.com/srep/2014/140122/srep03814/full/srep03814.html?message-global=remove\">Type I</a> and <a href=\"http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0009085\">Type II</a> diabetes are associated with lower alpha diversity than age-matched controls, so individuals with these conditions were also excluded.</p>\n",
      "<p><p>A relationship between <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2677729/\">Obesity</a> and lower alpha diversity has also been previously observed. In addition, we found that individuals who were considered underweight also had lower diversity than people in the \"normal\" or \"overweight\" BMI categories. As a result, we selected only to include subjects who had a BMI between 18.5 and 25.</p>\n",
      "<p>We start by removing anyone under the age of 20 for three reasons: <a href=\"http://www.nature.com/nature/journal/v486/n7402/full/nature11053.html\">young children</a> have low diversity compared to adults. <a href=\"http://win.niddk.nih.gov/statistics/\">BMI qualifications in children under 18</a> into categories such as \"underweight\", \"normal\", and \"overweight\" depend on the child's age and gender. Finally, we considered alcohol consumption as one of the variables which might affect alpha diversity in this study. Since the overwhelming majority of American Gut participants are American, we chose to set the lower limit of the age range near the legal drinking age to remove potential age-related biases among people who report never drinking.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Reads in the mapping file\n",
      "if 'SUBSET' not in raw_map.columns:\n",
      "    subset_f = {'AGE': lambda x: 19 < x < 70 and not np.isnan(x),\n",
      "                'DIABETES': lambda x: x == 'I do not have diabetes',\n",
      "                'IBD': lambda x: x == 'I do not have IBD',\n",
      "                'ANTIBIOTIC_SELECT': lambda x: x == 'Not in the last year',\n",
      "                'BMI': lambda x: 18.5 <= x < 30 and not np.isnan(x)}\n",
      "\n",
      "    # Determines which samples meet the requirements of the categories\n",
      "    new_bin = {}\n",
      "    for cat, f in subset_f.iteritems():\n",
      "        new_bin[cat] = raw_map[cat].apply(f)\n",
      "\n",
      "    # Builds up the new binary dataframe\n",
      "    bin_frame = pd.DataFrame(new_bin)\n",
      "\n",
      "    # Adds a column to the current dataframe to look at the subset\n",
      "    bin_series = pd.DataFrame(new_bin).all(1)\n",
      "    bin_series.name = 'SUBSET'\n",
      "\n",
      "    raw_map = raw_map.join(bin_series)\n",
      "\n",
      "# Saves the updated mapping file\n",
      "raw_map.loc[raw_otu.ids()]\n",
      "raw_map.to_csv(raw_map_fp,\n",
      "               sep=txt_delim,\n",
      "               na_rep=write_na,\n",
      "               index_label=map_index)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"all_beta\"></a>\n",
      "#### Calcululates beta diversity for all the samples\n",
      "\n",
      "For comparisons between different sample types, we need beta diversity for all samples. To get this, we'll calculate a single rarefaction of the table, and calculate the distance matrix.<br>\n",
      "<em>Note: This takes about 2 hours on a 16gb processor running nothing else.</em>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculates a single rarefaction for the beta diversity\n",
      "if not exists(aawtr_otu_fp) or overwrite:\n",
      "    !single_rarefaction.py -i $aawtu_otu_fp -o $aawtr_otu_fp -d $rarifaction_depth\n",
      "\n",
      "# Gets the beta diversity files\n",
      "if not exists(aawtr_ubd_fp) or not exists(aawtr_wbd_fp) or overwrite:\n",
      "    !beta_diversity.py -i $aawtr_otu_fp -t $tree_fp -m $beta_metrics -o $raw_dir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "<a id=\"splitsite\"></a>\n",
      "#### Splits the OTU table by bodysite\n",
      "\n",
      "Now, we can start stratifying our data. We'll first split the OTU table by bodysite, which the HMP suggests has a large impact on the composition of the microbiome (<a href=\"http://www.nature.com/nature/journal/v486/n7402/abs/nature11234.html\">The Human Microbiome Project Consortium, 2012</a>)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_bodyhabitat(map_fp, otu_fp, habitat):\n",
      "    \"\"\"Splits the raw table by BODY_HABITAT\"\"\"\n",
      "    \n",
      "    if not exists(map_fp) or not exists(otu_fp) or overwrite:\n",
      "        # Gets the map data\n",
      "        map_ = raw_map.loc[raw_map.BODY_HABITAT == habitat]\n",
      "        # Gets the otu table\n",
      "        otu_ = raw_otu.filter(map_.index.values, inplace=False)\n",
      "        # Writes the files\n",
      "        map_.to_csv(map_fp,\n",
      "                    sep=txt_delim,\n",
      "                    index_label=map_index)\n",
      "        write_biom(otu_, otu_fp)\n",
      "    else:\n",
      "        # Gets the map data\n",
      "        map_ = pad_index(pd.read_csv(map_fp,\n",
      "                                     sep=txt_delim, \n",
      "                                     na_values=map_nas,\n",
      "                                     parse_dates=date_cols),\n",
      "                         index_col=map_index)\n",
      "        # Gets the otu table\n",
      "        otu_ = load_table(otu_fp)\n",
      "        \n",
      "    return map_, otu_\n",
      "\n",
      "# Gets the skin table\n",
      "sawtu_map, sawtu_otu = get_bodyhabitat(sawtu_map_fp, sawtu_otu_fp, 'UBERON:skin')\n",
      "oawtu_map, oawtu_otu = get_bodyhabitat(oawtu_map_fp, oawtu_otu_fp, 'UBERON:oral cavity')\n",
      "fawtu_map, fawtu_otu = get_bodyhabitat(fawtu_map_fp, fawtu_otu_fp, 'UBERON:feces')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"filter\"></a>\n",
      "#### Filters the fecal data to remove uncommon OTUs\n",
      "\n",
      "For some analyses, it may be useful to have a set of OTU tables with a single sample per individual. So, we'll read in the mapping file and OTU table, and use these to generate a set of OTUs which are a single sample per individiual. To maximize the number of samples which are retained, samples will be weighted based on the number of sequences in the samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if not exists(ffwtu_otu_fp) or overwrite:\n",
      "    # Determines the number of samples in the OTU table\n",
      "    num_samps = fawtu_otu.ids().shape[0]\n",
      "\n",
      "    # Calculates the number of samples in which an OTU must appear to not be\n",
      "    # filtered out\n",
      "    filt_count = int(min(np.ceil(num_samps*filt_frac), filt_num))\n",
      "\n",
      "    # Filters out the low abundance samples\n",
      "    obs_counts = pd.Series(fawtu_otu.nonzero_counts('observation'),\n",
      "                           index=fawtu_otu.ids('observation'))\n",
      "    obs_keep = obs_counts[obs_counts > filt_count].index\n",
      "\n",
      "    # Gets the filtered table\n",
      "    ffwtu_otu = fawtu_otu.filter(obs_keep, axis='observation')\n",
      "\n",
      "    # Gets the mapping file associated with the OTU table\n",
      "    ffwtu_map = fawtu_map.copy()\n",
      "\n",
      "    # Saves a copy of the updated mapping file in the raw and filtered\n",
      "    # directory\n",
      "    ffwtu_map = fawtu_map.loc[ffwtu_otu.ids()]\n",
      "    ffwtu_map.to_csv(ffwtu_map_fp,\n",
      "                     sep=txt_delim,\n",
      "                     na_rep=write_na,\n",
      "                     index_label=map_index)\n",
      "    # Saves an updated copy of the mapping file\n",
      "    fawtu_map.to_csv(fawtu_map_fp,\n",
      "                     sep=txt_delim,\n",
      "                     na_rep=write_na,\n",
      "                     index_label=map_index)\n",
      "\n",
      "    # Saves a copy of the filtered table\n",
      "    write_biom(ffwtu_otu, ffwtu_otu_fp)\n",
      "\n",
      "else:\n",
      "    ffwtu_otu = load_table(ffwtu_otu_fp)\n",
      "    ffwtu_map = pad_index(pd.read_csv(ffwtu_map_fp,\n",
      "                                      sep=txt_delim,\n",
      "                                      na_values=map_nas,\n",
      "                                      parse_dates=date_cols),\n",
      "                          index_col=map_index)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"rarefaction\"></a>\n",
      "#### Rarefies the data\n",
      "\n",
      "<p>Next, we rarify the data to 10,000 sequences per sample. This depth was chosen since it balances a better picture of diversity with retaining samples. Rarifaction begins by removing samples from the table which do not have the minimum number of counts. Sequences are then drawn randomly out of a weighted pool until we reach the appropriate number. We can repeat the process multiple times to increase the probability that uncommon taxa are selected.</p>\n",
      "\n",
      "<p>We will use this random subsampling technique ten times, which when averaged, is more likely to recapitulate the original diversity.<br><em>Note: due to the size of the tables, this may take a while to run.</em></p>\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Preforms multiple rarifactions at an even depth\n",
      "if not exists(pjoin(skin_rare_dir, rare_filepattern % (rarifaction_depth, 0))) or overwrite:\n",
      "    !multiple_rarefactions_even_depth.py -i $sawtu_otu_fp -o $skin_rare_dir -d $rarifaction_depth -n $num_rarifactions  --lineages_included\n",
      "    \n",
      "if not exists(pjoin(oral_rare_dir, rare_filepattern % (rarifaction_depth, 0))) or overwrite:\n",
      "    !multiple_rarefactions_even_depth.py -i $oawtu_otu_fp -o $oral_rare_dir -d $rarifaction_depth -n $num_rarifactions  --lineages_included\n",
      "\n",
      "if not exists(pjoin(feces_rare_dir, rare_filepattern % (rarifaction_depth, 0))) or overwrite:\n",
      "    !multiple_rarefactions_even_depth.py -i $fawtu_otu_fp -o $feces_rare_dir -d $rarifaction_depth -n $num_rarifactions  --lineages_included\n",
      "    \n",
      "if not exists(pjoin(filt_rare_dir, rare_filepattern % (rarifaction_depth, 0))) or overwrite:\n",
      "    !multiple_rarefactions_even_depth.py -i $ffwtu_otu_fp -o $filt_rare_dir -d $rarifaction_depth -n $num_rarifactions --lineages_included"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"alpha\"></a>\n",
      "#### Calculates the alpha diversity for the filtered and unfiltered table\n",
      "\n",
      "Alpha diversity is a measure of intra sample diversity, or how much variability we find in each sample. There are a variety of ways to calculate alpha diversity. This notebook will calculate four metrics: \n",
      "<a href=\"http://scikit-bio.org/docs/latest/generated/generated/skbio.diversity.alpha.observed_otus.html#skbio.diversity.alpha.observed_otus\">\n",
      "Observed Species Diversity</a>, \n",
      "<a href=\"http://scikit-bio.org/docs/latest/generated/generated/skbio.diversity.alpha.chao1.html#skbio.diversity.alpha.chao1\">\n",
      "Chao1 diversity,</a>\n",
      "<a href=\"http://scikit-bio.org/docs/latest/generated/generated/skbio.diversity.alpha.shannon.html#skbio.diversity.alpha.shannon\">\n",
      "Shannon entropy</a>, and \n",
      "<a href=\"http://www.sciencemag.org/content/308/5728/1635.full\">PD Whole Tree Diversity</a>. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_alpha_diversity(rmap_fp, rotu_fp, rare_dir, alpha_dir,\n",
      "                        umap_, uotu_):\n",
      "    \"\"\"Adds alpha diversity to the mapping data\"\"\"\n",
      "    \n",
      "    if not exists(rmap_fp) or not exists(rotu_fp) or overwrite:\n",
      "        # Sets up a holding object for alpha diversity\n",
      "        alpha_rounds = {'%s' % m:{} for m in alpha_metrics.split(',')}\n",
      "\n",
      "        # Checks the alpha diversity files exist\n",
      "        exist_check = np.array([not exists(pjoin(alpha_dir, alpha_filepattern) \n",
      "                                        % (rarifaction_depth, i))\n",
      "                             for i in range(num_rarifactions)])\n",
      "\n",
      "        # Calculates the alpha diversity for each rarefaction instance\n",
      "        if exist_check.any() or overwrite:\n",
      "            !alpha_diversity.py -i $rare_dir -o $alpha_dir -m $alpha_metrics -t $tree_fp \n",
      "\n",
      "        # Loads the alpha diversity \n",
      "        for i in xrange(num_rarifactions):\n",
      "            # Reads in the alpha diversity\n",
      "            alpha_fp = pjoin(alpha_dir, alpha_filepattern) % (rarifaction_depth, i)\n",
      "            alpha = pad_index(pd.read_csv(alpha_fp, sep=txt_delim), 'Unnamed: 0')\n",
      "\n",
      "            # Extracts the data\n",
      "            for col in alpha_rounds:\n",
      "                alpha_rounds['%s' % col]['%i' % i] = alpha[col]\n",
      "                alpha_rounds['%s' % col]['%i' % i].name = '%i' % i\n",
      "\n",
      "            # Calculates metrics for each set of data\n",
      "            metrics = {}\n",
      "            for metric in alpha_rounds:\n",
      "                metrics['%s_mean' % metric] = pd.DataFrame(alpha_rounds[metric]).mean(1)\n",
      "                metrics['%s_stdv' % metric] = pd.DataFrame(alpha_rounds[metric]).std(1)\n",
      "                metrics['%s_med' % metric] = pd.DataFrame(alpha_rounds[metric]).median(1)\n",
      "\n",
      "        # Converts the metrics to a single dataframe\n",
      "        alpha_df = pd.DataFrame(metrics)\n",
      "\n",
      "        # Appends the data to the mapping file\n",
      "        rmap_ = umap_.copy()\n",
      "        rmap_ = rmap_.join(alpha_df)\n",
      "\n",
      "        # Selects the appropriate metric\n",
      "        all_rounds = pd.DataFrame(alpha_rounds[div_metric])\n",
      "        all_rounds = all_rounds.sort_index()\n",
      "\n",
      "        # Selects the means\n",
      "        alpha_df = alpha_df.sort_index()\n",
      "\n",
      "        # Calculates the distance between the two\n",
      "        mean_rounds = ([alpha_df['%s_mean' % div_metric].values]*np.ones((num_rarifactions, 1))).transpose()\n",
      "        diff = np.sqrt(np.square(all_rounds.values - mean_rounds))/mean_rounds\n",
      "\n",
      "        # Determines the minimum distance\n",
      "        round_labels = np.arange(0, 10)\n",
      "        round_avg = diff.mean(0)\n",
      "\n",
      "        # Identifies the best round\n",
      "        best_rarifaction = round_labels[round_avg == min(round_avg)][0]\n",
      "\n",
      "        # Saves the best rarefaction instance as an OTU table\n",
      "        copy2(pjoin(rare_dir, rare_filepattern % (rarifaction_depth, i)), rotu_fp)\n",
      "\n",
      "        # Loads the OTU table\n",
      "        rotu_ = load_table(rotu_fp)\n",
      "\n",
      "        # Cleans up the rarefied mapping file\n",
      "        rmap_ = rmap_.loc[rotu_.ids()]\n",
      "\n",
      "        # Saves the updated mapping file\n",
      "        rmap_.to_csv(rmap_fp, \n",
      "                     sep=txt_delim, \n",
      "                     na_rep=write_na, \n",
      "                     index_label=map_index)\n",
      "    else:\n",
      "        # Loads the out table and mapping file\n",
      "        rotu_ = load_table(rotu_fp)\n",
      "        rmap_ = pad_index(pd.read_csv(rmap_fp,\n",
      "                                      sep=txt_delim, \n",
      "                                      na_values=map_nas,\n",
      "                                      parse_dates=date_cols),\n",
      "                         index_col=map_index)\n",
      "\n",
      "    # Filters out OTUs with zero counts\n",
      "    rotu_ = filter_emtpy_otus(rotu_)\n",
      "    \n",
      "    return rmap_, rotu_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Gets the alpha diversity\n",
      "fawtr_map, fawtr_otu = add_alpha_diversity(fawtr_map_fp, fawtr_otu_fp,\n",
      "                                           feces_rare_dir, feces_alpha_dir,\n",
      "                                           fawtu_map, fawtu_otu)\n",
      "ffwtr_map, ffwtr_otu = add_alpha_diversity(ffwtr_map_fp, ffwtr_otu_fp,\n",
      "                                           filt_rare_dir, filt_alpha_dir,\n",
      "                                           ffwtu_map, ffwtu_otu)\n",
      "oawtr_map, oawtr_otu = add_alpha_diversity(oawtr_map_fp, oawtr_otu_fp,\n",
      "                                           oral_rare_dir, oral_alpha_dir,\n",
      "                                           oawtu_map, oawtu_otu)\n",
      "sawtr_map, sawtr_otu = add_alpha_diversity(sawtr_map_fp, sawtr_otu_fp,\n",
      "                                           skin_rare_dir, skin_alpha_dir,\n",
      "                                           sawtu_map, sawtu_otu)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"single\"></a>\n",
      "#### Identifies single samples in the OTU tables and mapping files.\n",
      "\n",
      "Single samples will be selected from the rarefied set, so the comparison can be made for samples which we know will can be used at at least the specified rarefaction depth. The samples will also be selected from the filtered table, to maximize the possibility they will appear in both the raw and the filtered tables.\n",
      "\n",
      "Individuals can be identified by the <code>HOST_SUBJECT_ID</code>."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Gets a single sample for skin samples\n",
      "if not exists(sastr_otu_fp) or not exists(sastu_otu_fp) or overwrite:\n",
      "    # Prealocates a place for the ids\n",
      "    single_ids = np.array([])\n",
      "    \n",
      "    # Groups the samples by host subject id\n",
      "    host_grouped = sawtr_map.groupby('HOST_SUBJECT_ID')\n",
      "    \n",
      "    # Gets a single sample per individual\n",
      "    for indv, ids in host_grouped.groups.iteritems():\n",
      "        if len(ids) == 1:\n",
      "            single_ids = np.hstack((single_ids, ids))\n",
      "        else:\n",
      "            single_ids = np.hstack((single_ids, np.random.choice(ids, 1)))\n",
      "    \n",
      "    # Filters the table to a single sample\n",
      "    sastu_otu = sawtu_otu.filter(single_ids, inplace=False)\n",
      "    sastr_otu = sawtr_otu.filter(single_ids, inplace=False)\n",
      "    sastu_map = sawtu_map.loc[single_ids]\n",
      "    sastr_map = sawtr_map.loc[single_ids]\n",
      "    \n",
      "    # Filters out otus which are 0 in the dataset\n",
      "    sastu_otu = filter_emtpy_otus(sastu_otu)\n",
      "    sastr_otu = filter_emtpy_otus(sastr_otu)\n",
      "    \n",
      "    # Saves the files\n",
      "    sastu_map.to_csv(sastu_map_fp,\n",
      "                     sep=txt_delim, \n",
      "                     na_rep=write_na, \n",
      "                     index_label=map_index)\n",
      "    sastr_map.to_csv(sastr_map_fp, \n",
      "                     sep=txt_delim, \n",
      "                     na_rep=write_na, \n",
      "                     index_label=map_index)\n",
      "\n",
      "    # Saves the OTU tables\n",
      "    write_biom(sastu_otu, sastu_otu_fp)\n",
      "    write_biom(sastr_otu, sastr_otu_fp)\n",
      "else:\n",
      "    sastu_map = pad_index(pd.read_csv(sastu_map_fp,\n",
      "                                      sep=txt_delim, \n",
      "                                      na_values=map_nas,\n",
      "                                      parse_dates=date_cols),\n",
      "                          index_col=map_index)\n",
      "    sastr_map = pad_index(pd.read_csv(sastr_map_fp,\n",
      "                                      sep=txt_delim, \n",
      "                                      na_values=map_nas,\n",
      "                                      parse_dates=date_cols),\n",
      "                          index_col=map_index)\n",
      "    sastr_otu = load_table(sastr_otu_fp)\n",
      "    sastr_otu = load_table(sastr_otu_fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Gets a single sample for oral samples\n",
      "if not exists(oastr_otu_fp) or not exists(oastu_otu_fp) or overwrite:\n",
      "    # Prealocates a place for the ids\n",
      "    single_ids = np.array([])\n",
      "    \n",
      "    # Groups the samples by host subject id\n",
      "    host_grouped = oawtr_map.groupby('HOST_SUBJECT_ID')\n",
      "    \n",
      "    # Gets a single sample per individual\n",
      "    for indv, ids in host_grouped.groups.iteritems():\n",
      "        if len(ids) == 1:\n",
      "            single_ids = np.hstack((single_ids, ids))\n",
      "        else:\n",
      "            single_ids = np.hstack((single_ids, np.random.choice(ids, 1)))\n",
      "    \n",
      "    # Filters the table to a single sample\n",
      "    oastu_otu = oawtu_otu.filter(single_ids, inplace=False)\n",
      "    oastr_otu = oawtr_otu.filter(single_ids, inplace=False)\n",
      "    oastu_map = oawtu_map.loc[single_ids]\n",
      "    oastr_map = oawtr_map.loc[single_ids]\n",
      "    \n",
      "    # Filters the single samples to remove empty otus\n",
      "    oastu_otu = filter_emtpy_otus(oastu_otu)\n",
      "    oastr_otu = filter_emtpy_otus(oastr_otu)\n",
      "    \n",
      "    # oaves the files\n",
      "    oastu_map.to_csv(oastu_map_fp,\n",
      "                     sep=txt_delim, \n",
      "                     na_rep=write_na, \n",
      "                     index_label=map_index)\n",
      "    oastr_map.to_csv(oastr_map_fp, \n",
      "                     sep=txt_delim, \n",
      "                     na_rep=write_na, \n",
      "                     index_label=map_index)\n",
      "\n",
      "    # oaves the OTU tables\n",
      "    write_biom(oastu_otu, oastu_otu_fp)\n",
      "    write_biom(oastr_otu, oastr_otu_fp)\n",
      "else:\n",
      "    oastu_map = pad_index(pd.read_csv(oastu_map_fp,\n",
      "                                      sep=txt_delim, \n",
      "                                      na_values=map_nas,\n",
      "                                      parse_dates=date_cols),\n",
      "                          index_col=map_index)\n",
      "    oastr_map = pad_index(pd.read_csv(oastr_map_fp,\n",
      "                                      sep=txt_delim, \n",
      "                                      na_values=map_nas,\n",
      "                                      parse_dates=date_cols),\n",
      "                          index_col=map_index)\n",
      "    oastr_otu = load_table(oastr_otu_fp)\n",
      "    oastr_otu = load_table(oastr_otu_fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Gets single sample ids for fecal samples\n",
      "if not exists(fawsu_otu_fp) or not exists(ffwsr_map_fp) or overwrite:\n",
      "    # Prealocates a place for the ids\n",
      "    single_ids = np.array([])\n",
      "\n",
      "    # Groups the filtered samples by the HOST SUBJECT ID\n",
      "    host_grouped = ffwtr_map.groupby('HOST_SUBJECT_ID')\n",
      "\n",
      "    # Loops through the host subject IDS\n",
      "    for indv, ids in host_grouped.groups.iteritems():\n",
      "        # Checks if a single sample has been submitted or passed rarefaction\n",
      "        # for the indiviudal\n",
      "        if len(ids) == 1:\n",
      "            single_ids = np.hstack((single_ids, ids))\n",
      "        # Otherwise, a random sample is selected for the indiviudal\n",
      "        else:\n",
      "            single_ids = np.hstack((single_ids, np.random.choice(ids, 1)))\n",
      "\n",
      "    # Filters the OTU tables down to single samples\n",
      "    fawsu_otu = fawtu_otu.filter(single_ids, inplace=False)\n",
      "    ffwsu_otu = ffwtu_otu.filter(single_ids, inplace=False)\n",
      "    fawsr_otu = fawtr_otu.filter(single_ids, inplace=False)\n",
      "    ffwsr_otu = ffwtr_otu.filter(single_ids, inplace=False)\n",
      "\n",
      "    # Filters the mapping files down to single samples\n",
      "    fawsu_map = fawtu_map.loc[fawsu_otu.ids()]\n",
      "    ffwsu_map = ffwtu_map.loc[ffwsu_otu.ids()]\n",
      "    fawsr_map = fawtr_map.loc[fawsr_otu.ids()]\n",
      "    ffwsr_map = ffwtr_map.loc[ffwsr_otu.ids()]\n",
      "\n",
      "    # Filters the single samples to remove empty otus\n",
      "    fawsu_otu = filter_emtpy_otus(fawsu_otu)\n",
      "    ffwsu_otu = filter_emtpy_otus(ffwsu_otu)\n",
      "    fawsr_otu = filter_emtpy_otus(fawsr_otu)\n",
      "    ffwsr_otu = filter_emtpy_otus(ffwsr_otu)\n",
      "\n",
      "    # Saves the mapping files\n",
      "    fawsu_map.to_csv(fawsu_map_fp, \n",
      "                     sep=txt_delim, \n",
      "                     na_rep=write_na, \n",
      "                     index_label=map_index)\n",
      "    ffwsu_map.to_csv(ffwsu_map_fp, \n",
      "                     sep=txt_delim, \n",
      "                     na_rep=write_na, \n",
      "                     index_label=map_index)\n",
      "    fawsr_map.to_csv(fawsr_map_fp, \n",
      "                     sep=txt_delim, \n",
      "                     na_rep=write_na, \n",
      "                     index_label=map_index)\n",
      "    ffwsr_map.to_csv(ffwsr_map_fp, \n",
      "                     sep=txt_delim, \n",
      "                     na_rep=write_na, \n",
      "                     index_label=map_index)\n",
      "\n",
      "    # Saves the OTU tables\n",
      "    write_biom(fawsu_otu, fawsu_otu_fp)\n",
      "    write_biom(ffwsu_otu, ffwsu_otu_fp)\n",
      "    write_biom(fawsr_otu, fawsr_otu_fp)\n",
      "    write_biom(ffwsr_otu, ffwsr_otu_fp)\n",
      "\n",
      "else:\n",
      "    # Loads the mapping files\n",
      "    fawsu_map = pad_index(pd.read_csv(fawsu_map_fp,\n",
      "                                      sep=txt_delim, \n",
      "                                      na_values=map_nas,\n",
      "                                      parse_dates=date_cols),\n",
      "                          index_col=map_index)\n",
      "    ffwsu_map = pad_index(pd.read_csv(ffwsu_map_fp,\n",
      "                                      sep=txt_delim, \n",
      "                                      na_values=map_nas,\n",
      "                                      parse_dates=date_cols),\n",
      "                          index_col=map_index)\n",
      "    fawsr_map = pad_index(pd.read_csv(fawsr_map_fp,\n",
      "                                      sep=txt_delim, \n",
      "                                      na_values=map_nas,\n",
      "                                      parse_dates=date_cols),\n",
      "                          index_col=map_index)\n",
      "    ffwsr_map = pad_index(pd.read_csv(ffwsr_map_fp,\n",
      "                                      sep=txt_delim, \n",
      "                                      na_values=map_nas,\n",
      "                                      parse_dates=date_cols),\n",
      "                          index_col=map_index)\n",
      "    # Loads the OTU tables\n",
      "    fawsu_otu = load_table(fawsu_otu_fp)\n",
      "    ffwsu_otu = load_table(ffwsu_otu_fp)\n",
      "    fawsr_otu = load_table(fawsr_otu_fp)\n",
      "    ffwsr_otu = load_table(ffwsr_otu_fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"subset\"></a>\n",
      "#### Splits the table into the healthy subsets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def save_subset_table(all_map, all_otu, sub_map_fp, sub_otu_fp):\n",
      "    \"\"\"Saves the subset data\"\"\"\n",
      "    # Writes the file if it does not exist\n",
      "    if not exists(sub_map_fp) or not exists(sub_otu_fp) or overwrite:\n",
      "        # Filters the mapping table to remove any samples not in the OTU table\n",
      "        all_map = all_map.loc[all_otu.ids()]\n",
      "\n",
      "        # Gets the subset table and otu table\n",
      "        sub_map = all_map.groupby('SUBSET').get_group(True)\n",
      "        sub_otu = all_otu.filter(sub_map.index.values)\n",
      "        \n",
      "        sub_otu = filter_emtpy_otus(sub_otu)\n",
      "\n",
      "        # Saves the map and otu table\n",
      "        sub_map.to_csv(sub_map_fp, \n",
      "                       sep=txt_delim, \n",
      "                       na_rep=write_na, \n",
      "                       index_label=map_index)\n",
      "        write_biom(sub_otu, sub_otu_fp)\n",
      "    else:\n",
      "        sub_map = pad_index(pd.read_csv(sub_map_fp,\n",
      "                                        sep=txt_delim, \n",
      "                                        na_values=map_nas,\n",
      "                                        parse_dates=date_cols),\n",
      "                            index_col=map_index)\n",
      "        sub_otu = load_table(sub_otu_fp)\n",
      "    return sub_map, sub_otu\n",
      "\n",
      "# Sets up a set of maps, otus, and filepaths\n",
      "all_data = [[fawtu_map, fawtu_otu, fastu_map_fp, fastu_otu_fp],\n",
      "            [fawtr_map, fawtr_otu, fastr_map_fp, fastr_otu_fp],\n",
      "            [fawsu_map, fawsu_otu, fassu_map_fp, fassu_otu_fp],\n",
      "            [fawsr_map, fawsr_otu, fassr_map_fp, fassr_otu_fp],\n",
      "            [ffwtu_map, ffwtu_otu, ffstu_map_fp, ffstu_otu_fp],\n",
      "            [ffwtr_map, ffwtr_otu, ffstr_map_fp, ffstr_otu_fp],\n",
      "            [ffwsu_map, ffwsu_otu, ffssu_map_fp, ffssu_otu_fp],\n",
      "            [ffwsr_map, ffwsr_otu, ffssr_map_fp, ffssr_otu_fp]]\n",
      "\n",
      "# Loops through the data, and writes the subset data\n",
      "single_tables = []\n",
      "for save_set in all_data:\n",
      "    single_tables.append(save_subset_table(*save_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[(fastu_map, fatsu_otu), (fastr_map, fastr_otu),\n",
      " (fassu_map, fassu_otu), (fassr_map, fassr_otu),\n",
      " (ffstu_map, ffstu_otu), (ffstr_map, ffstr_otu),\n",
      " (ffssu_map, ffssu_otu), (ffssr_map, ffssr_otu)] = single_tables"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"beta\"></a>\n",
      "#### Gets the beta diversity for the tables\n",
      "\n",
      "We can use the full sample table and divide it to produce a "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculates beta diveristy for the filtered fecal table\n",
      "fun_beta_fp = pjoin(ffwt_dir, 'unweighted_unifrac_%s.txt' \n",
      "                    % split(ffwtr_otu_fp)[1].replace('.biom', ''))\n",
      "fwe_beta_fp = pjoin(ffwt_dir, 'weighted_unifrac_%s.txt' \n",
      "                    % split(ffwtr_otu_fp)[1].replace('.biom', ''))\n",
      "if not exists(fun_beta_fp) or not exists(fwe_beta_fp) or overwrite:\n",
      "    !beta_diversity.py -i $ffwtr_otu_fp -o $ffwt_dir -t $tree_fp -m $beta_metrics\n",
      "\n",
      "# Loads the raw beta diveristy matrix and the filtered distance matrix\n",
      "aawtr_ubd = DistanceMatrix.from_file(aawtr_ubd_fp)\n",
      "aawtr_wbd = DistanceMatrix.from_file(aawtr_wbd_fp)\n",
      "ffwtr_ubd = DistanceMatrix.from_file(fun_beta_fp)\n",
      "ffwtr_wbd = DistanceMatrix.from_file(fwe_beta_fp)\n",
      "\n",
      "# Sets up the list of file directories\n",
      "all_ = [(fawt_dir, fawtr_otu_fp, fawtr_otu),\n",
      "        (fast_dir, fastr_otu_fp, fawsr_otu),\n",
      "        (faws_dir, fawsr_otu_fp, fawsr_otu), \n",
      "        (fass_dir, fassr_otu_fp, fassr_otu),\n",
      "        (sawt_dir, sawtr_otu_fp, sawtr_otu), \n",
      "        (sast_dir, sastr_otu_fp, sastr_otu),\n",
      "        (oawt_dir, oawtr_otu_fp, oawtr_otu), \n",
      "        (oast_dir, oastr_otu_fp, oastr_otu)]\n",
      "\n",
      "filt = [(ffst_dir, ffstr_otu_fp, ffstr_otu), \n",
      "        (ffws_dir, ffwsr_otu_fp, ffwsr_otu),\n",
      "        (ffss_dir, ffssr_otu_fp, ffssr_otu)]\n",
      "\n",
      "# Calculates the beta diverity for the tables\n",
      "for (out_dir, otu_fp, otu_) in all_:\n",
      "    un_beta_fp = pjoin(out_dir, 'unweighted_unifrac_%s.txt' % split(otu_fp)[1].replace('.biom', ''))\n",
      "    we_beta_fp = pjoin(out_dir, 'weighted_unifrac_%s.txt' % split(otu_fp)[1].replace('.biom', ''))\n",
      "    aawtr_ubd.filter(otu_.ids()).to_file(un_beta_fp)\n",
      "    aawtr_wbd.filter(otu_.ids()).to_file(we_beta_fp)\n",
      "for (out_dir, otu_fp, otu_) in filt:\n",
      "    un_beta_fp = pjoin(out_dir, 'unweighted_unifrac_%s.txt' % split(otu_fp)[1].replace('.biom', ''))\n",
      "    we_beta_fp = pjoin(out_dir, 'weighted_unifrac_%s.txt' % split(otu_fp)[1].replace('.biom', ''))\n",
      "    ffwtr_ubd.filter(otu_.ids()).to_file(un_beta_fp)\n",
      "    ffwtr_wbd.filter(otu_.ids()).to_file(we_beta_fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We've now generated all the files needed for initial analysis. Other files which are needed will can be generated in the notebooks.\n",
      "\n",
      "We will finish by deleting the intermediate files, if that appropriate."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if not retain_intermediate:\n",
      "    !rm -r $working_dir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}