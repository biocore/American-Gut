{
 "metadata": {
  "name": "",
  "signature": "sha256:c9758c6fca2bad86448a9c5aeaf9d8d93b5d1999b9d3e366c7872fd574d420c0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id=\"intro\"></a>\n",
      "# Power Analysis\n",
      "[Introduction to notebook]\n",
      "\n",
      "####Definition\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "####Caveats and Considerations\n",
      "This code functions on three assumptions about the data:\n",
      "1. The samples are a representative, <a href=\"http://en.wikipedia.org/wiki/Simple_random_sample\">random sample</a> of the underlying population. \n",
      "2. Samples are suffeciently large to allow random subsampling. My rule of thumb is around 30 observations in the smallest sample.\n",
      "3. There is a significant difference between the samples, or reason to believe there is a significant difference between the populations.\n",
      "4. Samples must satisfy any requirements of the statistical test, $f$ being used.\n",
      "\n",
      "####Notebook Requirements\n",
      "<ul><li><a href=\"https://www.python.org/download/releases/2.7/\">Python 2.7</a>\n",
      "</li><li><a href=\"https://pypi.python.org/pypi/numpy\">Numpy</a> $\\geq$ 1.7\n",
      "</li><li><a href=\"http://www.scipy.org\">Scipy 0.14.0</a>\n",
      "</li><li><a href=\"http://matplotlib.org\">Matplotlib</a> $\\geq$ 1.3\n",
      "</li><li><a href=\"http://qiime.org/install/install.html#latest-development-version\">Qiime 1.8-dev</a> commit 3e41198d2e43ef6c9492f994043c851b08268748\n",
      "</li><li><a href=\"http://biom-format.org\">Biom 2.0.1</a>\n",
      "</li><li><a href=\"http://scikit-bio.org\">Scikit Bio 0.2.10-dev</a> commit c90764add74040a937d81d647481dbe958d241d6\n",
      "</li><li><a href=\"http://pandas.pydata.org\">Pandas 0.14.1</a>\n",
      "</li><li><a href=\"http://statsmodels.sourceforge.net\">StatsModels 0.5.0</a>\n",
      "</li><li><a href=\"http://ipython.org\">iPython</a>\n",
      "</li><li>Custom code libraries, alpha_analysis.py and power.py, currently avalible through the <a href=\"https://github.com/biocore/American-Gut\">American Gut Github repository</a>.\n",
      "</li></ul>\n",
      "\n",
      "<a id=\"top\"></a>\n",
      "####Table of contents\n",
      "<ul><li><a href=\"#intro\">Introduction</a>\n",
      "</li><li><a href=\"#f_import\">Imports test function</a>\n",
      "</li><li><a href=\"#fun\">Defines analysis functions</a>\n",
      "</li><li><a href=\"#parameters\">Sets analysis parameters</a>\n",
      "</li><li><a href=\"#ifilepath\">Imports necessary files</a>\n",
      "</li><li><a href=\"#clean\">Cleans up the mapping categories</a>\n",
      "</li><li><a href=\"#functions\">Defines test functions</a>\n",
      "</li><li><a href=\"#bodysite\">Calculates power for alpha and beta diversity across body sites</a> \n",
      "</li><li><a href=\"#fecal_power\">Calculates power for alpha and beta diversity, focusing only on the fecal samples</a>\n",
      "</li><li><a href=\"#effect\">Estimates the effect size</a>\n",
      "</li><li><a href=\"#curves\">Plots power curves</a>\n",
      "\n",
      "<a id=\"f_import\"></a>\n",
      "We will start by importing necessary functions, and determining if files should be overwritten."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import remove, rename\n",
      "from os.path import abspath, isfile, exists, join as pjoin\n",
      "from shutil import move\n",
      "from copy import deepcopy\n",
      "from future.utils import viewitems\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "from biom import load_table\n",
      "\n",
      "from scipy.stats import kruskal, nanmean\n",
      "\n",
      "from matplotlib import rcParams\n",
      "import matplotlib.pylab as plt\n",
      "\n",
      "from americangut.diversity_analysis import check_dir, pad_index"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/jwdebelius/.virtualenvs/all_about_the_dev/lib/python2.7/site-packages/matplotlib/__init__.py:1312: UserWarning:  This call to matplotlib.use() has no effect\n",
        "because the backend has already been chosen;\n",
        "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
        "or matplotlib.backends is imported for the first time.\n",
        "\n",
        "  warnings.warn(_use_error_msg)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will also set up some plotting parameters so the generated figures use Helvetica or Arial as their default font."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sets up plotting parameters so that the default setting is use to Helvetica\n",
      "# in plots\n",
      "rcParams['font.family'] = 'sans-serif'\n",
      "rcParams['font.sans-serif'] = ['Helvetica', 'Arial']\n",
      "rcParams['text.usetex'] = True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"funs\"></a>\n",
      "We can write two additional functions. <strong><code>collate_effect_size</code></strong>, which will take a vector (1d array) of the number of samples drawn, and an array of power values calculated, and return a mean effect size with a confidence interval. <strong><code>plot_effects</code></strong> will leverage the functionality from statsmodels to generate a base power curve, and then modify that base power curve to show the confidence interval, allow line color changes, and other adjustments."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "def collate_effect_size(counts, powers, alpha):\n",
      "    \"\"\"Calculates the effects for power values\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    counts : array\n",
      "        the number of samples used to calculate the power\n",
      "\n",
      "    powers : list, ndarray\n",
      "        list of arrays of power values. If there are multiple power arrays,\n",
      "        each power array should have the same dimensions. If samples are\n",
      "        missing, these should be denoted by a nan.\n",
      "\n",
      "    alpha : float\n",
      "        the critical value used to calculate the power.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    effect_means : 1d array\n",
      "    effect_bounds : 1d array\n",
      "\n",
      "    Raises\n",
      "    ------\n",
      "    TypeError\n",
      "        if counts is not a one-dimensional array\n",
      "    ValueError\n",
      "        if the arrays in powers have different shapes\n",
      "    ValueError\n",
      "        if the length of the power arrays and the length of the count arrays\n",
      "        are different\n",
      "    \"\"\"\n",
      "\n",
      "    # Checks the power and counts iteratibility\n",
      "    if isinstance(powers, np.ndarray):\n",
      "        powers = [powers]\n",
      "\n",
      "    num_powers = len(powers)\n",
      "    if isinstance(counts, np.ndarray):\n",
      "        counts = [counts]*num_powers\n",
      "\n",
      "    # Checks there is a count for each power\n",
      "    if not len(counts) == len(powers):\n",
      "        raise ValueError('There must be a counts array for each power array.')\n",
      "\n",
      "    # Checks the shape array\n",
      "    for idx in xrange(num_powers):\n",
      "        count_shape = counts[idx].shape\n",
      "        power_shape = powers[idx].shape\n",
      "        # Checks the count array is 1d\n",
      "        if not len(count_shape) == 1:\n",
      "            raise TypeError('Each count array must be a 1d array.')\n",
      "\n",
      "        if len(power_shape) == 1:\n",
      "            if not count_shape[0] == power_shape[0]:\n",
      "                raise ValueError('There must be a sample count for each '\n",
      "                                 'power.')\n",
      "        elif not count_shape[0] == power_shape[1]:\n",
      "            raise ValueError('There must be a sample count for each power.')\n",
      "\n",
      "    # Prealocates the output arrays\n",
      "    effect_means = np.zeros((num_powers))\n",
      "    effect_bounds = np.zeros((num_powers))\n",
      "\n",
      "    # Iterates through the powers and calculates the effect sizes\n",
      "    for idp, pwr in enumerate(powers):\n",
      "        count = counts[idp]\n",
      "        pwr_shape = pwr.shape\n",
      "        # Calculates the effect size for the power array\n",
      "        eff = np.zeros(pwr_shape)\n",
      "        for id2, cnt in enumerate(count):\n",
      "            if len(pwr_shape) == 1:\n",
      "                eff[id2] = np.nan\n",
      "                try:\n",
      "                    eff[id2] = ft.solve_power(effect_size=None,\n",
      "                                              nobs=cnt,\n",
      "                                              alpha=alpha,\n",
      "                                              power=pwr[id2])\n",
      "                except:\n",
      "                    pass\n",
      "            else:\n",
      "                for id1 in xrange(pwr_shape[0]):\n",
      "                    eff[id1, id2] = np.nan\n",
      "                    try:\n",
      "                        eff[id1, id2] = ft.solve_power(effect_size=None,\n",
      "                                                       nobs=cnt,\n",
      "                                                       alpha=alpha,\n",
      "                                                       power=pwr[id1, id2])\n",
      "                    except:\n",
      "                        pass\n",
      "        # Caluclates the mean and bound\n",
      "        if np.isnan(eff).all():\n",
      "            effect_means[idp] = np.nan\n",
      "            effect_bounds[idp] = np.nan\n",
      "        else:\n",
      "            effect_means[idp] = np.nanmean(eff, None)\n",
      "            effect_bounds[idp] = confidence_bound(eff, alpha, None)\n",
      "\n",
      "    return effect_means, effect_bounds\n",
      "\n",
      "def plot_effects(effect_means, effect_bounds, labels, sample_counts, **kwargs):\n",
      "    \"\"\"Makes a power curve plot\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    effect_means: 1d array\n",
      "        the mean effect sizes to plots.\n",
      "\n",
      "    effect_bounds : {None, 1d array}\n",
      "        the range used for the confidence interval. If there is no effect to\n",
      "        show, this should be None.\n",
      "\n",
      "    labels : 1d array\n",
      "        a list of formatted strings describing the effects, to be used in the\n",
      "        legend.\n",
      "\n",
      "    sample_counts : 1d array\n",
      "        the counts where power should be calculated.\n",
      "\n",
      "    alpha : int, optional\n",
      "        Default is 0.05. The critical value for the power curves.\n",
      "\n",
      "    colormap : {None, array}, optional\n",
      "        Default is None. A colormap to use for the lines. Each color\n",
      "        designation must appear in a new row. If no colormap is supplied, the\n",
      "        defualt colormap will be used.\n",
      "\n",
      "    grid : bool, optional\n",
      "        Default is True. Show grid.\n",
      "\n",
      "    show_bound : bool\n",
      "        Default is True. Shows the confidence bounds on the effect size. If\n",
      "        `effect_bounds` is None, no bounds will be shown.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    fig : figure\n",
      "        a figure with the power curves plotted.\n",
      "\n",
      "    Other parameters\n",
      "    ----------------\n",
      "    leg_offset : tuple\n",
      "        Changes the legend position.\n",
      "\n",
      "    tick_size : usigned int\n",
      "        sets the font size for tick labels\n",
      "\n",
      "    label_size : unsigned int\n",
      "        sets the font size for the axis labels\n",
      "\n",
      "    title_size : unsigned int\n",
      "        sets the font size for the title\n",
      "\n",
      "    legend_size : unsigned int\n",
      "        sets the font size for enteries in the legend\n",
      "\n",
      "    \"\"\"\n",
      "    # Sets the keyword properties\n",
      "    kwds = {'alpha': 0.05,\n",
      "            'colormap': None,\n",
      "            'grid': True,\n",
      "            'title': '',\n",
      "            'show_bound': True,\n",
      "            'leg_offset': None,\n",
      "            'tick_size': 12,\n",
      "            'label_size': 15,\n",
      "            'title_size': 18,\n",
      "            'legend_size': 11}\n",
      "    for key, value in viewitems(kwargs):\n",
      "        if key in kwds:\n",
      "            kwds[key] = value\n",
      "        else:\n",
      "            raise ValueError('%s is not a property of plot_effects.' % key)\n",
      "\n",
      "    # Checks the effect, bound, and mean argument is sane\n",
      "    mean_shape = effect_means.shape\n",
      "    if effect_bounds is None:\n",
      "        kwds['show_bound'] = False\n",
      "        effect_bounds = zeros(mean_shape)\n",
      "    bound_shape = effect_bounds.shape\n",
      "    label_shape = labels.shape\n",
      "\n",
      "    if not len(mean_shape) == 1:\n",
      "        raise ValueError('Effect Mean must be a 1d numpy array')\n",
      "    elif mean_shape != bound_shape or mean_shape != label_shape:\n",
      "        raise ValueError('There must be a label and bound for each effect.')\n",
      "\n",
      "    # Plots the the lower bound data\n",
      "    fig = ft.plot_power(dep_var='nobs',\n",
      "                        nobs=sample_counts,\n",
      "                        effect_size=effect_means - effect_bounds,\n",
      "                        alpha=kwds['alpha'])\n",
      "    # Gets the axis of the first plot and its position\n",
      "    lax = fig.axes[0]\n",
      "    # Makes the lower bound lines dashed and thin, and changes the color if\n",
      "    # desired\n",
      "    for idx, l in enumerate(lax.get_lines()):\n",
      "        l.set_linestyle(':')\n",
      "        l.set_linewidth(1.5)\n",
      "        if kwds['colormap'] is not None and len(kwds['colormap'].shape) == 1:\n",
      "            l.set_color(kwds['colormap'][idx])\n",
      "        elif kwds['colormap'] is not None:\n",
      "            l.set_color(kwds['colormap'][idx, :])\n",
      "    # Hides the x ticks and labels\n",
      "    lax.set_title('')\n",
      "    lax.set_xticklabels('')\n",
      "    lax.set_yticklabels('')\n",
      "    lax.set_xlabel('')\n",
      "    # Hides the legend\n",
      "    lax.get_legend().set_visible(False)\n",
      "\n",
      "    # Plots the upper bound data\n",
      "    uax = fig.add_axes(lax.get_position())\n",
      "    fig = ft.plot_power('nobs', sample_counts, effect_means + effect_bounds,\n",
      "                        alpha=kwds['alpha'], ax=uax)\n",
      "    # Makes the lower bound axes visable, if desired\n",
      "    if kwds['show_bound']:\n",
      "        uax.set_axis_bgcolor('none')\n",
      "    # Makes the lower bound lines dashed and thin, and changes the color if\n",
      "    # desired\n",
      "    for idx, l in enumerate(uax.get_lines()):\n",
      "        l.set_linestyle(':')\n",
      "        l.set_linewidth(1.5)\n",
      "        if kwds['colormap'] is not None and len(kwds['colormap'].shape) == 1:\n",
      "            l.set_color(kwds['colormap'][idx])\n",
      "        elif kwds['colormap'] is not None:\n",
      "            l.set_color(kwds['colormap'][idx, :])\n",
      "    # Hides the x ticks and labels\n",
      "    uax.set_title('')\n",
      "    uax.set_xticklabels('')\n",
      "    uax.set_yticklabels('')\n",
      "    uax.set_xlabel('')\n",
      "    # Hides the legend\n",
      "    uax.get_legend().set_visible(False)\n",
      "\n",
      "    # Plots the mean data\n",
      "    axm = fig.add_axes(lax.get_position())\n",
      "    fig = ft.plot_power('nobs', sample_counts, effect_means, ax=axm,\n",
      "                        alpha=kwds['alpha'])\n",
      "\n",
      "    # Shows the confidence bounds, if desired\n",
      "    if kwds['show_bound']:\n",
      "        axm.set_axis_bgcolor('none')\n",
      "\n",
      "    # Recolors the lines, if desired\n",
      "    if kwds['colormap'] is not None and len(kwds['colormap'].shape) == 1:\n",
      "        for idx, l in enumerate(axm.get_lines()):\n",
      "            l.set_color(kwds['colormap'][idx])\n",
      "    elif kwds['colormap'] is not None:\n",
      "        for idx, l in enumerate(axm.get_lines()):\n",
      "            l.set_color(kwds['colormap'][idx, :])\n",
      "\n",
      "    # Sets up the labels\n",
      "    axm.set_xticklabels(map(int, axm.get_xticks()), size=kwds['tick_size'])\n",
      "    axm.set_yticklabels(axm.get_yticks(), size=kwds['tick_size'])\n",
      "    axm.set_xlabel('Number of Observations', size=kwds['label_size'])\n",
      "    axm.set_ylabel('Power of the Test', size=kwds['label_size'])\n",
      "    axm.set_title(kwds['title'], size=kwds['title_size'])\n",
      "\n",
      "    # Adds the grid, if desired\n",
      "    if kwds['grid']:\n",
      "        axm.grid()\n",
      "\n",
      "    leg = axm.get_legend()\n",
      "    # Sets the legend position\n",
      "    if kwds['leg_offset'] is not None:\n",
      "        leg.set_bbox_to_anchor(kwds['leg_offset'])\n",
      "    # Sets up the legend text\n",
      "    for idx, txt in enumerate(leg.get_texts()):\n",
      "        txt.set_text(labels[idx])\n",
      "        txt.set_size(kwds['legend_size'])\n",
      "\n",
      "    # Returns the figure\n",
      "    return fig"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=#top>Return to the top</a>\n",
      "\n",
      "<a id=\"parameters\"></a>\n",
      "### Sets analysis parameters\n",
      "\n",
      "We can also set some necessary parameters for handling files and this analysis.\n",
      "\n",
      "If <strong><code>overwrite</code></strong> is set to <code><font color=\"green\">True</font></code>, new files will be downloaded whenever the notebook is run. This is not recommended.\n",
      "\n",
      "###### Pandas file handling parameters\n",
      "* The <strong><code>txt_delim</code></strong> specifies the way columns are separated in the files. Qiime standards typically use text (.txt) files, which are separated by a tab-character (<code>'\\t'</code>).\n",
      "* <strong><code>map_index</code></strong> specifies the name of the file containing the sample names. In Qiime, this is named <code>\u201c#SampleID\u201d</code>.\n",
      "* It is possible the mapping file may be missing values, as participants are free to skip any question, so possible missing values are given by <strong><code>map_nas</code></strong>.\n",
      "* <strong><code>write_na</code></strong> gives a value used when the files are written. Using an empty space, (''), will cause certain Qiime scripts like group_signifigance.py will ignore the missing group.\n",
      "\n",
      "###### Alpha Diversity \n",
      "* We can specify the alpha diversity metric we wish to focus on by setting the <strong><code>a_div_metric</code></strong>. If the tables calculated in the Preprocessing notebook are used, options are PD whole tree (PD_whole_tree_mean), Observed Species (observed_species_mean), Chao1 (chao1_mean), and Shannon diversity. These are designated as \u201cmean\u201d since they are calculated as the mean diversity after 10 rounds of rarefaction.\n",
      "* The <strong><code>y_label</code></strong> is the text used as a ylabel for alpha diversity graphs.\n",
      "\n",
      "###### Power Analysis Parameters\n",
      "* <strong><code>cats</code></strong> is used to select the metadata categories we will be analyzing.\n",
      "* <strong><code>order</code></strong> narrows the focus of the analysis to samples within those categories.\n",
      "* <strong><code>labels</code></strong> describes the categories for plotting\n",
      "* <strong><code>control_cats</code></strong> are categories which are held constant throughout the analysis. \n",
      "If the current exerimental category (<code>cats[i]</code>) is a control category, it will be ignored as a control during that round of analysis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "overwrite = False\n",
      "# Sets parameters for reading tables into pandas\n",
      "txt_delim = '\\t'\n",
      "map_index = '#SampleID'\n",
      "map_nas = ['NA', 'no_data', 'unknown', '']\n",
      "write_na = ''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sets paramaters for alpha diversity\n",
      "a_div_metric = 'PD_whole_tree_mean'\n",
      "a_title = 'PD Whole Tree Diversity'\n",
      "bu_title = 'Unweighted UniFrac Distance'\n",
      "bw_title = 'Weighted UniFrac Distance'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sets up categories for power analysis\n",
      "cats = ['BODY_HABITAT',\n",
      "        'IBD',\n",
      "        'TYPES_OF_PLANTS',\n",
      "        'ANTIBIOTIC_SELECT',\n",
      "        'AGE_CAT',\n",
      "        'BMI_CAT',\n",
      "        'COLLECTION_SEASON',        \n",
      "        'EXERCISE_FREQUENCY',\n",
      "        'SLEEP_DURATION',\n",
      "        'EXERCISE_LOCATION',\n",
      "        'ALCOHOL_CONSUMPTION']\n",
      "\n",
      "# Sets the order for comparison\n",
      "order = [['UBERON:oral cavity', 'UBERON:feces'],\n",
      "         ['I do not have IBD', 'IBD'],\n",
      "         ['Less than 5', 'More than 30'],\n",
      "         ['In the past month', 'Not in the last year'],\n",
      "         ['20s', '60s'],\n",
      "         ['Normal', 'Obese'],\n",
      "         ['Winter', 'Summer'],\n",
      "         ['Rarely', 'Daily'],\n",
      "         ['Less than 6 hours', '8 or more hours'],\n",
      "         ['Yes', 'No']]\n",
      "\n",
      "# Sets up the labels for plotting\n",
      "labels = np.array(['Bodysite',\n",
      "                   'IBD',\n",
      "                   'Number of Plants',\n",
      "                   'Antibiotic Use',\n",
      "                   'Age',\n",
      "                   'BMI',\n",
      "                   'Season',\n",
      "                   'Exercise Frequency',\n",
      "                   'Sleep Duration',\n",
      "                   'Alcohol Use'])\n",
      "\n",
      "# Sets up the categories and their ordering\n",
      "control_cats = ['IBD', 'BMI_CAT', 'TYPES_OF_PLANTS', 'DIABETES', \n",
      "                'ANTIBIOTIC_SELECT', 'AGE_CAT', 'COLLECTION_SEASON',\n",
      "                'SLEEP_DURATION']\n",
      "\n",
      "cmap= np.array([[153,  30,  61], #345,  80,  60\n",
      "                [178,  53,  53], #  0,  70,  70\n",
      "                [204, 152,  81], # 30,  60,  80\n",
      "                [229, 219, 114], # 55,  50,  90\n",
      "                [204, 255, 153], # 90,  40, 100\n",
      "                [127, 255, 127], #120,  50, 100\n",
      "                [ 91, 229, 160], #150, 60, 90\n",
      "                [ 61, 204, 204], #180, 70, 80\n",
      "                [ 35, 107, 178], #210, 80, 70\n",
      "                ])/255."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"ifilepath\"></a>\n",
      "######Imports the files\n",
      "\n",
      "This points to the directories and files where analysis files are located. If the directories are not avaliable, they can be downloaded. Files for <a href=\"https://www.dropbox.com/s/so6hpzq1oib8xhv/all_samples.tgz?dl=0\">all bodysites, all samples</a>, <a href=\"https://www.dropbox.com/s/kq4rnn1aw23zwp3/oral_samples.tgz?dl=0\">oral Samples</a>, and <a href=\"https://www.dropbox.com/s/afzpr31tmf5i500/fecal_samples.tgz?dl=0\">fecal samples</a> can be downloaded automatically using this notebook, or from the links. This notebook focuses on samples from fecal and oral samples with a single sample per individual for all of the indiviudals who submitted samples.\n",
      "\n",
      "The data is assumed to be saved in a directory called <font color=\"darkblue\">agp_analysis</font> in the parent directory of the current directory. (It is assumed the filepath is located in the current, notebook directory). To change the location of these files, the <code>base_dir</code> can be changed to an alternative location.\n",
      "\n",
      "We can begin by setting up directories to save the files which will be downloaded and handled here. The default setting is to perform the analysis in a new directory, <code>AGPanalysis</code> in the parent directory of the current directory (assumed to be the notebook directory).\n",
      "\n",
      "To change where data is saved, the <code>base_dir</code> should be set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sets up the base directory\n",
      "base_dir = pjoin(abspath('/Users/jwdebelius/Desktop/'), 'agp_analysis')\n",
      "# base_dir = '/Users/jwdebelius/Repositories/AmGutNotebooks/agp_analysis'\n",
      "check_dir(base_dir)\n",
      "otu_dir = pjoin(base_dir, 'otu_tables')\n",
      "check_dir(otu_dir)\n",
      "\n",
      "# Sets the directory where the files should be located\n",
      "oral_dir = pjoin(otu_dir, 'oral_samples')\n",
      "check_dir(oral_dir)\n",
      "oaws_dir = pjoin(oral_dir, 'all_otus_single_samples')\n",
      "check_dir(oaws_dir)\n",
      "all_dir = pjoin(otu_dir, 'all_samples')\n",
      "check_dir(all_dir)\n",
      "fecal_dir = pjoin(otu_dir, 'fecal_samples')\n",
      "check_dir(fecal_dir)\n",
      "faws_dir = pjoin(fecal_dir, 'all_otus_single_samples')\n",
      "check_dir(faws_dir)\n",
      "\n",
      "# Sets the subset filepath for all samples\n",
      "oral_otu_fp = pjoin(oaws_dir, 'AGP_100nt_oral_even10k.biom')\n",
      "oral_map_fp = pjoin(oaws_dir, 'AGP_100nt_oral_even10k.txt')\n",
      "oral_ubd_fp = pjoin(oaws_dir, 'unweighted_unifrac_AGP_100nt_oral_even10k.txt')\n",
      "oral_wbd_fp = pjoin(oaws_dir, 'weighted_unifrac_AGP_100nt_oral_even10k.txt')\n",
      "\n",
      "awsr_otu_fp = pjoin(faws_dir, 'AGP_100nt_fecal_even10k.biom')\n",
      "awsr_map_fp = pjoin(faws_dir, 'AGP_100nt_fecal_even10k.txt')\n",
      "awsr_ubd_fp = pjoin(faws_dir, 'unweighted_unifrac_AGP_100nt_fecal_even10k.txt')\n",
      "awsr_wbd_fp = pjoin(faws_dir, 'weighted_unifrac_AGP_100nt_fecal_even10k.txt')\n",
      "\n",
      "# Loads the distance matrix between samples\n",
      "all_ubd_fp = pjoin(all_dir, 'unweighted_unifrac_AGP_100nt_even10k.txt')\n",
      "all_wbd_fp = pjoin(all_dir, 'weighted_unifrac_AGP_100nt_even10k.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Downloads the files if they are not already avaliable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Gets the biom tables for all samples\n",
      "if not exists(all_dir) or overwrite:\n",
      "    # Downloads the files\n",
      "    !curl -OL https://www.dropbox.com/s/so6hpzq1oib8xhv/all_samples.tgz\n",
      "    # Extracts the data\n",
      "    !tar -xzf all_samples.tgz\n",
      "    # Moves the data\n",
      "    remove(pjoin('.', 'all_samples.tgz'))\n",
      "    move(pjoin('.', 'all_samples'), otu_dir)\n",
      "    \n",
      "# Gets data for single samples with all the OTUs\n",
      "if not exists(faws_dir) or overwrite:\n",
      "    # Downloads the files\n",
      "    !curl -OL https://www.dropbox.com/s/w85221gh1tmr4u0/all_otus_single_samples.tgz\n",
      "    # Extracts the data\n",
      "    !tar -xzf all_otus_single_samples.tgz\n",
      "    # Moves the data\n",
      "    remove(pjoin('.', 'all_otus_single_samples.tgz'))\n",
      "    move(pjoin('.', 'all_otus_single_samples'), fecal_dir)\n",
      "\n",
      "# Downloads oral sample data\n",
      "if not exists(oaws_dir) or overwrite:\n",
      "    # Downloads the files\n",
      "    !curl -OL https://www.dropbox.com/s/kq4rnn1aw23zwp3/oral_samples.tgz\n",
      "    # Extracts the data\n",
      "    !tar -xzf oral_samples.tgz\n",
      "    # Moves the data\n",
      "    remove(pjoin('.', 'oral_samples.tgz'))\n",
      "    move(pjoin('.', 'oral_samples'), otu_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Loads the files into the notebook for analysis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Loads the whole sample table\n",
      "faws_otu = load_table(awsr_otu_fp)\n",
      "faws_map = pad_index(pd.read_csv(awsr_map_fp,\n",
      "                                 sep=txt_delim, \n",
      "                                 na_values=map_nas),\n",
      "                     index_col=map_index)\n",
      "faws_ubd = DistanceMatrix.read(awsr_ubd_fp)\n",
      "faws_wbd = DistanceMatrix.read(awsr_wbd_fp)\n",
      "# Loads the oral table\n",
      "oral_otu = load_table(oral_otu_fp)\n",
      "oral_map = pad_index(pd.read_csv(oral_map_fp,\n",
      "                                 sep=txt_delim, \n",
      "                                 na_values=map_nas),\n",
      "                     index_col=map_index)\n",
      "oral_ubd = DistanceMatrix.read(oral_ubd_fp)\n",
      "oral_wbd = DistanceMatrix.read(oral_wbd_fp)\n",
      "\n",
      "# Loads the full distance table\n",
      "comb_ubd = DistanceMatrix.read(all_ubd_fp)\n",
      "comb_wbd = DistanceMatrix.read(all_wbd_fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"clean\"></a>\n",
      "######Cleans up mapping categories\n",
      "We also need to clean up some categories in the mapping file for simplicity here. \n",
      "\n",
      "Due to the number of individuals who took antibiotics in the past week, compared to the number of indiviudals who took individuals in the past month, we will combine the two categories, and label them \"In the past month\".\n",
      "\n",
      "We will also convert both Crohn's disease and Ulcerative Colitis into a single category, IBD.\n",
      "\n",
      "We will combine people who exercise Never and those who exercise Rarely into a single category."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Combines antibiotic select into \"Last month\"\n",
      "faws_map.loc[faws_map.ANTIBIOTIC_SELECT == 'In the past week',\n",
      "             'ANTIBIOTIC_SELECT'] = 'In the past month'\n",
      "oral_map.loc[oral_map.ANTIBIOTIC_SELECT == 'In the past week',\n",
      "             'ANTIBIOTIC_SELECT'] = 'In the past month'\n",
      "\n",
      "# Combines individuals with Ulcerative Colitis and Crohn's disease into a\n",
      "# single category\n",
      "faws_map.loc[faws_map.IBD == \"Crohn's disease\", 'IBD'] = 'IBD'\n",
      "faws_map.loc[faws_map.IBD == \"Ulcerative colitis\", 'IBD'] = 'IBD'\n",
      "oral_map.loc[oral_map.IBD == \"Crohn's disease\", 'IBD'] = 'IBD'\n",
      "oral_map.loc[oral_map.IBD == \"Ulcerative colitis\", 'IBD'] = 'IBD'\n",
      "\n",
      "faws_map.loc[faws_map.EXERCISE_FREQUENCY == 'Never', 'EXERCISE_FREQUENCY'] = 'Rarely'\n",
      "faws_map.loc[faws_map.EXERCISE_FREQUENCY == 'Rarely (few times/month)', 'EXERCISE_FREQUENCY'] = 'Rarely'\n",
      "oral_map.loc[oral_map.EXERCISE_FREQUENCY == 'Never', 'EXERCISE_FREQUENCY'] = 'Rarely'\n",
      "oral_map.loc[oral_map.EXERCISE_FREQUENCY == 'Rarely (few times/month)', 'EXERCISE_FREQUENCY'] = 'Rarely'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"functions\"></a>\n",
      "###### Defines Test Functions\n",
      "We'll start by defining a set of functions to test our results. We'll test the differnce in alpha diversity (intrasample varaiblity) and beta diversity (intersample variability)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_alpha_kruskal(ids, meta, a_div_metric):\n",
      "    \"\"\"Calculates difference in alpha diverisity for category and distance matrix\"\"\"\n",
      "    # Gets the id values\n",
      "    alpha = [meta.loc[id_, a_div_metric] for id_ in ids]\n",
      "    return kruskal(*alpha)[1]\n",
      "\n",
      "def test_beta_permanova(ids, meta, dm, cat, num_iter=249):\n",
      "    \"\"\"Tests difference in beta diversity for a category and distance matrix\"\"\"\n",
      "    # Gets the map and distance matrix subset\n",
      "    all_ids = np.array(ids).flatten()\n",
      "    meta = meta.loc[all_ids]\n",
      "    dm2 = deepcopy(dm)\n",
      "    dm2 = dm2.filter(all_ids)\n",
      "    # Calculates the permanova\n",
      "    perma_results = permanova(dm2, meta, cat, num_iter)\n",
      "    return perma_results['p-value']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"bodysite\"></a>\n",
      "### Calculates Power for Body Sites\n",
      "We'll start by calculating power for the difference in bodysite. To do this, we need to create a map that will combine our fecal and oral samples. We'll then define two test functions, which can take a list of sample ids as input, and return a p value. We'll use the <code>test_alpha_kruskal</code> and <code>test_beta_permanova</code> functions we defined above, and just modify some parameters. \n",
      "Finally, we'll emperically calculate the power, using <code>get_subsampled_power</code>. We will select \"paired\" mode, which will select a sample in each category which match eachother in all the control categories, but vary in bodysite. If more than one sample satisfies the criteria, the sample used will be randomly selected. The subset is generated by randomly subsampling the control-matched samples.\n",
      "Due to time contraints (this takes about 30 minutes to run on a 16gb Macbook), we'll calculate power over 100 tests, but we'll try that 15 times for each sample count."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Combines the maps\n",
      "comb_map = pd.concat([faws_map, oral_map])\n",
      "\n",
      "# Adds the body site comparison\n",
      "alpha_test = lambda x: test_alpha_kruskal(x, comb_map, a_div_metric)\n",
      "beta_test = lambda x: test_beta_permanova(x, comb_map, comb_ubd, cats[0])\n",
      "\n",
      "# Gets the paired sample effect\n",
      "a_pwr, a_cnts = subsample_paired_power(test=alpha_test,\n",
      "                                       meta=comb_map,\n",
      "                                       cat=cats[0],\n",
      "                                       control_cats=control_cats,\n",
      "                                       min_counts=2,\n",
      "                                       counts_interval=1,\n",
      "                                       max_counts=20,\n",
      "                                       order=order[0],\n",
      "                                       num_iter=500,\n",
      "                                       num_runs=15,\n",
      "                                       alpha_pwr=0.05)\n",
      "\n",
      "b_pwr, b_cnts = subsample_paired_power(test=beta_test,\n",
      "                                       meta=comb_map,\n",
      "                                       cat=cats[0],\n",
      "                                       min_counts=2,\n",
      "                                       max_counts=20,\n",
      "                                       counts_interval=1,\n",
      "                                       control_cats=control_cats,\n",
      "                                       order=order[0],\n",
      "                                       num_iter=500,\n",
      "                                       num_runs=15,\n",
      "                                       alpha_pwr=0.05)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"fecal_power\"></a>\n",
      "### Calculates Power for the fecal samples\n",
      "We'll now analyze the rest of the categories, focusing on fecal samples. We'll keep track of the counts and power arrays.\n",
      "\n",
      "This step is again computationall expensive due to the number of iterations performed. It took between four and six hours to run on a 16gb ram laptop. \n",
      "\n",
      "Also, please note that becuase this is done with a pseudo-random number generator without a defined seed, results may vary slightly run-to-run, although over-all trends will remain constant."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sets up the list of tests for analyzing data\n",
      "alpha_tests = [lambda x: test_alpha_kruskal(x, faws_map, a_div_metric)\n",
      "              for cat in cats[1:]]\n",
      "beta_tests = [lambda x: test_beta_permanova(x, faws_map, faws_ubd, cat=cat)\n",
      "              for cat in cats[1:]]\n",
      "\n",
      "# Loops through the categories\n",
      "a_powers = [a_pwr]\n",
      "a_counts = [a_cnts]\n",
      "b_powers = [b_pwr]\n",
      "b_counts = [b_cnts]\n",
      "\n",
      "# Calculates the counts and paired sample for the bodysite\n",
      "for idx, cat in enumerate(cats[1:]):\n",
      "    # Gets the control categories for the group\n",
      "    ctrl_cats = deepcopy(control_cats)\n",
      "    if cat in ctrl_cats:\n",
      "        ctrl_cats.remove(cat)\n",
      "    # Calculates paired power for controlled samples\n",
      "    a_pwr, a_cnts = subsample_paired_power(test=alpha_tests[idx],\n",
      "                                           meta=faws_map,\n",
      "                                           control_cats=ctrl_cats,\n",
      "                                           cat=cat,\n",
      "                                           order=order[idx + 1],\n",
      "                                           num_iter=500,\n",
      "                                           num_runs=15)\n",
      "    b_pwr, b_cnts = subsample_paired_power(test=beta_tests[idx],\n",
      "                                           meta=faws_map,\n",
      "                                           control_cats=ctrl_cats,\n",
      "                                           cat=cat,\n",
      "                                           order=order[idx + 1],\n",
      "                                           num_iter=500,\n",
      "                                           num_runs=15)\n",
      "    a_powers.append(a_pwr)\n",
      "    a_counts.append(a_cnts)\n",
      "    b_powers.append(b_pwr)\n",
      "    b_counts.append(b_cnts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyError",
       "evalue": "'Less than 6 hours'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-12-f224872a9e3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                                            \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                            \u001b[0mnum_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                                            num_runs=15)\n\u001b[0m\u001b[1;32m     27\u001b[0m     b_pwr, b_cnts = subsample_paired_power(test=beta_tests[idx],\n\u001b[1;32m     28\u001b[0m                                            \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfaws_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jwdebelius/.virtualenvs/all_about_the_dev/lib/python2.7/site-packages/skbio/stats/power.pyc\u001b[0m in \u001b[0;36msubsample_paired_power\u001b[0;34m(test, meta, cat, control_cats, order, strict_match, alpha_pwr, min_observations, max_counts, counts_interval, min_counts, num_iter, num_runs)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;31m# Checks for the number of sampling pairs avaliable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m     \u001b[0msub_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaired_subsamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_cats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict_match\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;31m# Determines the minimum number of ids avaliable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jwdebelius/.virtualenvs/all_about_the_dev/lib/python2.7/site-packages/skbio/stats/power.pyc\u001b[0m in \u001b[0;36mpaired_subsamples\u001b[0;34m(meta, cat, control_cats, order, strict_match)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;31m# Determines the number of samples, and the experimental and control group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m     \u001b[0mgroup_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m     \u001b[0mctrl_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroup_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgroup_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morder\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mctrl_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyError\u001b[0m: 'Less than 6 hours'"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"effect\"></a>\n",
      "### Estimates the Effect Size\n",
      "With the power and effect size arrays, we can calculate the average effect sizes. This is useful for extrapolating curves when we have enough observations to estimate power, but not enough observations to complete the power curve.\n",
      "\n",
      "We will use the function we defined earlier, <a href=\"#funs\"><code>collate_effect_size</code></a>. Unfortunately, this calculation is challenging in body site samples becuase method we are using to solve power doens't work for fully powered studies. So, we will exclude the effect size calculation for the bodysite, and plot the curve based on our emperical results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a_eff_means, a_eff_bounds = collate_effect_size(a_counts[1:], a_powers[1:], alpha=0.05)\n",
      "b_eff_means, b_eff_bounds = collate_effect_size(b_counts[1:], b_powers[1:], alpha=0.05)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can estimate the number of samples we need in each group by taking the mean and rounding it up to the nearest 5 samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pad = ['\\t\\t', '', '\\t', '\\t\\t', '\\t\\t', \n",
      "       '', '', '', '\\t']\n",
      "print 'Category\\t\\tAlpha\\tBeta'\n",
      "print '---------------------------------------'\n",
      "for idx, eff_m in enumerate(a_eff_means):\n",
      "    print '%s%s\\t %i\\t %i' % (cats[idx + 1], \n",
      "                            pad[idx],\n",
      "                            np.ceil(ft.solve_power(eff_m, nobs=None, power=0.8, alpha=0.05)/5.)*5,\n",
      "                            np.ceil(ft.solve_power(b_eff_means[idx], nobs=None, power=0.8, alpha=0.05)/5.)*5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"#top\">Return to the top</a>\n",
      "\n",
      "<a id=\"curves\"></a>\n",
      "#### Plot power curves\n",
      "We can also generate figures, which are useful for making an over-all comparison of effect size. We'll start by estimating the power curve for bodysite, which we can generate emperically based on the data we have."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Gets the mean power\n",
      "a_body_mean = np.nanmean(a_powers[0], 0)\n",
      "# Gets the bounds on the power\n",
      "a_body_bound = confidence_bound(a_powers[0], axis=0)\n",
      "\n",
      "# Prevents the confidence interval on the power from being greater than 1\n",
      "a_body_lower = np.vstack((a_body_mean - a_body_bound, \n",
      "                          np.ones(a_body_mean.shape))).min(0)\n",
      "a_body_upper = np.vstack((a_body_mean + a_body_bound, \n",
      "                          np.ones(a_body_mean.shape))).min(0)\n",
      "\n",
      "# Gets the mean power\n",
      "b_body_mean = np.nanmean(b_powers[0], 0)\n",
      "# Gets the bounds on the power\n",
      "b_body_bound = confidence_bound(b_powers[0], axis=0)\n",
      "# Prevents the confidence interval on the power from being greater than 1\n",
      "b_body_lower = np.vstack((b_body_mean - b_body_bound, \n",
      "                          np.ones(b_body_mean.shape))).min(0)\n",
      "b_body_upper = np.vstack((b_body_mean + b_body_bound, \n",
      "                          np.ones(b_body_mean.shape))).min(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we can sort the colormap, categories, means and bounds based on the largest effect size for both alpha and beta diversity. In both cases, we know that bodysite has the largest effect, so this will be exlcuded."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Gets order associatied with the samples\n",
      "b_order = np.argsort(b_eff_means)\n",
      "a_order = np.argsort(a_eff_means)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can start by generating the curve for alpha diversity results. For the poster, the final figures were imported into illustrator where text size was edited, and legend position was adjusted."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generates a plot of the alpha diversity\n",
      "alpha_fig = plot_effects(effect_means=a_eff_means[a_order[::-1]],\n",
      "                         effect_bounds=a_eff_bounds[a_order[::-1]],\n",
      "                         labels=labels[a_order[::-1] + 1],\n",
      "                         sample_counts=np.hstack((np.array([2]), \n",
      "                                                  np.arange(5, 255, 5))),\n",
      "                         colormap=cmap[a_order[::-1], :],\n",
      "                         title='PD Whole Tree Diversity',\n",
      "                         legend_size=13)\n",
      "# Adds the bodysite line to the axis\n",
      "axes = alpha_fig.axes\n",
      "axes[0].plot(a_counts[0], a_body_lower, ':', color=[0.5, 0.5, 0.5])\n",
      "axes[1].plot(a_counts[0], a_body_upper, ':', color=[0.5, 0.5, 0.5])\n",
      "axes[2].plot(a_counts[0], a_body_mean, linewidth=2, color=[0.5, 0.5, 0.5], label='Bodysite')\n",
      "alpha_fig.set_size_inches(10, 5)\n",
      "alpha_fig.axes[0].set_position((0.125, 0.13, 0.55, 0.75))\n",
      "alpha_fig.axes[1].set_position((0.125, 0.13, 0.55, 0.75))\n",
      "alpha_fig.axes[2].set_position((0.125, 0.13, 0.55, 0.75))\n",
      "axes[2].get_legend().set_visible(False)\n",
      "handles = list(alpha_fig.axes[2].get_legend_handles_labels())\n",
      "handles[0].insert(0, handles[0].pop(-1))\n",
      "h_order = list(np.hstack((np.array([0]), a_order[::-1]+1)))\n",
      "leg2 = alpha_fig.legend(handles=handles[0],\n",
      "                        labels=labels[h_order])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generates a plot of the beta diversity\n",
      "beta_fig = plot_effects(effect_means=b_eff_means[b_order[::-1]],\n",
      "                         effect_bounds=b_eff_bounds[b_order[::-1]],\n",
      "                         labels=labels[b_order[::-1]],\n",
      "                         sample_counts=np.hstack((np.array([2]), \n",
      "                                                  np.arange(5, 255, 5))),\n",
      "                         colormap=cmap[b_order[::-1], :],\n",
      "                         title='Unweighted UniFrac',\n",
      "                         legend_size=13)\n",
      "# Adds the bodysite line to the axis\n",
      "axes = beta_fig.axes\n",
      "axes[0].plot(b_counts[0], b_body_lower, ':', color=[0.5, 0.5, 0.5])\n",
      "axes[1].plot(b_counts[0], b_body_upper, ':', color=[0.5, 0.5, 0.5])\n",
      "axes[2].plot(b_counts[0], b_body_mean, linewidth=2, color=[0.5, 0.5, 0.5], label='Bodysite')\n",
      "axes[2].get_legend().set_visible(False)\n",
      "handles = list(beta_fig.axes[2].get_legend_handles_labels())\n",
      "handles[0].insert(0, handles[0].pop(-1))\n",
      "h_order = list(np.hstack((np.array([0]), b_order[::-1]+1)))\n",
      "# leg2 = beta_fig.legend(handles=handles[0],\n",
      "#                         labels=labels[h_order])\n",
      "leg2 = beta_fig.legend(handles=handles[0],\n",
      "                        labels=labels[h_order])\n",
      "# Adjusts the figure and axes sizes to make things look nice\n",
      "beta_fig.set_size_inches(10, 5)\n",
      "beta_fig.axes[0].set_position((0.125, 0.13, 0.60, 0.75))\n",
      "beta_fig.axes[1].set_position((0.125, 0.13, 0.60, 0.75))\n",
      "beta_fig.axes[2].set_position((0.125, 0.13, 0.60, 0.75))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}